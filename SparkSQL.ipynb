{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://spark.apache.org/images/spark-logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql(query):\n",
    "    \n",
    "    conn = sqlite3.connect(';memory;')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    start_location = query.find('from')\n",
    "    table_name = query[start_location + len('from '):query[start_location].find(' ')]\n",
    "    \n",
    "    cursor = conn.execute(query)\n",
    "    names = [description[0] for description in cursor.description]\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    \n",
    "    return pd.DataFrame(result, columns=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Customer Placing the Largest Number of Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to sqlite\n",
    "conn = sqlite3.connect(';memory;')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert data into sqlite\n",
    "queries = '''\n",
    "drop table if exists orders;\n",
    "Create table If Not Exists orders (order_number int, customer_number int);\n",
    "insert into orders (order_number, customer_number) values ('1', '1');\n",
    "insert into orders (order_number, customer_number) values ('2', '2');\n",
    "insert into orders (order_number, customer_number) values ('3', '3');\n",
    "insert into orders (order_number, customer_number) values ('4', '3');\n",
    "'''\n",
    "\n",
    "query_list = queries.splitlines()[1:]\n",
    "\n",
    "for query in query_list:\n",
    "    cursor.execute(query)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Create a Pandas DataFrame from the data\n",
    "df = sql('''\n",
    "select *\n",
    "from orders\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|customer_number|\n",
      "+---------------+\n",
      "|              3|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def customer_orders(df):\n",
    "    \n",
    "    # Create a spark session, a spark dataframe and a temporary view\n",
    "    spark = SparkSession.builder.appName('Customer Orders').getOrCreate()\n",
    "    df = spark.createDataFrame(data = df)\n",
    "    df.createOrReplaceTempView('orders')\n",
    "    \n",
    "    # Query the data\n",
    "    query = '''\n",
    "\n",
    "    select \n",
    "        o.customer_number\n",
    "    from orders as o\n",
    "    group by \n",
    "        o.customer_number\n",
    "    order by count(*) desc\n",
    "    limit 1;\n",
    "    '''\n",
    "\n",
    "    # Return results\n",
    "    results = spark.sql(query)\n",
    "    return results.show()\n",
    "\n",
    "customer_orders(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|customer_number|\n",
      "+---------------+\n",
      "|              3|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def customers_orders(df):\n",
    "    \n",
    "    # Create a spark session, a spark dataframe and a temporary view\n",
    "    spark = SparkSession.builder.appName('Customer Orders').getOrCreate()\n",
    "    df = spark.createDataFrame(data=df)\n",
    "    df.createOrReplaceTempView('orders')\n",
    "    \n",
    "    # Query the data\n",
    "    temp = df.select(df[1]).groupby(df[1]).count()\n",
    "    results = temp.select(temp[0]).orderBy(temp[1], ascending=0).limit(num=1)\n",
    "    return results.show()\n",
    "\n",
    "customers_orders(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consecutive Numbers\n",
    "Write an SQL query to find all numbers that appear at least three times consecutively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def consecutiveNumbers():\n",
    "    \n",
    "    spark = SparkSession.builder.appName('ConsecutiveNumber').getOrCreate()\n",
    "\n",
    "    df = spark.read.option('header', 'true').csv('logs.csv')\n",
    "    df.createOrReplaceTempView('logs')\n",
    "\n",
    "    query = '''\n",
    "    select \n",
    "        distinct l1.num as ConsecutiveNumber\n",
    "    from logs as l1\n",
    "    join logs as l2\n",
    "    on l1.id = l2.id - 1\n",
    "    join logs as l3\n",
    "    on l2.id = l3.id -1\n",
    "    where \n",
    "        l1.num = l2.num and l2.num = l3.num\n",
    "    '''\n",
    "    result = spark.sql(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|ConsecutiveNumber|\n",
      "+-----------------+\n",
      "|              257|\n",
      "|              274|\n",
      "|              130|\n",
      "|               63|\n",
      "|              451|\n",
      "|               98|\n",
      "|              483|\n",
      "|              231|\n",
      "|               64|\n",
      "|              154|\n",
      "|              391|\n",
      "|              334|\n",
      "|               85|\n",
      "|              110|\n",
      "|              271|\n",
      "|              247|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = consecutiveNumbers()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Play Analysis I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an SQL query to report the first login date for each player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|player_id|first_login|\n",
      "+---------+-----------+\n",
      "|        1| 2016-03-01|\n",
      "|        2| 2017-06-25|\n",
      "|        3| 2016-03-02|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def game_play_analysis():\n",
    "    \n",
    "    spark = SparkSession.builder.appName('game_play').getOrCreate()\n",
    "    df = spark.read.option('header', 'true').csv('activity.csv')\n",
    "    df.createOrReplaceTempView('activity')\n",
    "    \n",
    "    query = '''\n",
    "    select \n",
    "    a1.player_id,\n",
    "    min(a1.event_date) as first_login\n",
    "    from activity as a1\n",
    "    group by a1.player_id;\n",
    "    '''\n",
    "    \n",
    "    result = spark.sql(query)\n",
    "    return result\n",
    "\n",
    "result = game_play_analysis()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_score():\n",
    "    \n",
    "    # Starting a session\n",
    "    spark = SparkSession.builder.appName('Nth_Highest_Salary').getOrCreate()\n",
    "\n",
    "    # loading the data\n",
    "    df = spark.read.option('header', 'true').option('inferSchema', 'true')\\\n",
    "    .csv('scores.csv')\n",
    "\n",
    "    df.createOrReplaceTempView('scores')\n",
    "\n",
    "    query = '''\n",
    "    select \n",
    "        aaa.score,\n",
    "        aaa.rank\n",
    "    from \n",
    "    (\n",
    "        select \n",
    "            s1.id,\n",
    "            s1.score,\n",
    "            count(*) as rank\n",
    "        from scores as s1, (select distinct score from scores) as s2\n",
    "        where s1.score <= s2.score\n",
    "        group by s1.id, s1.score\n",
    "        order by s1.score desc\n",
    "    ) aaa\n",
    "    '''\n",
    "\n",
    "    result = spark.sql(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|score|rank|\n",
      "+-----+----+\n",
      "|  4.0|   1|\n",
      "|  4.0|   1|\n",
      "| 3.85|   2|\n",
      "| 3.65|   3|\n",
      "| 3.65|   3|\n",
      "|  3.5|   4|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = rank_score()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nth_Highest_Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Nth_Highest_Salary|\n",
      "+------------------+\n",
      "|               200|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def Nth_Highest_Salary(n):\n",
    "    \n",
    "    # Starting a session\n",
    "    spark = SparkSession.builder.appName('Nth_Highest_Salary').getOrCreate()\n",
    "    \n",
    "    # loading the data\n",
    "    df = spark.read.option('header', 'true').option('inferSchema', 'true')\\\n",
    "    .csv('employee.csv')\n",
    "    \n",
    "    # Create a temp view called employee\n",
    "    df.createOrReplaceTempView('employee')\n",
    "    \n",
    "    # Query Data\n",
    "    query = '''\n",
    "    SELECT min(salary) as Nth_Highest_Salary\n",
    "    from\n",
    "    (\n",
    "    select salary\n",
    "    FROM Employee\n",
    "    ORDER BY salary DESC\n",
    "    LIMIT {}\n",
    "    )\n",
    "    '''.format(n)\n",
    "\n",
    "    result = spark.sql(query)\n",
    "    return result\n",
    "\n",
    "\n",
    "result = Nth_Highest_Salary(2)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Spark to Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract - tables called movies and users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download postgresql-42.2.18.jar and put into the location: \"C:/Users/UserPc/Downloads/postgresql-42.2.18.jar\"\n",
    "- Launch a postgres server with the details below.\n",
    "- The postgres server should have a database called:  \"data_engineering_linkedin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import required libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "'''\n",
    "Create Spark Session\n",
    "'''\n",
    "spark = pyspark.sql.SparkSession \\\n",
    "   .builder \\\n",
    "   .appName(\"Python Spark SQL basic example\") \\\n",
    "   .config('spark.driver.extraClassPath', \"C:/Users/UserPc/Downloads/postgresql-42.2.18.jar\") \\\n",
    "   .getOrCreate()\n",
    "\n",
    "\n",
    "'''\n",
    "Extract - From Postgres\n",
    "'''\n",
    "def extract_movie_table():\n",
    "\n",
    "   ##read table from db using spark jdbc\n",
    "   movies_df = spark.read \\\n",
    "      .format(\"jdbc\") \\\n",
    "      .option(\"url\", \"jdbc:postgresql://localhost:5432/data_engineering_linkedin\") \\\n",
    "      .option(\"dbtable\", \"movies\") \\\n",
    "      .option(\"user\", \"postgres\") \\\n",
    "      .option(\"password\", \"1365\") \\\n",
    "      .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "      .load()\n",
    "\n",
    "    return movies_df\n",
    "\n",
    "def extract_users_table():\n",
    "\n",
    "   ##read table from db using spark jdbc\n",
    "   users_df = spark.read \\\n",
    "      .format(\"jdbc\") \\\n",
    "      .option(\"url\", \"jdbc:postgresql://localhost:5432/data_engineering_linkedin\") \\\n",
    "      .option(\"dbtable\", \"users\") \\\n",
    "      .option(\"user\", \"postgres\") \\\n",
    "      .option(\"password\", \"1365\") \\\n",
    "      .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "      .load()\n",
    "\n",
    "    return users_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+---+---------+--------------------+----------+\n",
    "| id|     name|         description|  category|\n",
    "+---+---------+--------------------+----------+\n",
    "|  1|   Avatar|Avatar is a 2009 ...|    Sci-Fi|\n",
    "|  2| Avengers|  Avengers is a 2009|    Sci-Fi|\n",
    "|  3| Holidate|  Holidate is a 2009|Not Action|\n",
    "|  4|John Wick| John Wick is a 2009|    Action|\n",
    "+---+---------+--------------------+----------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_to_db(df):\n",
    "\n",
    "   ##load transformed dataframe to the database\n",
    "    properties = {\"user\": \"postgres\",\n",
    "                  \"password\": \"1365\",\n",
    "                  \"driver\": \"org.postgresql.Driver\"\n",
    "                  }\n",
    "                  \n",
    "    df.write.jdbc(url=\"jdbc:postgresql://localhost:5432/data_engineering_linkedin\",\n",
    "                  table = \"avg_ratings\",\n",
    "                  mode = \"overwrite\",\n",
    "                  properties = properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full ETL Example - Extract from Postgres, transform and load to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import required libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "'''\n",
    "Create Spark Session\n",
    "'''\n",
    "spark = pyspark.sql.SparkSession \\\n",
    "   .builder \\\n",
    "   .appName(\"Python Spark SQL basic example\") \\\n",
    "   .config('spark.driver.extraClassPath', \"C:/Users/UserPc/Downloads/postgresql-42.2.18.jar\") \\\n",
    "   .getOrCreate()\n",
    "\n",
    "\n",
    "'''\n",
    "Extract - From Postgres\n",
    "'''\n",
    "def extract_movie_table():\n",
    "\n",
    "   ##read table from db using spark jdbc\n",
    "   movies_df = spark.read \\\n",
    "      .format(\"jdbc\") \\\n",
    "      .option(\"url\", \"jdbc:postgresql://localhost:5432/data_engineering_linkedin\") \\\n",
    "      .option(\"dbtable\", \"movies\") \\\n",
    "      .option(\"user\", \"postgres\") \\\n",
    "      .option(\"password\", \"1365\") \\\n",
    "      .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "      .load()\n",
    "\n",
    "    return movies_df\n",
    "\n",
    "def extract_users_table():\n",
    "\n",
    "   ##read table from db using spark jdbc\n",
    "   users_df = spark.read \\\n",
    "      .format(\"jdbc\") \\\n",
    "      .option(\"url\", \"jdbc:postgresql://localhost:5432/data_engineering_linkedin\") \\\n",
    "      .option(\"dbtable\", \"users\") \\\n",
    "      .option(\"user\", \"postgres\") \\\n",
    "      .option(\"password\", \"1365\") \\\n",
    "      .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "      .load()\n",
    "\n",
    "    return users_df\n",
    "\n",
    "\n",
    "'''\n",
    "Transform - Find average ratings per moive\n",
    "'''\n",
    "def transform_average_ratings(movies_df, users_df):\n",
    "\n",
    "   # Create temporary views for both tables\n",
    "   movies_df.createOrReplaceTempView('movies')\n",
    "    users_df.createOrReplaceTempView('users')\n",
    "\n",
    "   # Extract average ratings\n",
    "   query = '''\n",
    "   select \n",
    "      u.movie_id,\n",
    "      m.name,\n",
    "      avg(rating) as Average_Rating\n",
    "   from users as u\n",
    "   join movies as m\n",
    "   on u.movie_id = m.id\n",
    "   group by \n",
    "      u.movie_id,\n",
    "      m.name\n",
    "   order by u.movie_id desc;\n",
    "   '''\n",
    "\n",
    "    results = spark.sql(query)\n",
    "    return results\n",
    "\n",
    "\n",
    "'''\n",
    "Load - back to Postgres\n",
    "'''\n",
    "\n",
    "def load_df_to_db(df):\n",
    "\n",
    "   ##load transformed dataframe to the database\n",
    "    properties = {\"user\": \"postgres\",\n",
    "                  \"password\": \"1365\",\n",
    "                  \"driver\": \"org.postgresql.Driver\"\n",
    "                  }\n",
    "                  \n",
    "    df.write.jdbc(url=\"jdbc:postgresql://localhost:5432/data_engineering_linkedin\",\n",
    "                  table = \"avg_ratings\",\n",
    "                  mode = \"overwrite\",\n",
    "                  properties = properties)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "   # Extract \n",
    "    movies_df = extract_movie_table()\n",
    "    users_df = extract_users_table()\n",
    "\n",
    "   # Transform\n",
    "   ratings_df = transform_average_ratings(movies_df, users_df)\n",
    "\n",
    "   # Load\n",
    "   load_df_to_db(ratings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Camp Cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://datacamp-community-prod.s3.amazonaws.com/02213cb4-b391-4516-adcd-57243ced8eed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notion Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.notion.so/bardadon/Udemy-Taming-Big-Data-with-Apache-Spark-and-Python-Hands-On-4bc70632803f4801b513df2ed99a41f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>__Spark RDD__<ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Settings configurations\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Context UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-3U7IV4I:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RatingsHistogram</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=RatingsHistogram>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Spark UI in the localhost\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an rdd from the file u.data\n",
    "rdd = sc.textFile('u.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('hdfs://user/maria_dev/ml-100k/u.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard coded RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD from a Hive database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hivCtx = HiveContext(sc)\n",
    "rdd = hivCtx.sql(\"select name, age  from users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 1), (3, 1), (4, 1)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4])\n",
    "new_rdd = rdd.map(lambda x: (x,1))\n",
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Context Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Commands\n",
    "- map\n",
    "- flatmap\n",
    "- filter\n",
    "- distinct\n",
    "- sample\n",
    "- union\n",
    "- intersection\n",
    "- subtract\n",
    "- cartesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4])\n",
    "rdd2 = sc.parallelize([2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### map\n",
    "- Return a new RDD, by applying a function.\n",
    "- For example, lets return a new rdd where each item is raised to second power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Using lambda function__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rdd = rdd1.map(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Using my own function inside lambda__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_2(x):\n",
    "    x_pow_2 = x**2\n",
    "    return x_pow_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rdd = rdd1.map(lambda x: power_2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Using my own function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the age and number of friends\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an rdd from a csv file\n",
    "rdd1 = sc.textFile('fakefriends.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,Will,33,385',\n",
       " '1,Jean-Luc,26,2',\n",
       " '2,Hugh,55,221',\n",
       " '3,Deanna,40,465',\n",
       " '4,Quark,68,21']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a sample of the first 5 items\n",
    "rdd1.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets collect the age and number of friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rdd = rdd1.map(parseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, 385), (26, 2), (55, 221), (40, 465), (68, 21)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatmap\n",
    "- Returns a new flat RDD.\n",
    "- By flat i mean that it takes a dictionary rdd and flatten it, it puts the keys and value in the same iteratble\n",
    "- For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary RDD using map\n",
    "new_rdd = rdd1.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 1), (3, 1), (4, 1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a flat RDD using flatMap\n",
    "new_rdd = rdd1.flatMap(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 3, 1, 4, 1]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, these are the same values, only flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between map and flatMap\n",
    "- Lets split each line in the book using map and flatMap and check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-Employment: Building an Internet Business of One',\n",
       " 'Achieving Financial and Personal Freedom through a Lifestyle Technology Business',\n",
       " 'By Frank Kane',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Copyright � 2015 Frank Kane. ',\n",
       " 'All rights reserved worldwide.',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-Employment:',\n",
       " 'Building',\n",
       " 'an',\n",
       " 'Internet',\n",
       " 'Business',\n",
       " 'of',\n",
       " 'One',\n",
       " 'Achieving',\n",
       " 'Financial',\n",
       " 'and']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_words = book.flatMap(lambda x: x.split())\n",
    "flat_words.collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Self-Employment:', 'Building', 'an', 'Internet', 'Business', 'of', 'One'],\n",
       " ['Achieving',\n",
       "  'Financial',\n",
       "  'and',\n",
       "  'Personal',\n",
       "  'Freedom',\n",
       "  'through',\n",
       "  'a',\n",
       "  'Lifestyle',\n",
       "  'Technology',\n",
       "  'Business'],\n",
       " ['By', 'Frank', 'Kane'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Copyright', '�', '2015', 'Frank', 'Kane.'],\n",
       " ['All', 'rights', 'reserved', 'worldwide.'],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = book.map(lambda x: x.split())\n",
    "words.collect()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, with flatMap, each line has been split into words\n",
    "- And with map, each line is now a list of words.\n",
    "- So by using flat, we kinda go down a level and extract the items into a single iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter\n",
    "- Returns a new rdd that meets the conditions of a function\n",
    "- For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using lambda - Collect all the items that are bigger then 2\n",
    "new_rdd = rdd1.filter(lambda x: x > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapValues\n",
    "- Return a new dictionary rdd\n",
    "- Run a function on each value of the items, without changing the keys.\n",
    "- This also retains the original RDD's partitioning to make it much faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, 385), (26, 2), (55, 221), (40, 465), (68, 21)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.collect()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the value into a key-value pair of (value, 1)\n",
    "new_rdd = new_rdd.mapValues(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, (385, 1)), (26, (2, 1)), (55, (221, 1)), (40, (465, 1)), (68, (21, 1))]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, the function only handled the values and not the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Actions\n",
    "- Once we have the rdd, we can run some RDD actions on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common RDD Actions\n",
    "- collect\n",
    "- count\n",
    "- countByValue\n",
    "- take\n",
    "- top\n",
    "- reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can run commands on the file\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a list with all the contents\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'196\\t242\\t3\\t881250949'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the first element\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark context before making another one\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RDD Actions\n",
    "- reduceByKey - Merge values for each key, according to a function\n",
    "- countByValue() - Return the count of each unique value in a RDD as a dictionary of (value, count) pairs.\n",
    "- groupByKey() - Group values with the same key\n",
    "- sortByKey() - Sort values by key\n",
    "- keys(), values() - Return an RDD of just the keys or values\n",
    "\n",
    "__SQL Actions__\n",
    "- join, rightOuterJoin, leftOuterJoin, cogroup, subtractByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, (385, 1)), (26, (2, 1)), (55, (221, 1)), (40, (465, 1)), (68, (21, 1))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__reduceByKey__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_friends_by_age = new_rdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, (3904, 12)),\n",
       " (26, (4115, 17)),\n",
       " (55, (3842, 13)),\n",
       " (40, (4264, 17)),\n",
       " (68, (2696, 10))]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_friends_by_age.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 37.19), (35, 65.89), (2, 40.64), (47, 14.98), (29, 13.08)]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedLines.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use reduceByKey to sum the second column by merging similar keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the amount per customer\n",
    "amount_per_customer = parsedLines.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 4756.8899999999985),\n",
       " (35, 5155.419999999999),\n",
       " (2, 5994.59),\n",
       " (47, 4316.299999999999),\n",
       " (29, 5032.529999999999)]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount_per_customer.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also add values with the add method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 4756.8899999999985),\n",
       " (35, 5155.419999999999),\n",
       " (2, 5994.59),\n",
       " (47, 4316.299999999999),\n",
       " (29, 5032.529999999999),\n",
       " (91, 4642.259999999999),\n",
       " (70, 5368.249999999999),\n",
       " (85, 5503.43),\n",
       " (53, 4945.299999999999),\n",
       " (14, 4735.030000000001)]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "# Sum the amount per customer\n",
    "amount_per_customer = parsedLines.reduceByKey(add)\n",
    "\n",
    "# Print the top 10 rows\n",
    "amount_per_customer.collect()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__countByValue__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-Employment:', 'Building', 'an', 'Internet', 'Business']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have a list of words\n",
    "words.collect()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Self-Employment:': 1,\n",
       "             'Building': 5,\n",
       "             'an': 172,\n",
       "             'Internet': 13,\n",
       "             'Business': 19,\n",
       "             'of': 941,\n",
       "             'One': 12,\n",
       "             'Achieving': 1,\n",
       "             'Financial': 3,\n",
       "             'and': 901,\n",
       "             'Personal': 3,\n",
       "             'Freedom': 7,\n",
       "             'through': 55,\n",
       "             'a': 1148,\n",
       "             'Lifestyle': 5,\n",
       "             'Technology': 2,\n",
       "             'By': 9,\n",
       "             'Frank': 10,\n",
       "             'Kane': 7,\n",
       "             'Copyright': 1,\n",
       "             '�': 174,\n",
       "             '2015': 3,\n",
       "             'Kane.': 1,\n",
       "             'All': 13,\n",
       "             'rights': 3,\n",
       "             'reserved': 2,\n",
       "             'worldwide.': 2,\n",
       "             'CONTENTS': 1,\n",
       "             'Disclaimer': 1,\n",
       "             'Preface': 1,\n",
       "             'Part': 2,\n",
       "             'I:': 2,\n",
       "             'Making': 5,\n",
       "             'the': 1176,\n",
       "             'Big': 1,\n",
       "             'Decision': 1,\n",
       "             'Overcoming': 1,\n",
       "             'Inertia': 1,\n",
       "             'Fear': 1,\n",
       "             'Failure': 1,\n",
       "             'Career': 1,\n",
       "             'Indoctrination': 2,\n",
       "             'The': 88,\n",
       "             'Carrot': 1,\n",
       "             'on': 399,\n",
       "             'Stick': 2,\n",
       "             'Ego': 1,\n",
       "             'Protection': 1,\n",
       "             'Your': 62,\n",
       "             'Employer': 2,\n",
       "             'as': 297,\n",
       "             'Security': 2,\n",
       "             'Blanket': 1,\n",
       "             'Why': 3,\n",
       "             'it�s': 28,\n",
       "             'Worth': 1,\n",
       "             'it': 311,\n",
       "             'Unlimited': 2,\n",
       "             'Growth': 4,\n",
       "             'Potential': 1,\n",
       "             'Investing': 3,\n",
       "             'in': 552,\n",
       "             'Yourself,': 1,\n",
       "             'Not': 7,\n",
       "             'Someone': 2,\n",
       "             'Else': 1,\n",
       "             'No': 14,\n",
       "             'Dependencies': 1,\n",
       "             'Commute': 1,\n",
       "             'to': 1789,\n",
       "             'Live': 3,\n",
       "             'Where': 2,\n",
       "             'You': 144,\n",
       "             'Want': 5,\n",
       "             'Work': 4,\n",
       "             'When': 31,\n",
       "             'How': 29,\n",
       "             'Is': 17,\n",
       "             'Self-Employment': 1,\n",
       "             'for': 500,\n",
       "             'You?': 1,\n",
       "             'Flowchart:': 1,\n",
       "             'Should': 3,\n",
       "             'I': 322,\n",
       "             'Even': 35,\n",
       "             'Consider': 5,\n",
       "             'Self-Employment?': 2,\n",
       "             'Having': 2,\n",
       "             'Safety': 2,\n",
       "             'Net': 2,\n",
       "             'Planning': 3,\n",
       "             'Health': 2,\n",
       "             'Care': 2,\n",
       "             'Self-Assessment': 1,\n",
       "             'Quiz': 1,\n",
       "             'PART': 5,\n",
       "             'II:': 2,\n",
       "             'Happen': 1,\n",
       "             'Designing': 1,\n",
       "             'Fallacy': 1,\n",
       "             'Introducing': 1,\n",
       "             'Ideal': 1,\n",
       "             'Case': 1,\n",
       "             'Study:': 1,\n",
       "             'Sundog': 20,\n",
       "             'Software': 12,\n",
       "             'Other': 4,\n",
       "             'Ideas': 2,\n",
       "             'Key': 1,\n",
       "             'Points': 1,\n",
       "             'Evaluating': 1,\n",
       "             'Idea': 1,\n",
       "             'Writing': 4,\n",
       "             'Plan': 4,\n",
       "             'Description': 2,\n",
       "             'Vision': 2,\n",
       "             'Definition': 1,\n",
       "             'Market': 2,\n",
       "             'Product': 2,\n",
       "             'Organization': 1,\n",
       "             'Management': 2,\n",
       "             'Marketing': 5,\n",
       "             'Sales': 2,\n",
       "             'Strategy': 1,\n",
       "             'your': 1339,\n",
       "             'Reality': 1,\n",
       "             'Check': 4,\n",
       "             'Employment': 1,\n",
       "             'Agreement': 1,\n",
       "             'Developing': 1,\n",
       "             'Side': 1,\n",
       "             'Naming': 1,\n",
       "             'Obtaining': 1,\n",
       "             'Licenses': 1,\n",
       "             'Tax': 2,\n",
       "             'Launching': 2,\n",
       "             'Product,': 1,\n",
       "             'Measuring': 1,\n",
       "             'Results': 1,\n",
       "             'Risk': 1,\n",
       "             'Mitigation': 1,\n",
       "             \"Don't\": 22,\n",
       "             'Have': 9,\n",
       "             'Go': 7,\n",
       "             'It': 42,\n",
       "             'Alone': 1,\n",
       "             'Pulling': 1,\n",
       "             'Trigger': 1,\n",
       "             'Final': 1,\n",
       "             'Checklist': 1,\n",
       "             'Telling': 1,\n",
       "             'Boss': 1,\n",
       "             'III:': 2,\n",
       "             'Last': 1,\n",
       "             'Fueling': 1,\n",
       "             'Fire': 1,\n",
       "             'Website': 2,\n",
       "             'Basics': 5,\n",
       "             'Public': 1,\n",
       "             'Relations': 1,\n",
       "             'Online': 8,\n",
       "             'Advertising': 3,\n",
       "             'Offline': 1,\n",
       "             'Search': 6,\n",
       "             'Engine': 2,\n",
       "             'Optimization': 3,\n",
       "             'Landing': 4,\n",
       "             'Page': 2,\n",
       "             'Importance': 1,\n",
       "             'Email': 5,\n",
       "             'Campaigns': 1,\n",
       "             'Using': 5,\n",
       "             'Social': 2,\n",
       "             'Networks': 1,\n",
       "             'Measure': 1,\n",
       "             'Act': 2,\n",
       "             'Interpreting': 1,\n",
       "             'Metrics': 2,\n",
       "             'Tracking': 1,\n",
       "             'Closing': 1,\n",
       "             'Avoiding': 2,\n",
       "             'Pitfalls': 1,\n",
       "             'Beware': 5,\n",
       "             'Leeches': 1,\n",
       "             'Well-Meaning': 1,\n",
       "             'Advice': 1,\n",
       "             'Hiring': 3,\n",
       "             'Salespeople': 1,\n",
       "             'Pressure': 1,\n",
       "             'Hire': 1,\n",
       "             'Misleading': 1,\n",
       "             'Statistics': 4,\n",
       "             'Finding': 4,\n",
       "             'Time': 2,\n",
       "             'People': 4,\n",
       "             'Compounding': 1,\n",
       "             'Effect': 1,\n",
       "             'Adapting': 1,\n",
       "             'New': 6,\n",
       "             'Life': 2,\n",
       "             \"Survivor's\": 1,\n",
       "             'Guilt': 1,\n",
       "             'Letting': 1,\n",
       "             'Fixed': 1,\n",
       "             'Schedules': 1,\n",
       "             'Little': 1,\n",
       "             'Recommended': 1,\n",
       "             'Reading': 1,\n",
       "             'Acknowledgments': 1,\n",
       "             'About': 4,\n",
       "             'Author': 1,\n",
       "             'DISCLAIMER': 1,\n",
       "             'What': 38,\n",
       "             'you': 1267,\n",
       "             'do': 156,\n",
       "             'with': 292,\n",
       "             'career': 18,\n",
       "             'is': 531,\n",
       "             'own': 114,\n",
       "             'decision.': 1,\n",
       "             'This': 57,\n",
       "             'book': 31,\n",
       "             'tells': 4,\n",
       "             'how': 127,\n",
       "             'improved': 2,\n",
       "             'my': 199,\n",
       "             'life': 21,\n",
       "             'by': 109,\n",
       "             'quitting': 8,\n",
       "             'job,': 24,\n",
       "             'creating': 14,\n",
       "             'sustainable': 3,\n",
       "             'growing': 9,\n",
       "             'income': 17,\n",
       "             'self-employment.': 10,\n",
       "             'learned': 10,\n",
       "             'along': 10,\n",
       "             'way': 42,\n",
       "             'could': 44,\n",
       "             'help': 42,\n",
       "             'same.': 1,\n",
       "             'But': 38,\n",
       "             'no': 60,\n",
       "             'matter': 10,\n",
       "             'much': 78,\n",
       "             'advice': 34,\n",
       "             'or': 267,\n",
       "             'many': 63,\n",
       "             'stories': 3,\n",
       "             'give': 31,\n",
       "             'you,': 31,\n",
       "             'end': 39,\n",
       "             'success': 12,\n",
       "             'relies': 5,\n",
       "             'preparation,': 1,\n",
       "             'planning,': 2,\n",
       "             'skillset,': 1,\n",
       "             'talents,': 2,\n",
       "             'determination,': 1,\n",
       "             'little': 27,\n",
       "             'bit': 8,\n",
       "             'luck.': 1,\n",
       "             \"can't\": 22,\n",
       "             'all': 109,\n",
       "             'that': 641,\n",
       "             'you.': 53,\n",
       "             'If': 237,\n",
       "             'this': 208,\n",
       "             'gives': 5,\n",
       "             'confidence': 1,\n",
       "             'strike': 1,\n",
       "             'out': 138,\n",
       "             'that�s': 6,\n",
       "             'great.': 2,\n",
       "             'responsibly,': 2,\n",
       "             'realize': 10,\n",
       "             'failure': 1,\n",
       "             'very': 55,\n",
       "             'real': 27,\n",
       "             'possibility': 3,\n",
       "             'must': 28,\n",
       "             'plan': 43,\n",
       "             'for.': 9,\n",
       "             'blame': 1,\n",
       "             'me': 26,\n",
       "             'decisions': 5,\n",
       "             'make;': 1,\n",
       "             'they': 192,\n",
       "             'are': 391,\n",
       "             'ultimately': 7,\n",
       "             'own.': 14,\n",
       "             'Now,': 4,\n",
       "             'adventure.': 1,\n",
       "             'PREFACE': 1,\n",
       "             'Three': 2,\n",
       "             'years': 23,\n",
       "             'ago,': 3,\n",
       "             'worked': 16,\n",
       "             'big': 36,\n",
       "             'company.': 11,\n",
       "             'did': 12,\n",
       "             'everything': 8,\n",
       "             'good': 67,\n",
       "             'American': 4,\n",
       "             'employee': 6,\n",
       "             'supposed': 4,\n",
       "             'do.': 15,\n",
       "             'hard.': 3,\n",
       "             'devoted': 1,\n",
       "             'thoughts': 2,\n",
       "             'making': 14,\n",
       "             'company': 83,\n",
       "             'even': 68,\n",
       "             'more': 187,\n",
       "             'successful.': 3,\n",
       "             'commuted': 1,\n",
       "             'hour': 12,\n",
       "             'each': 18,\n",
       "             'way,': 7,\n",
       "             'without': 39,\n",
       "             'complaint.': 1,\n",
       "             '50': 4,\n",
       "             'hours': 33,\n",
       "             'every': 28,\n",
       "             'week': 17,\n",
       "             'at': 189,\n",
       "             'minimum.': 3,\n",
       "             'And': 14,\n",
       "             'when': 69,\n",
       "             'got': 23,\n",
       "             'home,': 8,\n",
       "             'let': 17,\n",
       "             'them': 108,\n",
       "             'page': 30,\n",
       "             '2': 4,\n",
       "             'AM': 4,\n",
       "             'if': 174,\n",
       "             'something': 50,\n",
       "             'needed': 7,\n",
       "             'attention': 14,\n",
       "             'which': 36,\n",
       "             'happened': 3,\n",
       "             'pretty': 18,\n",
       "             'often.': 2,\n",
       "             'Over': 2,\n",
       "             'years,': 4,\n",
       "             'was': 84,\n",
       "             'rewarded': 1,\n",
       "             'responsibility,': 2,\n",
       "             'money': 62,\n",
       "             'although': 5,\n",
       "             'never': 26,\n",
       "             'seemed': 2,\n",
       "             'be': 347,\n",
       "             'quite': 5,\n",
       "             'enough.': 3,\n",
       "             'considered': 4,\n",
       "             'role-model': 1,\n",
       "             'software': 33,\n",
       "             'development': 15,\n",
       "             'manager,': 2,\n",
       "             'felt': 4,\n",
       "             'At': 24,\n",
       "             'home': 14,\n",
       "             'however,': 10,\n",
       "             'marriage': 1,\n",
       "             'under': 7,\n",
       "             'stress,': 1,\n",
       "             'kids': 5,\n",
       "             'hardly': 2,\n",
       "             'knew': 6,\n",
       "             'me,': 3,\n",
       "             'prescription': 2,\n",
       "             'sleep': 3,\n",
       "             'aids': 2,\n",
       "             'order': 31,\n",
       "             'combat': 1,\n",
       "             'extreme': 2,\n",
       "             'stress': 8,\n",
       "             'under.': 1,\n",
       "             'My': 16,\n",
       "             'family': 19,\n",
       "             \"wasn't\": 5,\n",
       "             'happy': 13,\n",
       "             'living': 17,\n",
       "             'part': 25,\n",
       "             'country': 4,\n",
       "             'we': 18,\n",
       "             'lived': 2,\n",
       "             'in,': 6,\n",
       "             \"weren't\": 1,\n",
       "             'me.': 5,\n",
       "             'But,': 12,\n",
       "             'secondary': 1,\n",
       "             'duty': 2,\n",
       "             'employer,': 6,\n",
       "             'provider': 2,\n",
       "             'family.': 3,\n",
       "             'doing': 38,\n",
       "             'what': 177,\n",
       "             'thought': 4,\n",
       "             'had': 23,\n",
       "             'faced': 4,\n",
       "             'ultimatum': 1,\n",
       "             'from': 161,\n",
       "             'them.': 35,\n",
       "             'A': 39,\n",
       "             'few': 38,\n",
       "             'weeks': 1,\n",
       "             'later,': 4,\n",
       "             'quit': 19,\n",
       "             'job.': 19,\n",
       "             'walked': 1,\n",
       "             'away': 12,\n",
       "             'about': 181,\n",
       "             'million': 6,\n",
       "             'dollars�': 1,\n",
       "             'worth': 32,\n",
       "             'stock': 7,\n",
       "             'grants': 2,\n",
       "             'so,': 24,\n",
       "             'it.': 62,\n",
       "             'made': 12,\n",
       "             'decision': 7,\n",
       "             'try': 40,\n",
       "             'working': 52,\n",
       "             'myself': 9,\n",
       "             'before': 66,\n",
       "             'seeking': 6,\n",
       "             'another': 23,\n",
       "             'traditional': 9,\n",
       "             'Today,': 2,\n",
       "             'run': 15,\n",
       "             'home.': 4,\n",
       "             'have': 299,\n",
       "             'employees': 8,\n",
       "             'worry': 8,\n",
       "             'about,': 5,\n",
       "             'revenue': 33,\n",
       "             'stream': 7,\n",
       "             'matches': 1,\n",
       "             'anything': 18,\n",
       "             'ever': 9,\n",
       "             'successful': 14,\n",
       "             'senior': 3,\n",
       "             'manager': 7,\n",
       "             'tech': 5,\n",
       "             'now': 23,\n",
       "             'live': 17,\n",
       "             'someplace': 7,\n",
       "             'lower': 12,\n",
       "             'cost': 35,\n",
       "             'living,': 4,\n",
       "             \"don't\": 97,\n",
       "             'eat': 3,\n",
       "             'much,': 6,\n",
       "             'commuting': 4,\n",
       "             'costs': 14,\n",
       "             'deal': 6,\n",
       "             'with�': 1,\n",
       "             'so': 101,\n",
       "             'get': 116,\n",
       "             'keep': 35,\n",
       "             'money.': 12,\n",
       "             'entire': 15,\n",
       "             'happy,': 1,\n",
       "             \"we're\": 6,\n",
       "             'closer': 2,\n",
       "             'than': 92,\n",
       "             \"we've\": 4,\n",
       "             'been.': 1,\n",
       "             'anymore.': 1,\n",
       "             'growth,': 3,\n",
       "             'saying': 4,\n",
       "             'wrong': 9,\n",
       "             'thing': 23,\n",
       "             'some': 111,\n",
       "             'executive.': 1,\n",
       "             'commute': 7,\n",
       "             'meetings.': 2,\n",
       "             'office': 17,\n",
       "             '9': 5,\n",
       "             'AM,': 1,\n",
       "             'nor': 1,\n",
       "             'any': 60,\n",
       "             'artificial': 2,\n",
       "             'requirement': 1,\n",
       "             'work': 114,\n",
       "             'set': 25,\n",
       "             'number': 34,\n",
       "             'day,': 7,\n",
       "             'week.': 4,\n",
       "             'write': 6,\n",
       "             'receive': 10,\n",
       "             'performance': 9,\n",
       "             'reviews.': 1,\n",
       "             'fire': 4,\n",
       "             'anyone.': 2,\n",
       "             'Nobody': 6,\n",
       "             'can': 319,\n",
       "             'nobody': 7,\n",
       "             'lay': 2,\n",
       "             'off.': 3,\n",
       "             'take': 52,\n",
       "             'longer,': 2,\n",
       "             'because': 27,\n",
       "             'level': 7,\n",
       "             'basically': 11,\n",
       "             'zero.': 2,\n",
       "             \"I'm\": 25,\n",
       "             '43': 1,\n",
       "             'old,': 1,\n",
       "             'sure': 65,\n",
       "             'retirement': 2,\n",
       "             'feel': 23,\n",
       "             'like': 66,\n",
       "             'except': 2,\n",
       "             'not': 169,\n",
       "             'too': 24,\n",
       "             'old': 6,\n",
       "             'enjoy': 10,\n",
       "             'it,': 26,\n",
       "             'broke.': 1,\n",
       "             'telling': 3,\n",
       "             'gloat.': 1,\n",
       "             'maybe': 6,\n",
       "             \"you're\": 159,\n",
       "             'three': 6,\n",
       "             'ago.': 1,\n",
       "             'Maybe': 13,\n",
       "             'knows': 5,\n",
       "             'only': 77,\n",
       "             'reward': 5,\n",
       "             'hard': 39,\n",
       "             'going': 53,\n",
       "             'heart': 1,\n",
       "             'attack': 1,\n",
       "             'but': 192,\n",
       "             'trapped': 2,\n",
       "             'salary': 2,\n",
       "             'responsibilities.': 2,\n",
       "             \"You've\": 7,\n",
       "             'accepted': 3,\n",
       "             \"society's\": 1,\n",
       "             'expectations': 4,\n",
       "             'focusing': 5,\n",
       "             'entirely': 13,\n",
       "             'until': 15,\n",
       "             '65': 1,\n",
       "             'point': 20,\n",
       "             'hope': 5,\n",
       "             \"you'll\": 67,\n",
       "             'enough': 51,\n",
       "             'saved': 6,\n",
       "             'actually': 32,\n",
       "             'retire,': 1,\n",
       "             'health': 28,\n",
       "             'left': 6,\n",
       "             'here': 11,\n",
       "             'tell': 13,\n",
       "             'there': 82,\n",
       "             'path.': 3,\n",
       "             'Taking': 3,\n",
       "             \"isn't\": 22,\n",
       "             'easy,': 4,\n",
       "             'means': 27,\n",
       "             'personal': 43,\n",
       "             'financial': 12,\n",
       "             'freedom': 23,\n",
       "             'possible.': 12,\n",
       "             'while': 38,\n",
       "             'minimizing': 2,\n",
       "             'risks': 3,\n",
       "             'process.': 7,\n",
       "             'shares': 5,\n",
       "             \"I've\": 23,\n",
       "             'way.': 3,\n",
       "             'rich': 6,\n",
       "             'this,': 10,\n",
       "             'need': 166,\n",
       "             'developer.': 1,\n",
       "             'smarts': 1,\n",
       "             'tenacity': 1,\n",
       "             'self-sufficient,': 1,\n",
       "             'read': 12,\n",
       "             'may': 106,\n",
       "             'just': 131,\n",
       "             'save': 5,\n",
       "             'life,': 3,\n",
       "             'ways': 19,\n",
       "             'one.': 8,\n",
       "             'MAKING': 4,\n",
       "             'THE': 18,\n",
       "             'BIG': 1,\n",
       "             'DECISION': 1,\n",
       "             'OVERCOMING': 1,\n",
       "             'INERTIA': 1,\n",
       "             'first': 26,\n",
       "             'step': 3,\n",
       "             'toward': 11,\n",
       "             'self-employment': 38,\n",
       "             'want': 99,\n",
       "             \"you've\": 49,\n",
       "             'spent': 13,\n",
       "             'professional': 13,\n",
       "             'someone': 57,\n",
       "             'else,': 5,\n",
       "             'scary': 4,\n",
       "             'proposition.': 1,\n",
       "             'familiar': 4,\n",
       "             'comfortable,': 1,\n",
       "             'easiest': 2,\n",
       "             'continue': 7,\n",
       "             'current': 18,\n",
       "             'Discarding': 1,\n",
       "             'striking': 1,\n",
       "             'bold': 1,\n",
       "             'psyche': 1,\n",
       "             'rebel': 1,\n",
       "             'against': 12,\n",
       "             'change': 17,\n",
       "             'magnitude,': 1,\n",
       "             'find': 76,\n",
       "             'justify': 1,\n",
       "             'trying': 24,\n",
       "             \"It's\": 46,\n",
       "             'therefore': 2,\n",
       "             'surprising': 1,\n",
       "             'self-employed': 17,\n",
       "             'individuals': 3,\n",
       "             'were': 38,\n",
       "             'forced': 6,\n",
       "             'into': 75,\n",
       "             'their': 117,\n",
       "             'situation.': 5,\n",
       "             'ones': 15,\n",
       "             'become': 13,\n",
       "             'talk': 27,\n",
       "             'lot': 54,\n",
       "             'result,': 3,\n",
       "             'motivated': 3,\n",
       "             'achieve': 8,\n",
       "             'According': 5,\n",
       "             '2012': 7,\n",
       "             'Freelance': 4,\n",
       "             'Industry': 4,\n",
       "             'Report,': 2,\n",
       "             '29%': 2,\n",
       "             'freelancers': 9,\n",
       "             'fell': 1,\n",
       "             'necessity,': 1,\n",
       "             'after': 29,\n",
       "             'being': 29,\n",
       "             'laid': 1,\n",
       "             'off': 17,\n",
       "             'downsized.': 1,\n",
       "             'better': 40,\n",
       "             'transition': 3,\n",
       "             'terms.': 3,\n",
       "             'start': 39,\n",
       "             'planning': 8,\n",
       "             'up': 155,\n",
       "             'new': 143,\n",
       "             'business': 261,\n",
       "             'day': 56,\n",
       "             'ensure': 13,\n",
       "             'flowing': 2,\n",
       "             'yourself.': 13,\n",
       "             'will': 220,\n",
       "             'arm': 1,\n",
       "             'details': 3,\n",
       "             'make': 88,\n",
       "             'happen,': 4,\n",
       "             'decide': 19,\n",
       "             'effort': 18,\n",
       "             'taking.': 2,\n",
       "             'Before': 3,\n",
       "             'self-employed,': 9,\n",
       "             'understand': 30,\n",
       "             'forces': 3,\n",
       "             'come': 26,\n",
       "             'terms': 11,\n",
       "             'them,': 15,\n",
       "             'reasons': 1,\n",
       "             'why': 18,\n",
       "             'overcome': 2,\n",
       "             \"That's\": 18,\n",
       "             'about:': 1,\n",
       "             'understanding': 6,\n",
       "             'barriers': 3,\n",
       "             'self-employment,': 3,\n",
       "             'both': 11,\n",
       "             'imagined,': 1,\n",
       "             'still': 62,\n",
       "             'goal': 14,\n",
       "             'pursuing.': 1,\n",
       "             'done,': 3,\n",
       "             \"we'll\": 1,\n",
       "             'self-assessment': 2,\n",
       "             'see': 46,\n",
       "             'really': 39,\n",
       "             'position': 9,\n",
       "             'leap': 2,\n",
       "             'FEAR': 1,\n",
       "             'OF': 16,\n",
       "             'FAILURE': 1,\n",
       "             'paycheck': 7,\n",
       "             'puts': 2,\n",
       "             'food': 2,\n",
       "             'table': 2,\n",
       "             'roof': 1,\n",
       "             'over': 58,\n",
       "             'head': 2,\n",
       "             'reliably.': 1,\n",
       "             'idea': 44,\n",
       "             'giving': 15,\n",
       "             'quickly': 12,\n",
       "             'brings': 3,\n",
       "             'images': 6,\n",
       "             'starving': 1,\n",
       "             'house': 7,\n",
       "             'getting': 28,\n",
       "             'foreclosed.': 1,\n",
       "             'risk,': 1,\n",
       "             'proper': 3,\n",
       "             'educated': 2,\n",
       "             'making,': 1,\n",
       "             'risk': 12,\n",
       "             'minimize.': 1,\n",
       "             'responsibilities': 8,\n",
       "             'should': 63,\n",
       "             'job': 43,\n",
       "             'until:': 1,\n",
       "             '*': 22,\n",
       "             'safety': 4,\n",
       "             'net': 10,\n",
       "             'place,': 2,\n",
       "             'fast': 4,\n",
       "             'rules': 4,\n",
       "             'low': 8,\n",
       "             'willing': 11,\n",
       "             'those': 63,\n",
       "             'reserves': 5,\n",
       "             'returning': 1,\n",
       "             'workplace': 2,\n",
       "             'already': 20,\n",
       "             'prototyped': 1,\n",
       "             'side': 24,\n",
       "             'proven': 1,\n",
       "             'viable': 6,\n",
       "             'So,': 14,\n",
       "             'upper': 7,\n",
       "             'bound': 10,\n",
       "             'lose': 8,\n",
       "             'calling': 1,\n",
       "             'quits.': 1,\n",
       "             'Worst': 1,\n",
       "             'case,': 10,\n",
       "             'things': 34,\n",
       "             'risky': 5,\n",
       "             'entrepreneurship?': 1,\n",
       "             'Bureau': 2,\n",
       "             'Labor': 2,\n",
       "             '(BLS)': 1,\n",
       "             'US': 14,\n",
       "             'Small': 3,\n",
       "             'Administration': 2,\n",
       "             '(SBA),': 1,\n",
       "             '50%': 5,\n",
       "             'businesses': 31,\n",
       "             'survive': 2,\n",
       "             'least': 31,\n",
       "             'five': 5,\n",
       "             'years.': 9,\n",
       "             'third': 3,\n",
       "             'ten': 5,\n",
       "             'more.': 5,\n",
       "             'think': 37,\n",
       "             'long': 40,\n",
       "             'expect': 10,\n",
       "             'last.': 1,\n",
       "             'Would': 2,\n",
       "             'say': 19,\n",
       "             \"there's\": 7,\n",
       "             'chance': 12,\n",
       "             'employer': 25,\n",
       "             'around': 15,\n",
       "             'now,': 6,\n",
       "             'same': 28,\n",
       "             'company?': 3,\n",
       "             'has': 40,\n",
       "             'answer': 6,\n",
       "             'well.': 20,\n",
       "             'average': 9,\n",
       "             'tenure': 1,\n",
       "             '4.6': 1,\n",
       "             'Does': 2,\n",
       "             'starting': 10,\n",
       "             'seem': 11,\n",
       "             'now?': 3,\n",
       "             \"Here's\": 7,\n",
       "             'statistic': 1,\n",
       "             'Report': 1,\n",
       "             'shows': 3,\n",
       "             '22%': 1,\n",
       "             'less': 23,\n",
       "             'financially': 4,\n",
       "             'secure': 1,\n",
       "             'employee.': 2,\n",
       "             '28%': 1,\n",
       "             '\"strongly': 1,\n",
       "             'agree\"': 1,\n",
       "             'themselves,': 4,\n",
       "             '\"moderately': 1,\n",
       "             'agree.\"': 1,\n",
       "             'Freelancers': 1,\n",
       "             'represent': 2,\n",
       "             \"it's\": 107,\n",
       "             'always': 16,\n",
       "             'option': 6,\n",
       "             '22': 1,\n",
       "             'Americans': 1,\n",
       "             '2010,': 1,\n",
       "             'representing': 1,\n",
       "             '14%': 1,\n",
       "             'workforce.': 1,\n",
       "             'fringe': 1,\n",
       "             'They': 26,\n",
       "             'range': 3,\n",
       "             'housekeepers,': 2,\n",
       "             'construction': 2,\n",
       "             'workers,': 3,\n",
       "             'web': 28,\n",
       "             'developers,': 5,\n",
       "             'doctors.': 1,\n",
       "             'fancy': 2,\n",
       "             'education': 2,\n",
       "             'lots': 7,\n",
       "             'connections': 6,\n",
       "             'advantage': 11,\n",
       "             'millions': 2,\n",
       "             'who': 85,\n",
       "             'managed': 4,\n",
       "             'full': 18,\n",
       "             'time': 165,\n",
       "             'earn': 16,\n",
       "             'living.': 3,\n",
       "             'CAREER': 1,\n",
       "             'INDOCTRINATION': 1,\n",
       "             'spend': 42,\n",
       "             'most': 61,\n",
       "             'waking': 2,\n",
       "             'possible': 10,\n",
       "             'somebody': 2,\n",
       "             'else': 14,\n",
       "             'richer?': 1,\n",
       "             'Well,': 2,\n",
       "             'raised': 2,\n",
       "             'believe': 15,\n",
       "             'worthy': 3,\n",
       "             \"We'll\": 4,\n",
       "             'external': 2,\n",
       "             'biggest': 2,\n",
       "             'barrier': 2,\n",
       "             'probably': 76,\n",
       "             'internal': 1,\n",
       "             'do!': 1,\n",
       "             'strong': 4,\n",
       "             'word,': 1,\n",
       "             'usually': 14,\n",
       "             'religious': 1,\n",
       "             'cults': 1,\n",
       "             'brainwashing.': 1,\n",
       "             'taught': 3,\n",
       "             'beliefs': 1,\n",
       "             'questioning': 1,\n",
       "             'Just': 11,\n",
       "             'question': 8,\n",
       "             'responsible': 4,\n",
       "             'member': 2,\n",
       "             'society?': 1,\n",
       "             'common': 8,\n",
       "             'indoctrination': 1,\n",
       "             'instill': 1,\n",
       "             'ideas': 19,\n",
       "             'youth': 1,\n",
       "             'people': 122,\n",
       "             'authority.': 1,\n",
       "             'That�s': 2,\n",
       "             'precisely': 2,\n",
       "             'grew': 1,\n",
       "             'up,': 4,\n",
       "             'pushed': 2,\n",
       "             'track': 7,\n",
       "             'corporate': 17,\n",
       "             'school,': 1,\n",
       "             'told': 5,\n",
       "             'parents': 2,\n",
       "             'teachers': 1,\n",
       "             'grades': 1,\n",
       "             'college.': 2,\n",
       "             'In': 41,\n",
       "             'college,': 1,\n",
       "             'would': 32,\n",
       "             'graduated.': 1,\n",
       "             'lucky': 2,\n",
       "             'absorbed': 1,\n",
       "             'training': 9,\n",
       "             'culture': 1,\n",
       "             'promotes': 1,\n",
       "             'loyalty': 2,\n",
       "             'employer.': 3,\n",
       "             'It�s': 6,\n",
       "             'result': 17,\n",
       "             'lifestyle': 29,\n",
       "             'choice': 5,\n",
       "             'indoctrinated': 1,\n",
       "             'early': 11,\n",
       "             'age.': 1,\n",
       "             'Once': 23,\n",
       "             'landed': 1,\n",
       "             'world,': 5,\n",
       "             'next': 15,\n",
       "             'promotion,': 3,\n",
       "             'fired.': 1,\n",
       "             'been': 24,\n",
       "             'busy': 2,\n",
       "             'following': 8,\n",
       "             'path,': 1,\n",
       "             \"haven't\": 6,\n",
       "             'consider': 20,\n",
       "             'alternatives.': 2,\n",
       "             'Meanwhile,': 1,\n",
       "             'picked': 2,\n",
       "             'children,': 1,\n",
       "             'mortgage,': 1,\n",
       "             'student': 3,\n",
       "             'loan': 7,\n",
       "             'debt': 4,\n",
       "             'steady': 2,\n",
       "             'seems': 5,\n",
       "             'terrifying,': 1,\n",
       "             'barely': 1,\n",
       "             'meet': 8,\n",
       "             'demands.': 1,\n",
       "             'bills': 7,\n",
       "             'pay': 43,\n",
       "             'mouths': 1,\n",
       "             'feed,': 1,\n",
       "             'having': 27,\n",
       "             'look': 23,\n",
       "             'forward': 1,\n",
       "             'morning': 1,\n",
       "             'anyone': 6,\n",
       "             'does.': 3,\n",
       "             'best': 39,\n",
       "             'fulfill': 1,\n",
       "             'thrust': 1,\n",
       "             'upon': 6,\n",
       "             'year': 17,\n",
       "             'year.': 10,\n",
       "             'hear': 9,\n",
       "             'businesses,': 7,\n",
       "             'beyond': 7,\n",
       "             'buying': 4,\n",
       "             'foam': 1,\n",
       "             'cup': 1,\n",
       "             'Ramen': 1,\n",
       "             'noodles.': 1,\n",
       "             'You�ve': 1,\n",
       "             'developed': 3,\n",
       "             'known': 9,\n",
       "             '�learned': 1,\n",
       "             'helplessness�': 1,\n",
       "             'aren�t': 2,\n",
       "             'stressful': 2,\n",
       "             'career,': 9,\n",
       "             'available': 19,\n",
       "             'not.': 8,\n",
       "             'As': 45,\n",
       "             'breaking': 2,\n",
       "             'indoctrination,': 1,\n",
       "             'there,': 7,\n",
       "             'develop': 7,\n",
       "             'desire': 1,\n",
       "             'alternate': 1,\n",
       "             'paths.': 1,\n",
       "             'Striking': 1,\n",
       "             'becoming': 4,\n",
       "             'one': 69,\n",
       "             'such': 39,\n",
       "             'path': 15,\n",
       "             'improve': 3,\n",
       "             'lifestyle,': 1,\n",
       "             'done': 17,\n",
       "             'responsibly.': 1,\n",
       "             'CARROT': 1,\n",
       "             'ON': 2,\n",
       "             'STICK': 1,\n",
       "             'where': 49,\n",
       "             ...})"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count unique values into a dictionary\n",
    "words_dict = words.countByValue()\n",
    "words_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__sortBy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 94.79), (35, 28.2), (2, 56.05), (47, 75.45), (29, 46.37)]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount_per_customer.collect()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort it by the second column(amount paid)\n",
    "sorted_amount_per_customer = amount_per_customer.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(83, 98.32), (54, 98.29), (77, 98.29), (42, 95.58), (44, 94.79)]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_amount_per_customer.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a rating historgram by counting values from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6110\n",
      "2 11370\n",
      "3 27145\n",
      "4 34174\n",
      "5 21201\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "# Settings configurations\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# Loading the file  u.data\n",
    "lines = sc.textFile(\"u.data\")\n",
    "\n",
    "# Parsing it\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "\n",
    "# Counting ratings\n",
    "result = ratings.countByValue()\n",
    "\n",
    "# Sorting results\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like the most frequent rating is 4 starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the average number of friends per age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 325.3333333333333)\n",
      "(26, 242.05882352941177)\n",
      "(55, 295.53846153846155)\n",
      "(40, 250.8235294117647)\n",
      "(68, 269.6)\n",
      "(59, 220.0)\n",
      "(37, 249.33333333333334)\n",
      "(54, 278.0769230769231)\n",
      "(38, 193.53333333333333)\n",
      "(27, 228.125)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Configure and start a context manager\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# Take the age and number of friends\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "# Sum the number of friends per age\n",
    "lines = sc.textFile(\"fakefriends.csv\")\n",
    "rdd = lines.map(parseLine)\n",
    "totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# Take the average number of friends\n",
    "averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
    "\n",
    "# Create a list of that result\n",
    "results = averagesByAge.collect()\n",
    "\n",
    "# Print a sample of 10 results\n",
    "counter = 0\n",
    "for result in results:\n",
    "    \n",
    "    print(result)\n",
    "    counter += 1\n",
    "    if counter == 10:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the minimum temperature by station id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t5.36F\n",
      "EZE00100082\t7.70F\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile(\"1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the maximum temperature for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Context with configurations\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MaxTemperatures\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ITE00100554,18000101,TMAX,-75,,,E,',\n",
       " 'ITE00100554,18000101,TMIN,-148,,,E,',\n",
       " 'GM000010962,18000101,PRCP,0,,,E,',\n",
       " 'EZE00100082,18000101,TMAX,-86,,,E,',\n",
       " 'EZE00100082,18000101,TMIN,-135,,,E,']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets create an rdd and look at a sample\n",
    "lines = sc.textFile('1800.csv')\n",
    "lines.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, each line is a string, so ill need to turn them into separate items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLines(line):\n",
    "    field = line.split(',')\n",
    "    station_id = field[0]\n",
    "    entryType = field[2]\n",
    "    temp = float(field[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    \n",
    "    return (station_id, entryType, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedLines = lines.map(parseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 'TMAX', 18.5),\n",
       " ('ITE00100554', 'TMIN', 5.359999999999999),\n",
       " ('GM000010962', 'PRCP', 32.0),\n",
       " ('EZE00100082', 'TMAX', 16.52),\n",
       " ('EZE00100082', 'TMIN', 7.699999999999999)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedLines.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So the maxium values are rows where there's TMAX in them, lets extract the maximum value for each unique key(i.e station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 'TMAX', 18.5),\n",
       " ('EZE00100082', 'TMAX', 16.52),\n",
       " ('ITE00100554', 'TMAX', 21.2),\n",
       " ('EZE00100082', 'TMAX', 24.08),\n",
       " ('ITE00100554', 'TMAX', 27.86)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxTemps = parsedLines.filter(lambda x: \"TMAX\" in x[1])\n",
    "maxTemps.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets filter only the required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationTemps = maxTemps.map(lambda x: (x[0], x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 18.5),\n",
       " ('EZE00100082', 16.52),\n",
       " ('ITE00100554', 21.2),\n",
       " ('EZE00100082', 24.08),\n",
       " ('ITE00100554', 27.86)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationTemps.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets find the maximum values for each UNIUQE station\n",
    "- Ill need to use the reduceByKey action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTemps = stationTemps.reduceByKey(lambda x, y: max(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 90.14000000000001), ('EZE00100082', 90.14000000000001)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxTemps.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So the maximum temp was for two stations which is 90.14 Farenheight "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Word Occurences using Flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and setting a Spark Context\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MaxTemperatures\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-Employment: Building an Internet Business of One',\n",
       " 'Achieving Financial and Personal Freedom through a Lifestyle Technology Business',\n",
       " 'By Frank Kane',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Copyright � 2015 Frank Kane. ',\n",
       " 'All rights reserved worldwide.',\n",
       " '',\n",
       " '',\n",
       " 'CONTENTS',\n",
       " 'Disclaimer',\n",
       " 'Preface',\n",
       " 'Part I: Making the Big Decision',\n",
       " 'Overcoming Inertia',\n",
       " 'Fear of Failure',\n",
       " 'Career Indoctrination',\n",
       " 'The Carrot on a Stick',\n",
       " 'Ego Protection',\n",
       " 'Your Employer as a Security Blanket',\n",
       " 'Why it�s Worth it',\n",
       " 'Unlimited Growth Potential',\n",
       " 'Investing in Yourself, Not Someone Else',\n",
       " 'No Dependencies',\n",
       " 'No Commute']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an RDD from a book\n",
    "book = sc.textFile('Book')\n",
    "book.collect()[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use flatmap to divide all of these words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['No Dependencies, ', 'No Commute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-Employment:',\n",
       " 'Building',\n",
       " 'an',\n",
       " 'Internet',\n",
       " 'Business',\n",
       " 'of',\n",
       " 'One',\n",
       " 'Achieving',\n",
       " 'Financial',\n",
       " 'and',\n",
       " 'Personal',\n",
       " 'Freedom',\n",
       " 'through',\n",
       " 'a',\n",
       " 'Lifestyle',\n",
       " 'Technology',\n",
       " 'Business',\n",
       " 'By',\n",
       " 'Frank',\n",
       " 'Kane',\n",
       " 'Copyright',\n",
       " '�',\n",
       " '2015',\n",
       " 'Frank',\n",
       " 'Kane.']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use flatMap to split the lines into words\n",
    "words = book.flatMap(lambda x: x.split())\n",
    "words.collect()[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets count by unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Self-Employment:': 1,\n",
       "             'Building': 5,\n",
       "             'an': 172,\n",
       "             'Internet': 13,\n",
       "             'Business': 19,\n",
       "             'of': 941,\n",
       "             'One': 12,\n",
       "             'Achieving': 1,\n",
       "             'Financial': 3,\n",
       "             'and': 901,\n",
       "             'Personal': 3,\n",
       "             'Freedom': 7,\n",
       "             'through': 55,\n",
       "             'a': 1148,\n",
       "             'Lifestyle': 5,\n",
       "             'Technology': 2,\n",
       "             'By': 9,\n",
       "             'Frank': 10,\n",
       "             'Kane': 7,\n",
       "             'Copyright': 1,\n",
       "             '�': 174,\n",
       "             '2015': 3,\n",
       "             'Kane.': 1,\n",
       "             'All': 13,\n",
       "             'rights': 3,\n",
       "             'reserved': 2,\n",
       "             'worldwide.': 2,\n",
       "             'CONTENTS': 1,\n",
       "             'Disclaimer': 1,\n",
       "             'Preface': 1,\n",
       "             'Part': 2,\n",
       "             'I:': 2,\n",
       "             'Making': 5,\n",
       "             'the': 1176,\n",
       "             'Big': 1,\n",
       "             'Decision': 1,\n",
       "             'Overcoming': 1,\n",
       "             'Inertia': 1,\n",
       "             'Fear': 1,\n",
       "             'Failure': 1,\n",
       "             'Career': 1,\n",
       "             'Indoctrination': 2,\n",
       "             'The': 88,\n",
       "             'Carrot': 1,\n",
       "             'on': 399,\n",
       "             'Stick': 2,\n",
       "             'Ego': 1,\n",
       "             'Protection': 1,\n",
       "             'Your': 62,\n",
       "             'Employer': 2,\n",
       "             'as': 297,\n",
       "             'Security': 2,\n",
       "             'Blanket': 1,\n",
       "             'Why': 3,\n",
       "             'it�s': 28,\n",
       "             'Worth': 1,\n",
       "             'it': 311,\n",
       "             'Unlimited': 2,\n",
       "             'Growth': 4,\n",
       "             'Potential': 1,\n",
       "             'Investing': 3,\n",
       "             'in': 552,\n",
       "             'Yourself,': 1,\n",
       "             'Not': 7,\n",
       "             'Someone': 2,\n",
       "             'Else': 1,\n",
       "             'No': 14,\n",
       "             'Dependencies': 1,\n",
       "             'Commute': 1,\n",
       "             'to': 1789,\n",
       "             'Live': 3,\n",
       "             'Where': 2,\n",
       "             'You': 144,\n",
       "             'Want': 5,\n",
       "             'Work': 4,\n",
       "             'When': 31,\n",
       "             'How': 29,\n",
       "             'Is': 17,\n",
       "             'Self-Employment': 1,\n",
       "             'for': 500,\n",
       "             'You?': 1,\n",
       "             'Flowchart:': 1,\n",
       "             'Should': 3,\n",
       "             'I': 322,\n",
       "             'Even': 35,\n",
       "             'Consider': 5,\n",
       "             'Self-Employment?': 2,\n",
       "             'Having': 2,\n",
       "             'Safety': 2,\n",
       "             'Net': 2,\n",
       "             'Planning': 3,\n",
       "             'Health': 2,\n",
       "             'Care': 2,\n",
       "             'Self-Assessment': 1,\n",
       "             'Quiz': 1,\n",
       "             'PART': 5,\n",
       "             'II:': 2,\n",
       "             'Happen': 1,\n",
       "             'Designing': 1,\n",
       "             'Fallacy': 1,\n",
       "             'Introducing': 1,\n",
       "             'Ideal': 1,\n",
       "             'Case': 1,\n",
       "             'Study:': 1,\n",
       "             'Sundog': 20,\n",
       "             'Software': 12,\n",
       "             'Other': 4,\n",
       "             'Ideas': 2,\n",
       "             'Key': 1,\n",
       "             'Points': 1,\n",
       "             'Evaluating': 1,\n",
       "             'Idea': 1,\n",
       "             'Writing': 4,\n",
       "             'Plan': 4,\n",
       "             'Description': 2,\n",
       "             'Vision': 2,\n",
       "             'Definition': 1,\n",
       "             'Market': 2,\n",
       "             'Product': 2,\n",
       "             'Organization': 1,\n",
       "             'Management': 2,\n",
       "             'Marketing': 5,\n",
       "             'Sales': 2,\n",
       "             'Strategy': 1,\n",
       "             'your': 1339,\n",
       "             'Reality': 1,\n",
       "             'Check': 4,\n",
       "             'Employment': 1,\n",
       "             'Agreement': 1,\n",
       "             'Developing': 1,\n",
       "             'Side': 1,\n",
       "             'Naming': 1,\n",
       "             'Obtaining': 1,\n",
       "             'Licenses': 1,\n",
       "             'Tax': 2,\n",
       "             'Launching': 2,\n",
       "             'Product,': 1,\n",
       "             'Measuring': 1,\n",
       "             'Results': 1,\n",
       "             'Risk': 1,\n",
       "             'Mitigation': 1,\n",
       "             \"Don't\": 22,\n",
       "             'Have': 9,\n",
       "             'Go': 7,\n",
       "             'It': 42,\n",
       "             'Alone': 1,\n",
       "             'Pulling': 1,\n",
       "             'Trigger': 1,\n",
       "             'Final': 1,\n",
       "             'Checklist': 1,\n",
       "             'Telling': 1,\n",
       "             'Boss': 1,\n",
       "             'III:': 2,\n",
       "             'Last': 1,\n",
       "             'Fueling': 1,\n",
       "             'Fire': 1,\n",
       "             'Website': 2,\n",
       "             'Basics': 5,\n",
       "             'Public': 1,\n",
       "             'Relations': 1,\n",
       "             'Online': 8,\n",
       "             'Advertising': 3,\n",
       "             'Offline': 1,\n",
       "             'Search': 6,\n",
       "             'Engine': 2,\n",
       "             'Optimization': 3,\n",
       "             'Landing': 4,\n",
       "             'Page': 2,\n",
       "             'Importance': 1,\n",
       "             'Email': 5,\n",
       "             'Campaigns': 1,\n",
       "             'Using': 5,\n",
       "             'Social': 2,\n",
       "             'Networks': 1,\n",
       "             'Measure': 1,\n",
       "             'Act': 2,\n",
       "             'Interpreting': 1,\n",
       "             'Metrics': 2,\n",
       "             'Tracking': 1,\n",
       "             'Closing': 1,\n",
       "             'Avoiding': 2,\n",
       "             'Pitfalls': 1,\n",
       "             'Beware': 5,\n",
       "             'Leeches': 1,\n",
       "             'Well-Meaning': 1,\n",
       "             'Advice': 1,\n",
       "             'Hiring': 3,\n",
       "             'Salespeople': 1,\n",
       "             'Pressure': 1,\n",
       "             'Hire': 1,\n",
       "             'Misleading': 1,\n",
       "             'Statistics': 4,\n",
       "             'Finding': 4,\n",
       "             'Time': 2,\n",
       "             'People': 4,\n",
       "             'Compounding': 1,\n",
       "             'Effect': 1,\n",
       "             'Adapting': 1,\n",
       "             'New': 6,\n",
       "             'Life': 2,\n",
       "             \"Survivor's\": 1,\n",
       "             'Guilt': 1,\n",
       "             'Letting': 1,\n",
       "             'Fixed': 1,\n",
       "             'Schedules': 1,\n",
       "             'Little': 1,\n",
       "             'Recommended': 1,\n",
       "             'Reading': 1,\n",
       "             'Acknowledgments': 1,\n",
       "             'About': 4,\n",
       "             'Author': 1,\n",
       "             'DISCLAIMER': 1,\n",
       "             'What': 38,\n",
       "             'you': 1267,\n",
       "             'do': 156,\n",
       "             'with': 292,\n",
       "             'career': 18,\n",
       "             'is': 531,\n",
       "             'own': 114,\n",
       "             'decision.': 1,\n",
       "             'This': 57,\n",
       "             'book': 31,\n",
       "             'tells': 4,\n",
       "             'how': 127,\n",
       "             'improved': 2,\n",
       "             'my': 199,\n",
       "             'life': 21,\n",
       "             'by': 109,\n",
       "             'quitting': 8,\n",
       "             'job,': 24,\n",
       "             'creating': 14,\n",
       "             'sustainable': 3,\n",
       "             'growing': 9,\n",
       "             'income': 17,\n",
       "             'self-employment.': 10,\n",
       "             'learned': 10,\n",
       "             'along': 10,\n",
       "             'way': 42,\n",
       "             'could': 44,\n",
       "             'help': 42,\n",
       "             'same.': 1,\n",
       "             'But': 38,\n",
       "             'no': 60,\n",
       "             'matter': 10,\n",
       "             'much': 78,\n",
       "             'advice': 34,\n",
       "             'or': 267,\n",
       "             'many': 63,\n",
       "             'stories': 3,\n",
       "             'give': 31,\n",
       "             'you,': 31,\n",
       "             'end': 39,\n",
       "             'success': 12,\n",
       "             'relies': 5,\n",
       "             'preparation,': 1,\n",
       "             'planning,': 2,\n",
       "             'skillset,': 1,\n",
       "             'talents,': 2,\n",
       "             'determination,': 1,\n",
       "             'little': 27,\n",
       "             'bit': 8,\n",
       "             'luck.': 1,\n",
       "             \"can't\": 22,\n",
       "             'all': 109,\n",
       "             'that': 641,\n",
       "             'you.': 53,\n",
       "             'If': 237,\n",
       "             'this': 208,\n",
       "             'gives': 5,\n",
       "             'confidence': 1,\n",
       "             'strike': 1,\n",
       "             'out': 138,\n",
       "             'that�s': 6,\n",
       "             'great.': 2,\n",
       "             'responsibly,': 2,\n",
       "             'realize': 10,\n",
       "             'failure': 1,\n",
       "             'very': 55,\n",
       "             'real': 27,\n",
       "             'possibility': 3,\n",
       "             'must': 28,\n",
       "             'plan': 43,\n",
       "             'for.': 9,\n",
       "             'blame': 1,\n",
       "             'me': 26,\n",
       "             'decisions': 5,\n",
       "             'make;': 1,\n",
       "             'they': 192,\n",
       "             'are': 391,\n",
       "             'ultimately': 7,\n",
       "             'own.': 14,\n",
       "             'Now,': 4,\n",
       "             'adventure.': 1,\n",
       "             'PREFACE': 1,\n",
       "             'Three': 2,\n",
       "             'years': 23,\n",
       "             'ago,': 3,\n",
       "             'worked': 16,\n",
       "             'big': 36,\n",
       "             'company.': 11,\n",
       "             'did': 12,\n",
       "             'everything': 8,\n",
       "             'good': 67,\n",
       "             'American': 4,\n",
       "             'employee': 6,\n",
       "             'supposed': 4,\n",
       "             'do.': 15,\n",
       "             'hard.': 3,\n",
       "             'devoted': 1,\n",
       "             'thoughts': 2,\n",
       "             'making': 14,\n",
       "             'company': 83,\n",
       "             'even': 68,\n",
       "             'more': 187,\n",
       "             'successful.': 3,\n",
       "             'commuted': 1,\n",
       "             'hour': 12,\n",
       "             'each': 18,\n",
       "             'way,': 7,\n",
       "             'without': 39,\n",
       "             'complaint.': 1,\n",
       "             '50': 4,\n",
       "             'hours': 33,\n",
       "             'every': 28,\n",
       "             'week': 17,\n",
       "             'at': 189,\n",
       "             'minimum.': 3,\n",
       "             'And': 14,\n",
       "             'when': 69,\n",
       "             'got': 23,\n",
       "             'home,': 8,\n",
       "             'let': 17,\n",
       "             'them': 108,\n",
       "             'page': 30,\n",
       "             '2': 4,\n",
       "             'AM': 4,\n",
       "             'if': 174,\n",
       "             'something': 50,\n",
       "             'needed': 7,\n",
       "             'attention': 14,\n",
       "             'which': 36,\n",
       "             'happened': 3,\n",
       "             'pretty': 18,\n",
       "             'often.': 2,\n",
       "             'Over': 2,\n",
       "             'years,': 4,\n",
       "             'was': 84,\n",
       "             'rewarded': 1,\n",
       "             'responsibility,': 2,\n",
       "             'money': 62,\n",
       "             'although': 5,\n",
       "             'never': 26,\n",
       "             'seemed': 2,\n",
       "             'be': 347,\n",
       "             'quite': 5,\n",
       "             'enough.': 3,\n",
       "             'considered': 4,\n",
       "             'role-model': 1,\n",
       "             'software': 33,\n",
       "             'development': 15,\n",
       "             'manager,': 2,\n",
       "             'felt': 4,\n",
       "             'At': 24,\n",
       "             'home': 14,\n",
       "             'however,': 10,\n",
       "             'marriage': 1,\n",
       "             'under': 7,\n",
       "             'stress,': 1,\n",
       "             'kids': 5,\n",
       "             'hardly': 2,\n",
       "             'knew': 6,\n",
       "             'me,': 3,\n",
       "             'prescription': 2,\n",
       "             'sleep': 3,\n",
       "             'aids': 2,\n",
       "             'order': 31,\n",
       "             'combat': 1,\n",
       "             'extreme': 2,\n",
       "             'stress': 8,\n",
       "             'under.': 1,\n",
       "             'My': 16,\n",
       "             'family': 19,\n",
       "             \"wasn't\": 5,\n",
       "             'happy': 13,\n",
       "             'living': 17,\n",
       "             'part': 25,\n",
       "             'country': 4,\n",
       "             'we': 18,\n",
       "             'lived': 2,\n",
       "             'in,': 6,\n",
       "             \"weren't\": 1,\n",
       "             'me.': 5,\n",
       "             'But,': 12,\n",
       "             'secondary': 1,\n",
       "             'duty': 2,\n",
       "             'employer,': 6,\n",
       "             'provider': 2,\n",
       "             'family.': 3,\n",
       "             'doing': 38,\n",
       "             'what': 177,\n",
       "             'thought': 4,\n",
       "             'had': 23,\n",
       "             'faced': 4,\n",
       "             'ultimatum': 1,\n",
       "             'from': 161,\n",
       "             'them.': 35,\n",
       "             'A': 39,\n",
       "             'few': 38,\n",
       "             'weeks': 1,\n",
       "             'later,': 4,\n",
       "             'quit': 19,\n",
       "             'job.': 19,\n",
       "             'walked': 1,\n",
       "             'away': 12,\n",
       "             'about': 181,\n",
       "             'million': 6,\n",
       "             'dollars�': 1,\n",
       "             'worth': 32,\n",
       "             'stock': 7,\n",
       "             'grants': 2,\n",
       "             'so,': 24,\n",
       "             'it.': 62,\n",
       "             'made': 12,\n",
       "             'decision': 7,\n",
       "             'try': 40,\n",
       "             'working': 52,\n",
       "             'myself': 9,\n",
       "             'before': 66,\n",
       "             'seeking': 6,\n",
       "             'another': 23,\n",
       "             'traditional': 9,\n",
       "             'Today,': 2,\n",
       "             'run': 15,\n",
       "             'home.': 4,\n",
       "             'have': 299,\n",
       "             'employees': 8,\n",
       "             'worry': 8,\n",
       "             'about,': 5,\n",
       "             'revenue': 33,\n",
       "             'stream': 7,\n",
       "             'matches': 1,\n",
       "             'anything': 18,\n",
       "             'ever': 9,\n",
       "             'successful': 14,\n",
       "             'senior': 3,\n",
       "             'manager': 7,\n",
       "             'tech': 5,\n",
       "             'now': 23,\n",
       "             'live': 17,\n",
       "             'someplace': 7,\n",
       "             'lower': 12,\n",
       "             'cost': 35,\n",
       "             'living,': 4,\n",
       "             \"don't\": 97,\n",
       "             'eat': 3,\n",
       "             'much,': 6,\n",
       "             'commuting': 4,\n",
       "             'costs': 14,\n",
       "             'deal': 6,\n",
       "             'with�': 1,\n",
       "             'so': 101,\n",
       "             'get': 116,\n",
       "             'keep': 35,\n",
       "             'money.': 12,\n",
       "             'entire': 15,\n",
       "             'happy,': 1,\n",
       "             \"we're\": 6,\n",
       "             'closer': 2,\n",
       "             'than': 92,\n",
       "             \"we've\": 4,\n",
       "             'been.': 1,\n",
       "             'anymore.': 1,\n",
       "             'growth,': 3,\n",
       "             'saying': 4,\n",
       "             'wrong': 9,\n",
       "             'thing': 23,\n",
       "             'some': 111,\n",
       "             'executive.': 1,\n",
       "             'commute': 7,\n",
       "             'meetings.': 2,\n",
       "             'office': 17,\n",
       "             '9': 5,\n",
       "             'AM,': 1,\n",
       "             'nor': 1,\n",
       "             'any': 60,\n",
       "             'artificial': 2,\n",
       "             'requirement': 1,\n",
       "             'work': 114,\n",
       "             'set': 25,\n",
       "             'number': 34,\n",
       "             'day,': 7,\n",
       "             'week.': 4,\n",
       "             'write': 6,\n",
       "             'receive': 10,\n",
       "             'performance': 9,\n",
       "             'reviews.': 1,\n",
       "             'fire': 4,\n",
       "             'anyone.': 2,\n",
       "             'Nobody': 6,\n",
       "             'can': 319,\n",
       "             'nobody': 7,\n",
       "             'lay': 2,\n",
       "             'off.': 3,\n",
       "             'take': 52,\n",
       "             'longer,': 2,\n",
       "             'because': 27,\n",
       "             'level': 7,\n",
       "             'basically': 11,\n",
       "             'zero.': 2,\n",
       "             \"I'm\": 25,\n",
       "             '43': 1,\n",
       "             'old,': 1,\n",
       "             'sure': 65,\n",
       "             'retirement': 2,\n",
       "             'feel': 23,\n",
       "             'like': 66,\n",
       "             'except': 2,\n",
       "             'not': 169,\n",
       "             'too': 24,\n",
       "             'old': 6,\n",
       "             'enjoy': 10,\n",
       "             'it,': 26,\n",
       "             'broke.': 1,\n",
       "             'telling': 3,\n",
       "             'gloat.': 1,\n",
       "             'maybe': 6,\n",
       "             \"you're\": 159,\n",
       "             'three': 6,\n",
       "             'ago.': 1,\n",
       "             'Maybe': 13,\n",
       "             'knows': 5,\n",
       "             'only': 77,\n",
       "             'reward': 5,\n",
       "             'hard': 39,\n",
       "             'going': 53,\n",
       "             'heart': 1,\n",
       "             'attack': 1,\n",
       "             'but': 192,\n",
       "             'trapped': 2,\n",
       "             'salary': 2,\n",
       "             'responsibilities.': 2,\n",
       "             \"You've\": 7,\n",
       "             'accepted': 3,\n",
       "             \"society's\": 1,\n",
       "             'expectations': 4,\n",
       "             'focusing': 5,\n",
       "             'entirely': 13,\n",
       "             'until': 15,\n",
       "             '65': 1,\n",
       "             'point': 20,\n",
       "             'hope': 5,\n",
       "             \"you'll\": 67,\n",
       "             'enough': 51,\n",
       "             'saved': 6,\n",
       "             'actually': 32,\n",
       "             'retire,': 1,\n",
       "             'health': 28,\n",
       "             'left': 6,\n",
       "             'here': 11,\n",
       "             'tell': 13,\n",
       "             'there': 82,\n",
       "             'path.': 3,\n",
       "             'Taking': 3,\n",
       "             \"isn't\": 22,\n",
       "             'easy,': 4,\n",
       "             'means': 27,\n",
       "             'personal': 43,\n",
       "             'financial': 12,\n",
       "             'freedom': 23,\n",
       "             'possible.': 12,\n",
       "             'while': 38,\n",
       "             'minimizing': 2,\n",
       "             'risks': 3,\n",
       "             'process.': 7,\n",
       "             'shares': 5,\n",
       "             \"I've\": 23,\n",
       "             'way.': 3,\n",
       "             'rich': 6,\n",
       "             'this,': 10,\n",
       "             'need': 166,\n",
       "             'developer.': 1,\n",
       "             'smarts': 1,\n",
       "             'tenacity': 1,\n",
       "             'self-sufficient,': 1,\n",
       "             'read': 12,\n",
       "             'may': 106,\n",
       "             'just': 131,\n",
       "             'save': 5,\n",
       "             'life,': 3,\n",
       "             'ways': 19,\n",
       "             'one.': 8,\n",
       "             'MAKING': 4,\n",
       "             'THE': 18,\n",
       "             'BIG': 1,\n",
       "             'DECISION': 1,\n",
       "             'OVERCOMING': 1,\n",
       "             'INERTIA': 1,\n",
       "             'first': 26,\n",
       "             'step': 3,\n",
       "             'toward': 11,\n",
       "             'self-employment': 38,\n",
       "             'want': 99,\n",
       "             \"you've\": 49,\n",
       "             'spent': 13,\n",
       "             'professional': 13,\n",
       "             'someone': 57,\n",
       "             'else,': 5,\n",
       "             'scary': 4,\n",
       "             'proposition.': 1,\n",
       "             'familiar': 4,\n",
       "             'comfortable,': 1,\n",
       "             'easiest': 2,\n",
       "             'continue': 7,\n",
       "             'current': 18,\n",
       "             'Discarding': 1,\n",
       "             'striking': 1,\n",
       "             'bold': 1,\n",
       "             'psyche': 1,\n",
       "             'rebel': 1,\n",
       "             'against': 12,\n",
       "             'change': 17,\n",
       "             'magnitude,': 1,\n",
       "             'find': 76,\n",
       "             'justify': 1,\n",
       "             'trying': 24,\n",
       "             \"It's\": 46,\n",
       "             'therefore': 2,\n",
       "             'surprising': 1,\n",
       "             'self-employed': 17,\n",
       "             'individuals': 3,\n",
       "             'were': 38,\n",
       "             'forced': 6,\n",
       "             'into': 75,\n",
       "             'their': 117,\n",
       "             'situation.': 5,\n",
       "             'ones': 15,\n",
       "             'become': 13,\n",
       "             'talk': 27,\n",
       "             'lot': 54,\n",
       "             'result,': 3,\n",
       "             'motivated': 3,\n",
       "             'achieve': 8,\n",
       "             'According': 5,\n",
       "             '2012': 7,\n",
       "             'Freelance': 4,\n",
       "             'Industry': 4,\n",
       "             'Report,': 2,\n",
       "             '29%': 2,\n",
       "             'freelancers': 9,\n",
       "             'fell': 1,\n",
       "             'necessity,': 1,\n",
       "             'after': 29,\n",
       "             'being': 29,\n",
       "             'laid': 1,\n",
       "             'off': 17,\n",
       "             'downsized.': 1,\n",
       "             'better': 40,\n",
       "             'transition': 3,\n",
       "             'terms.': 3,\n",
       "             'start': 39,\n",
       "             'planning': 8,\n",
       "             'up': 155,\n",
       "             'new': 143,\n",
       "             'business': 261,\n",
       "             'day': 56,\n",
       "             'ensure': 13,\n",
       "             'flowing': 2,\n",
       "             'yourself.': 13,\n",
       "             'will': 220,\n",
       "             'arm': 1,\n",
       "             'details': 3,\n",
       "             'make': 88,\n",
       "             'happen,': 4,\n",
       "             'decide': 19,\n",
       "             'effort': 18,\n",
       "             'taking.': 2,\n",
       "             'Before': 3,\n",
       "             'self-employed,': 9,\n",
       "             'understand': 30,\n",
       "             'forces': 3,\n",
       "             'come': 26,\n",
       "             'terms': 11,\n",
       "             'them,': 15,\n",
       "             'reasons': 1,\n",
       "             'why': 18,\n",
       "             'overcome': 2,\n",
       "             \"That's\": 18,\n",
       "             'about:': 1,\n",
       "             'understanding': 6,\n",
       "             'barriers': 3,\n",
       "             'self-employment,': 3,\n",
       "             'both': 11,\n",
       "             'imagined,': 1,\n",
       "             'still': 62,\n",
       "             'goal': 14,\n",
       "             'pursuing.': 1,\n",
       "             'done,': 3,\n",
       "             \"we'll\": 1,\n",
       "             'self-assessment': 2,\n",
       "             'see': 46,\n",
       "             'really': 39,\n",
       "             'position': 9,\n",
       "             'leap': 2,\n",
       "             'FEAR': 1,\n",
       "             'OF': 16,\n",
       "             'FAILURE': 1,\n",
       "             'paycheck': 7,\n",
       "             'puts': 2,\n",
       "             'food': 2,\n",
       "             'table': 2,\n",
       "             'roof': 1,\n",
       "             'over': 58,\n",
       "             'head': 2,\n",
       "             'reliably.': 1,\n",
       "             'idea': 44,\n",
       "             'giving': 15,\n",
       "             'quickly': 12,\n",
       "             'brings': 3,\n",
       "             'images': 6,\n",
       "             'starving': 1,\n",
       "             'house': 7,\n",
       "             'getting': 28,\n",
       "             'foreclosed.': 1,\n",
       "             'risk,': 1,\n",
       "             'proper': 3,\n",
       "             'educated': 2,\n",
       "             'making,': 1,\n",
       "             'risk': 12,\n",
       "             'minimize.': 1,\n",
       "             'responsibilities': 8,\n",
       "             'should': 63,\n",
       "             'job': 43,\n",
       "             'until:': 1,\n",
       "             '*': 22,\n",
       "             'safety': 4,\n",
       "             'net': 10,\n",
       "             'place,': 2,\n",
       "             'fast': 4,\n",
       "             'rules': 4,\n",
       "             'low': 8,\n",
       "             'willing': 11,\n",
       "             'those': 63,\n",
       "             'reserves': 5,\n",
       "             'returning': 1,\n",
       "             'workplace': 2,\n",
       "             'already': 20,\n",
       "             'prototyped': 1,\n",
       "             'side': 24,\n",
       "             'proven': 1,\n",
       "             'viable': 6,\n",
       "             'So,': 14,\n",
       "             'upper': 7,\n",
       "             'bound': 10,\n",
       "             'lose': 8,\n",
       "             'calling': 1,\n",
       "             'quits.': 1,\n",
       "             'Worst': 1,\n",
       "             'case,': 10,\n",
       "             'things': 34,\n",
       "             'risky': 5,\n",
       "             'entrepreneurship?': 1,\n",
       "             'Bureau': 2,\n",
       "             'Labor': 2,\n",
       "             '(BLS)': 1,\n",
       "             'US': 14,\n",
       "             'Small': 3,\n",
       "             'Administration': 2,\n",
       "             '(SBA),': 1,\n",
       "             '50%': 5,\n",
       "             'businesses': 31,\n",
       "             'survive': 2,\n",
       "             'least': 31,\n",
       "             'five': 5,\n",
       "             'years.': 9,\n",
       "             'third': 3,\n",
       "             'ten': 5,\n",
       "             'more.': 5,\n",
       "             'think': 37,\n",
       "             'long': 40,\n",
       "             'expect': 10,\n",
       "             'last.': 1,\n",
       "             'Would': 2,\n",
       "             'say': 19,\n",
       "             \"there's\": 7,\n",
       "             'chance': 12,\n",
       "             'employer': 25,\n",
       "             'around': 15,\n",
       "             'now,': 6,\n",
       "             'same': 28,\n",
       "             'company?': 3,\n",
       "             'has': 40,\n",
       "             'answer': 6,\n",
       "             'well.': 20,\n",
       "             'average': 9,\n",
       "             'tenure': 1,\n",
       "             '4.6': 1,\n",
       "             'Does': 2,\n",
       "             'starting': 10,\n",
       "             'seem': 11,\n",
       "             'now?': 3,\n",
       "             \"Here's\": 7,\n",
       "             'statistic': 1,\n",
       "             'Report': 1,\n",
       "             'shows': 3,\n",
       "             '22%': 1,\n",
       "             'less': 23,\n",
       "             'financially': 4,\n",
       "             'secure': 1,\n",
       "             'employee.': 2,\n",
       "             '28%': 1,\n",
       "             '\"strongly': 1,\n",
       "             'agree\"': 1,\n",
       "             'themselves,': 4,\n",
       "             '\"moderately': 1,\n",
       "             'agree.\"': 1,\n",
       "             'Freelancers': 1,\n",
       "             'represent': 2,\n",
       "             \"it's\": 107,\n",
       "             'always': 16,\n",
       "             'option': 6,\n",
       "             '22': 1,\n",
       "             'Americans': 1,\n",
       "             '2010,': 1,\n",
       "             'representing': 1,\n",
       "             '14%': 1,\n",
       "             'workforce.': 1,\n",
       "             'fringe': 1,\n",
       "             'They': 26,\n",
       "             'range': 3,\n",
       "             'housekeepers,': 2,\n",
       "             'construction': 2,\n",
       "             'workers,': 3,\n",
       "             'web': 28,\n",
       "             'developers,': 5,\n",
       "             'doctors.': 1,\n",
       "             'fancy': 2,\n",
       "             'education': 2,\n",
       "             'lots': 7,\n",
       "             'connections': 6,\n",
       "             'advantage': 11,\n",
       "             'millions': 2,\n",
       "             'who': 85,\n",
       "             'managed': 4,\n",
       "             'full': 18,\n",
       "             'time': 165,\n",
       "             'earn': 16,\n",
       "             'living.': 3,\n",
       "             'CAREER': 1,\n",
       "             'INDOCTRINATION': 1,\n",
       "             'spend': 42,\n",
       "             'most': 61,\n",
       "             'waking': 2,\n",
       "             'possible': 10,\n",
       "             'somebody': 2,\n",
       "             'else': 14,\n",
       "             'richer?': 1,\n",
       "             'Well,': 2,\n",
       "             'raised': 2,\n",
       "             'believe': 15,\n",
       "             'worthy': 3,\n",
       "             \"We'll\": 4,\n",
       "             'external': 2,\n",
       "             'biggest': 2,\n",
       "             'barrier': 2,\n",
       "             'probably': 76,\n",
       "             'internal': 1,\n",
       "             'do!': 1,\n",
       "             'strong': 4,\n",
       "             'word,': 1,\n",
       "             'usually': 14,\n",
       "             'religious': 1,\n",
       "             'cults': 1,\n",
       "             'brainwashing.': 1,\n",
       "             'taught': 3,\n",
       "             'beliefs': 1,\n",
       "             'questioning': 1,\n",
       "             'Just': 11,\n",
       "             'question': 8,\n",
       "             'responsible': 4,\n",
       "             'member': 2,\n",
       "             'society?': 1,\n",
       "             'common': 8,\n",
       "             'indoctrination': 1,\n",
       "             'instill': 1,\n",
       "             'ideas': 19,\n",
       "             'youth': 1,\n",
       "             'people': 122,\n",
       "             'authority.': 1,\n",
       "             'That�s': 2,\n",
       "             'precisely': 2,\n",
       "             'grew': 1,\n",
       "             'up,': 4,\n",
       "             'pushed': 2,\n",
       "             'track': 7,\n",
       "             'corporate': 17,\n",
       "             'school,': 1,\n",
       "             'told': 5,\n",
       "             'parents': 2,\n",
       "             'teachers': 1,\n",
       "             'grades': 1,\n",
       "             'college.': 2,\n",
       "             'In': 41,\n",
       "             'college,': 1,\n",
       "             'would': 32,\n",
       "             'graduated.': 1,\n",
       "             'lucky': 2,\n",
       "             'absorbed': 1,\n",
       "             'training': 9,\n",
       "             'culture': 1,\n",
       "             'promotes': 1,\n",
       "             'loyalty': 2,\n",
       "             'employer.': 3,\n",
       "             'It�s': 6,\n",
       "             'result': 17,\n",
       "             'lifestyle': 29,\n",
       "             'choice': 5,\n",
       "             'indoctrinated': 1,\n",
       "             'early': 11,\n",
       "             'age.': 1,\n",
       "             'Once': 23,\n",
       "             'landed': 1,\n",
       "             'world,': 5,\n",
       "             'next': 15,\n",
       "             'promotion,': 3,\n",
       "             'fired.': 1,\n",
       "             'been': 24,\n",
       "             'busy': 2,\n",
       "             'following': 8,\n",
       "             'path,': 1,\n",
       "             \"haven't\": 6,\n",
       "             'consider': 20,\n",
       "             'alternatives.': 2,\n",
       "             'Meanwhile,': 1,\n",
       "             'picked': 2,\n",
       "             'children,': 1,\n",
       "             'mortgage,': 1,\n",
       "             'student': 3,\n",
       "             'loan': 7,\n",
       "             'debt': 4,\n",
       "             'steady': 2,\n",
       "             'seems': 5,\n",
       "             'terrifying,': 1,\n",
       "             'barely': 1,\n",
       "             'meet': 8,\n",
       "             'demands.': 1,\n",
       "             'bills': 7,\n",
       "             'pay': 43,\n",
       "             'mouths': 1,\n",
       "             'feed,': 1,\n",
       "             'having': 27,\n",
       "             'look': 23,\n",
       "             'forward': 1,\n",
       "             'morning': 1,\n",
       "             'anyone': 6,\n",
       "             'does.': 3,\n",
       "             'best': 39,\n",
       "             'fulfill': 1,\n",
       "             'thrust': 1,\n",
       "             'upon': 6,\n",
       "             'year': 17,\n",
       "             'year.': 10,\n",
       "             'hear': 9,\n",
       "             'businesses,': 7,\n",
       "             'beyond': 7,\n",
       "             'buying': 4,\n",
       "             'foam': 1,\n",
       "             'cup': 1,\n",
       "             'Ramen': 1,\n",
       "             'noodles.': 1,\n",
       "             'You�ve': 1,\n",
       "             'developed': 3,\n",
       "             'known': 9,\n",
       "             '�learned': 1,\n",
       "             'helplessness�': 1,\n",
       "             'aren�t': 2,\n",
       "             'stressful': 2,\n",
       "             'career,': 9,\n",
       "             'available': 19,\n",
       "             'not.': 8,\n",
       "             'As': 45,\n",
       "             'breaking': 2,\n",
       "             'indoctrination,': 1,\n",
       "             'there,': 7,\n",
       "             'develop': 7,\n",
       "             'desire': 1,\n",
       "             'alternate': 1,\n",
       "             'paths.': 1,\n",
       "             'Striking': 1,\n",
       "             'becoming': 4,\n",
       "             'one': 69,\n",
       "             'such': 39,\n",
       "             'path': 15,\n",
       "             'improve': 3,\n",
       "             'lifestyle,': 1,\n",
       "             'done': 17,\n",
       "             'responsibly.': 1,\n",
       "             'CARROT': 1,\n",
       "             'ON': 2,\n",
       "             'STICK': 1,\n",
       "             'where': 49,\n",
       "             ...})"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = words.countByValue()\n",
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets improve this by using regex to filter only the words(and get rid of all the non-word characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self 111\n",
      "employment 75\n",
      "building 33\n",
      "an 178\n",
      "internet 26\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "\n",
    "import re\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "input = sc.textFile(\"book\")\n",
    "words = input.flatMap(normalizeWords)\n",
    "wordCounts = words.countByValue()\n",
    "\n",
    "counter = 0\n",
    "for word, count in wordCounts.items():\n",
    "    cleanWord = word.encode('ascii', 'ignore')\n",
    "    if (cleanWord):\n",
    "        print(cleanWord.decode() + \" \" + str(count))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter == 5:\n",
    "        break;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets sort it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\t\t1191\n",
      "the:\t\t1292\n",
      "your:\t\t1420\n",
      "to:\t\t1828\n",
      "you:\t\t1878\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "\n",
    "import re\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "input = sc.textFile(\"book\")\n",
    "words = input.flatMap(normalizeWords)\n",
    "\n",
    "wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "wordCountsSorted = wordCounts.map(lambda x: (x[1], x[0])).sortByKey()\n",
    "results = wordCountsSorted.collect()\n",
    "\n",
    "for result in results:\n",
    "    count = str(result[0])\n",
    "    word = result[1].encode('ascii', 'ignore')\n",
    "    if (word) and int(count) > 1000:\n",
    "        print(word.decode() + \":\\t\\t\" + count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum the total amount paid per customerid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the first line is the customer id, the second line is the amount paid\n",
    "- We should first split the rdd into only the first and third columns.\n",
    "- Then sum the total paid by customerId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('CountAmountPaid')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['44,8602,37.19',\n",
       " '35,5368,65.89',\n",
       " '2,3391,40.64',\n",
       " '47,6694,14.98',\n",
       " '29,680,13.08']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders = sc.textFile('customer-orders.csv')\n",
    "orders.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, lets parse these lines as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLines(line):\n",
    "    fields = line.split(',')\n",
    "    customerID = int(fields[0])\n",
    "    amountPaid = float(fields[2])\n",
    "    \n",
    "    return (customerID, amountPaid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 37.19), (35, 65.89), (2, 40.64), (47, 14.98), (29, 13.08)]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedLines = orders.map(parseLines)\n",
    "parsedLines.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, sum the amount paid per customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the amount per customer\n",
    "amount_per_customer = parsedLines.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 4756.8899999999985),\n",
       " (35, 5155.419999999999),\n",
       " (2, 5994.59),\n",
       " (47, 4316.299999999999),\n",
       " (29, 5032.529999999999),\n",
       " (91, 4642.259999999999),\n",
       " (70, 5368.249999999999),\n",
       " (85, 5503.43),\n",
       " (53, 4945.299999999999),\n",
       " (14, 4735.030000000001)]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 10 rows\n",
    "amount_per_customer.collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort it by the second column(amount paid)\n",
    "sorted_amount_per_customer = amount_per_customer.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(68, 6375.449999999997),\n",
       " (73, 6206.199999999999),\n",
       " (39, 6193.109999999999),\n",
       " (54, 6065.389999999999),\n",
       " (71, 5995.660000000003),\n",
       " (2, 5994.59),\n",
       " (97, 5977.189999999995),\n",
       " (46, 5963.109999999999),\n",
       " (42, 5696.840000000003),\n",
       " (59, 5642.89)]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 10 rows\n",
    "sorted_amount_per_customer.collect()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>__SparkSQL__<ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a DataFrame\n",
    "- If the data that we want to load is structured(I.E tabualr form and with headers) then we can load it straight into a DatFrame.\n",
    "- If the data is not structured. We should load it first into an RDD, transform it, and then load it into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types:\n",
    "https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading data without headers(unstructured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example1 - mapping headers with Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile('fakefriends.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,Will,33,385',\n",
       " '1,Jean-Luc,26,2',\n",
       " '2,Hugh,55,221',\n",
       " '3,Deanna,40,465',\n",
       " '4,Quark,68,21']"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So the data doesnt have any headers, we cant simply load it into a DataFrame.\n",
    "- Lets transform it using a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(\n",
    "            ID = int(fields[0]),\n",
    "            name = str(fields[1].encode('utf-8')),\n",
    "            age = int(fields[2]),\n",
    "            numFriends = int(fields[3])\n",
    "    )\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_rdd = rdd.map(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can load it into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaPeople = spark.createDataFrame(structured_rdd).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we need to create a temporary view from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaPeople.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example2 - Reading text\n",
    "- Here we are reading a text into a DataFrame.\n",
    "- When working with texts we probably should use some RDD commands as they are more suited then SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting a session, loading the data, and inferring the schema\n",
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()\n",
    "book = spark.read.text('Book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have a view called people that contains the data from the CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example3 - Building a schema\n",
    "- We can also give Spark a schema of our choosing\n",
    "- For example, the dataset 1800.csv doesnt have a header, so we can load it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MinTemperatures\").getOrCreate()\n",
    "\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"stationID\", StringType(), True), \\\n",
    "                     StructField(\"date\", IntegerType(), True), \\\n",
    "                     StructField(\"measure_type\", StringType(), True), \\\n",
    "                     StructField(\"temperature\", FloatType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stationID: string (nullable = true)\n",
      " |-- date: integer (nullable = true)\n",
      " |-- measure_type: string (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# // Read the file as dataframe\n",
    "df = spark.read.schema(schema).csv(\"1800.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, each column got the header that we specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create DataFrame WITH headers(structured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example1 - inferring schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we specify that the datafile has headers, and we let Spark infer the types of data himself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = spark.read.option('header', 'true').option('inferSchema', 'true')\\\n",
    ".csv('fakefriends-header.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can take a look at the inffered schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Everything looks good. Now we can either create a temp view and run sql queries.\n",
    "- OR, run commands like filter, select, groupBy etc... straight on the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can just query it as a normal DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|Average|\n",
      "+---+-------+\n",
      "| 25| 197.45|\n",
      "| 24|  233.8|\n",
      "| 23|  246.3|\n",
      "| 22| 206.43|\n",
      "| 21| 350.88|\n",
      "| 20|  165.0|\n",
      "| 19| 213.27|\n",
      "| 18| 343.38|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select age,\n",
    "       round(avg(friends),2) as Average\n",
    "from people\n",
    "where age between 13 and 25\n",
    "group by age\n",
    "order by age desc;\n",
    "'''\n",
    "\n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example2 - Tab separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up movie data as dataframe\n",
    "moviesDF = spark.read.option(\"sep\", \"\\t\").schema(schema).csv(\"u.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('test').master('local').getOrCreate()\n",
    "df = spark.read.option('header', 'true').csv(\"activity.csv\")\n",
    "df.createOrReplaceTempView('activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+------------+\n",
      "|player_id|device_id|event_date|games_played|\n",
      "+---------+---------+----------+------------+\n",
      "|     1503|        1|2021-10-15|           2|\n",
      "|     4106|        3|2020-07-22|           3|\n",
      "|     3170|        3|2021-07-21|           9|\n",
      "|     4518|        1|2020-08-03|           1|\n",
      "|     4501|        1|2020-10-22|           4|\n",
      "+---------+---------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('select * from activity')\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.toPandas().to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o275.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$25(FileFormatWriter.scala:267)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:267)\r\n\t... 41 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/activity_parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o275.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$25(FileFormatWriter.scala:267)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:267)\r\n\t... 41 more\r\n"
     ]
    }
   ],
   "source": [
    "result.write.format('parquet').mode('overwrite').option('compression', 'gzip').save('data/activity_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export with Partitioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.write.format('parquet').mode('overwrite').option('compression', 'gzip').PartitionBy('Product').save('data/activity_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export with Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o284.saveAsTable.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:384)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:175)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucketBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1041\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1041\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o284.saveAsTable.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:384)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:175)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "result.write.format('parquet').mode('overwrite').option('compression', 'gzip').bucketBy(3,'device_id').saveAsTable(\"device_bucket_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SparkSession Commands\n",
    "- Broadcast - Transfer an object to the Spark Driver\n",
    "- value - Get the object from the Spark Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting a dictionary and retrieving it using a user defined function(udf)\n",
    "- This allows us to only broadcast an object once, and that object will be stored in all of the clusters.\n",
    "- This is suitable for small datasets.\n",
    "- It also reduced overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting the dictionary to the Spark Driver\n",
    "nameDict = spark.sparkContext.broadcast(loadMovieNames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-defined function to look up movie names from our broadcasted dictionary\n",
    "def lookupName(movieID):\n",
    "    return nameDict.value[movieID]\n",
    "\n",
    "lookupNameUDF = func.udf(lookupName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulator\n",
    "- Store a synced variable across all clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Create a Counter variable that accumulates counters, and share it across all clusters.\n",
    "- Also, iterate it by 1, by using the method .add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitCounter = sc.accumulator(0)\n",
    "hitCounter.add(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the average number of friends per age(Structured Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting a session, loading the data, and inferring the schema\n",
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()\n",
    "people = spark.read.option('header', 'true').option('inferSchema', 'true')\\\n",
    ".csv('fakefriends-header.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a temp view\n",
    "people.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|Average|\n",
      "+---+-------+\n",
      "| 69|  235.2|\n",
      "| 68|  269.6|\n",
      "| 67| 214.63|\n",
      "| 66| 276.44|\n",
      "| 65|  298.2|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying the data\n",
    "query = '''\n",
    "select \n",
    "    age,\n",
    "    round(avg(friends), 2) as Average\n",
    "from people\n",
    "group by age\n",
    "order by age desc\n",
    "limit 5\n",
    "'''\n",
    "\n",
    "df  = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting words(Unstructured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not every problem is a SQL problem.\n",
    "- This is an example of a problem that is better suited to solve using RDD's\n",
    "- Also we dont need to pick only one, we can use both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting a session, loading the data, and inferring the schema\n",
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()\n",
    "book = spark.read.text('Book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|              teamed|    1|\n",
      "|               boost|    1|\n",
      "|        manipulation|    1|\n",
      "|            preneurs|    1|\n",
      "|            laureate|    1|\n",
      "|                unto|    1|\n",
      "|             scaling|    1|\n",
      "|          recommends|    1|\n",
      "|            reacting|    1|\n",
      "|             frankly|    1|\n",
      "|             dilemma|    1|\n",
      "|       documentation|    1|\n",
      "|            rejected|    1|\n",
      "|             harmony|    1|\n",
      "|                bryc|    1|\n",
      "|             legwork|    1|\n",
      "|          production|    1|\n",
      "|         supervision|    1|\n",
      "|             dropped|    1|\n",
      "|                reis|    1|\n",
      "|              landed|    1|\n",
      "|               apple|    1|\n",
      "|                none|    1|\n",
      "|                 hot|    1|\n",
      "|              proven|    1|\n",
      "|          marketable|    1|\n",
      "|              quoted|    1|\n",
      "|              bundle|    1|\n",
      "|              attend|    1|\n",
      "|                spin|    1|\n",
      "|             textual|    1|\n",
      "|            replying|    1|\n",
      "|             hardest|    1|\n",
      "|           accompany|    1|\n",
      "|            billions|    1|\n",
      "|          presenting|    1|\n",
      "|                  85|    1|\n",
      "|         prospecting|    1|\n",
      "|       congratulated|    1|\n",
      "|              blinks|    1|\n",
      "|              column|    1|\n",
      "|            thriving|    1|\n",
      "|                cast|    1|\n",
      "|             burdens|    1|\n",
      "|           fashioned|    1|\n",
      "|             classes|    1|\n",
      "|       visualization|    1|\n",
      "|              expert|    1|\n",
      "|          newsletter|    1|\n",
      "|            outliers|    1|\n",
      "|           wondering|    1|\n",
      "|           realizing|    1|\n",
      "|           diligence|    1|\n",
      "|               nobel|    1|\n",
      "|             pursuit|    1|\n",
      "|               stern|    1|\n",
      "|          guarantees|    1|\n",
      "|            painting|    1|\n",
      "|            dictates|    1|\n",
      "|           fostering|    1|\n",
      "|            embedded|    1|\n",
      "|            slightly|    1|\n",
      "|           converted|    1|\n",
      "|             sponsor|    1|\n",
      "|                junk|    1|\n",
      "|              coding|    1|\n",
      "|           component|    1|\n",
      "|          difficulty|    1|\n",
      "|                 gas|    1|\n",
      "|                   0|    1|\n",
      "|              mature|    1|\n",
      "|            critical|    1|\n",
      "|             godaddy|    1|\n",
      "|            obtained|    1|\n",
      "|               stone|    1|\n",
      "|          overlooked|    1|\n",
      "|        questionable|    1|\n",
      "|               vague|    1|\n",
      "|               spell|    1|\n",
      "|          registered|    1|\n",
      "|      overwhelmingly|    1|\n",
      "|              fringe|    1|\n",
      "|          variations|    1|\n",
      "|           officials|    1|\n",
      "|             tapping|    1|\n",
      "|              colors|    1|\n",
      "|                  h2|    1|\n",
      "|                cell|    1|\n",
      "|         contingency|    1|\n",
      "|         storefronts|    1|\n",
      "|             listing|    1|\n",
      "|         suggestions|    1|\n",
      "|             tighten|    1|\n",
      "|          skyscraper|    1|\n",
      "|            inclined|    1|\n",
      "|          salesforce|    1|\n",
      "|              driven|    1|\n",
      "|             toolset|    1|\n",
      "|            produces|    1|\n",
      "|                kiss|    1|\n",
      "|           energetic|    1|\n",
      "|              turned|    1|\n",
      "|            restrict|    1|\n",
      "|                 800|    1|\n",
      "|             persist|    1|\n",
      "|             ignored|    1|\n",
      "|              shaped|    1|\n",
      "|           aggregate|    1|\n",
      "|              ranged|    1|\n",
      "|              motive|    1|\n",
      "|                  47|    1|\n",
      "|           immersive|    1|\n",
      "|             durable|    1|\n",
      "|               filed|    1|\n",
      "|          helpcenter|    1|\n",
      "|            welcomes|    1|\n",
      "|          readership|    1|\n",
      "|          distribute|    1|\n",
      "|            provides|    1|\n",
      "|           diversify|    1|\n",
      "|         boilerplate|    1|\n",
      "|         preparation|    1|\n",
      "|            doubting|    1|\n",
      "|                lawn|    1|\n",
      "|         continually|    1|\n",
      "|               hires|    1|\n",
      "|       relationships|    1|\n",
      "|                sued|    1|\n",
      "|              desist|    1|\n",
      "|            powerful|    1|\n",
      "|            scenario|    1|\n",
      "|          optimistic|    1|\n",
      "|             lecture|    1|\n",
      "|               chief|    1|\n",
      "|         terminology|    1|\n",
      "|              formal|    1|\n",
      "|              signup|    1|\n",
      "|         presumption|    1|\n",
      "|              strict|    1|\n",
      "|              desire|    1|\n",
      "|              server|    1|\n",
      "|             receipt|    1|\n",
      "|         startlingly|    1|\n",
      "|              please|    1|\n",
      "|             cursory|    1|\n",
      "|                 san|    1|\n",
      "|          surrounded|    1|\n",
      "|             phrases|    1|\n",
      "|               words|    1|\n",
      "|              elance|    1|\n",
      "|             sticker|    1|\n",
      "|              tastes|    1|\n",
      "|            motivate|    1|\n",
      "|           possesses|    1|\n",
      "|               seven|    1|\n",
      "|                suit|    1|\n",
      "|               troll|    1|\n",
      "|               awake|    1|\n",
      "|            refilled|    1|\n",
      "|       architectural|    1|\n",
      "|             upgrade|    1|\n",
      "|           separated|    1|\n",
      "|                four|    1|\n",
      "|            critique|    1|\n",
      "|              trades|    1|\n",
      "|       consultations|    1|\n",
      "|         immortalize|    1|\n",
      "|                 fax|    1|\n",
      "|             handing|    1|\n",
      "|            selected|    1|\n",
      "|               venue|    1|\n",
      "|            kahneman|    1|\n",
      "|             scammer|    1|\n",
      "|        particularly|    1|\n",
      "|              reader|    1|\n",
      "|            imagined|    1|\n",
      "|               hopes|    1|\n",
      "|          terrifying|    1|\n",
      "|             zealand|    1|\n",
      "|             violate|    1|\n",
      "|        demographics|    1|\n",
      "|             refills|    1|\n",
      "|          subsequent|    1|\n",
      "|           reviewing|    1|\n",
      "|             viewers|    1|\n",
      "|              boring|    1|\n",
      "|                 nor|    1|\n",
      "|                dire|    1|\n",
      "|                  ey|    1|\n",
      "|                rush|    1|\n",
      "|            horribly|    1|\n",
      "|             runaway|    1|\n",
      "|              touted|    1|\n",
      "|            survival|    1|\n",
      "|          popularity|    1|\n",
      "|             stumble|    1|\n",
      "|            entering|    1|\n",
      "|               trips|    1|\n",
      "|         customizing|    1|\n",
      "|               harsh|    1|\n",
      "|                 fla|    1|\n",
      "|             officer|    1|\n",
      "|                span|    1|\n",
      "|        deteriorates|    1|\n",
      "|               afoul|    1|\n",
      "|               chime|    1|\n",
      "|              boston|    1|\n",
      "|               proof|    1|\n",
      "|             perfect|    1|\n",
      "|               cable|    1|\n",
      "|          insulating|    1|\n",
      "|             breathe|    1|\n",
      "|      accomplishment|    1|\n",
      "|            obscured|    1|\n",
      "|          simulating|    1|\n",
      "|            optional|    1|\n",
      "|           foolproof|    1|\n",
      "|             legally|    1|\n",
      "|            outreach|    1|\n",
      "|             promise|    1|\n",
      "|              genius|    1|\n",
      "|             deborah|    1|\n",
      "|         institution|    1|\n",
      "|               texas|    1|\n",
      "|                vote|    1|\n",
      "|          disposable|    1|\n",
      "|               loyal|    1|\n",
      "|                door|    1|\n",
      "|           permitted|    1|\n",
      "|            tradeoff|    1|\n",
      "|          afterwards|    1|\n",
      "|             devoted|    1|\n",
      "|                  43|    1|\n",
      "|               paths|    1|\n",
      "|             attempt|    1|\n",
      "|                inch|    1|\n",
      "|         educational|    1|\n",
      "|            attracts|    1|\n",
      "|            approved|    1|\n",
      "|          attracting|    1|\n",
      "|                1099|    1|\n",
      "|              cycles|    1|\n",
      "|          suspicious|    1|\n",
      "|             existed|    1|\n",
      "|               threw|    1|\n",
      "|          colleagues|    1|\n",
      "|            teachers|    1|\n",
      "|                 401|    1|\n",
      "|               pesky|    1|\n",
      "|               delay|    1|\n",
      "|             looming|    1|\n",
      "|             utility|    1|\n",
      "|              normal|    1|\n",
      "|              topics|    1|\n",
      "|           dartmouth|    1|\n",
      "|           essential|    1|\n",
      "|        inflammatory|    1|\n",
      "|           admitting|    1|\n",
      "|           cardboard|    1|\n",
      "|               bench|    1|\n",
      "|               parks|    1|\n",
      "|              debate|    1|\n",
      "|       indoctrinated|    1|\n",
      "|      identification|    1|\n",
      "|              thrust|    1|\n",
      "|               cares|    1|\n",
      "|           reinforce|    1|\n",
      "|             banking|    1|\n",
      "|          fraudsters|    1|\n",
      "|            cheapest|    1|\n",
      "|             mailbox|    1|\n",
      "|               super|    1|\n",
      "|             divorce|    1|\n",
      "|                hits|    1|\n",
      "|              laying|    1|\n",
      "|              harper|    1|\n",
      "|           timeframe|    1|\n",
      "|            stagnant|    1|\n",
      "|          depression|    1|\n",
      "|               magic|    1|\n",
      "|             optimal|    1|\n",
      "|               creep|    1|\n",
      "|                hell|    1|\n",
      "|                cult|    1|\n",
      "|         outperforms|    1|\n",
      "|           expanding|    1|\n",
      "|            checkups|    1|\n",
      "|           contacted|    1|\n",
      "|                 360|    1|\n",
      "|             realism|    1|\n",
      "|         technically|    1|\n",
      "|         symmetrical|    1|\n",
      "|              square|    1|\n",
      "|            saturate|    1|\n",
      "|                onto|    1|\n",
      "|            commerce|    1|\n",
      "|        personalized|    1|\n",
      "|              harass|    1|\n",
      "|          repeatedly|    1|\n",
      "|             ethical|    1|\n",
      "|             farming|    1|\n",
      "|           breakfast|    1|\n",
      "|                fell|    1|\n",
      "|             archive|    1|\n",
      "|             smarter|    1|\n",
      "|                 do7|    1|\n",
      "|         achievement|    1|\n",
      "|           gestating|    1|\n",
      "|          strategies|    1|\n",
      "|           furniture|    1|\n",
      "|                 six|    1|\n",
      "|                bits|    1|\n",
      "|                tall|    1|\n",
      "|           inquiries|    1|\n",
      "|             solicit|    1|\n",
      "|            evolving|    1|\n",
      "|             avoided|    1|\n",
      "|         transitions|    1|\n",
      "|        interviewing|    1|\n",
      "|          guaranteed|    1|\n",
      "|          obligation|    1|\n",
      "|           broadcast|    1|\n",
      "|              defend|    1|\n",
      "|          proficient|    1|\n",
      "|            reaction|    1|\n",
      "|                hall|    1|\n",
      "|         distributed|    1|\n",
      "|           designers|    1|\n",
      "|            intended|    1|\n",
      "|               sneak|    1|\n",
      "|           excellent|    1|\n",
      "|               adopt|    1|\n",
      "|               lines|    1|\n",
      "|          increasing|    1|\n",
      "|             special|    1|\n",
      "|           happiness|    1|\n",
      "|           pinterest|    1|\n",
      "|          aggregated|    1|\n",
      "|             offsets|    1|\n",
      "|           alternate|    1|\n",
      "|                legs|    1|\n",
      "|          techcrunch|    1|\n",
      "|                  17|    1|\n",
      "|              plunge|    1|\n",
      "|               angel|    1|\n",
      "|             chances|    1|\n",
      "|              shying|    1|\n",
      "|               beast|    1|\n",
      "|            bankrupt|    1|\n",
      "|          outrageous|    1|\n",
      "|              passed|    1|\n",
      "|            portable|    1|\n",
      "|             laminar|    1|\n",
      "|          passionate|    1|\n",
      "|             serving|    1|\n",
      "|       uncomfortable|    1|\n",
      "|              helped|    1|\n",
      "|        recommending|    1|\n",
      "|             crafted|    1|\n",
      "|              damage|    1|\n",
      "|        moneybuilder|    1|\n",
      "|        artificially|    1|\n",
      "|            filtered|    1|\n",
      "|              fruits|    1|\n",
      "|               dream|    1|\n",
      "|           processed|    1|\n",
      "|              ageist|    1|\n",
      "|               drove|    1|\n",
      "|       disappearance|    1|\n",
      "|           packaging|    1|\n",
      "|            recharge|    1|\n",
      "|              mouths|    1|\n",
      "|            lobbying|    1|\n",
      "|           americans|    1|\n",
      "|                road|    1|\n",
      "|          struggling|    1|\n",
      "|              nuance|    1|\n",
      "|           requiring|    1|\n",
      "|         restaurants|    1|\n",
      "|               gross|    1|\n",
      "|              smarts|    1|\n",
      "|             conjure|    1|\n",
      "|                   u|    1|\n",
      "|           beachside|    1|\n",
      "|              volume|    1|\n",
      "|                 150|    1|\n",
      "|               suite|    1|\n",
      "|            cropping|    1|\n",
      "|          engineered|    1|\n",
      "|             heavily|    1|\n",
      "|              nearby|    1|\n",
      "|           purchases|    1|\n",
      "|                nine|    1|\n",
      "|               broke|    1|\n",
      "|             designs|    1|\n",
      "|            assuming|    1|\n",
      "|                sunk|    1|\n",
      "|         undertaking|    1|\n",
      "|                 tap|    1|\n",
      "|          objectives|    1|\n",
      "|           abandoned|    1|\n",
      "|          implicitly|    1|\n",
      "|           discovers|    1|\n",
      "|               shelf|    1|\n",
      "|          anticipate|    1|\n",
      "|              broken|    1|\n",
      "|                 tag|    1|\n",
      "|        capabilities|    1|\n",
      "|           arranging|    1|\n",
      "|            congress|    1|\n",
      "|         copywriting|    1|\n",
      "|              ruined|    1|\n",
      "|         incorrectly|    1|\n",
      "|            delivery|    1|\n",
      "|               naval|    1|\n",
      "|              scared|    1|\n",
      "|                 dug|    1|\n",
      "|              phrase|    1|\n",
      "|             vietnam|    1|\n",
      "|             reserve|    1|\n",
      "|           signature|    1|\n",
      "|             succeed|    1|\n",
      "|            promotes|    1|\n",
      "|             tweeted|    1|\n",
      "|             outlook|    1|\n",
      "|         trademarked|    1|\n",
      "|            violates|    1|\n",
      "|           analyzing|    1|\n",
      "|              linear|    1|\n",
      "|               alexa|    1|\n",
      "|            concerns|    1|\n",
      "|                pair|    1|\n",
      "|          proceeding|    1|\n",
      "|               types|    1|\n",
      "|                vent|    1|\n",
      "|            playbook|    1|\n",
      "|            measured|    1|\n",
      "|             closely|    1|\n",
      "|          originally|    1|\n",
      "|          attractive|    1|\n",
      "|               upset|    1|\n",
      "|            doubling|    1|\n",
      "|        commensurate|    1|\n",
      "|            handedly|    1|\n",
      "|          materially|    1|\n",
      "|         cooperation|    1|\n",
      "|      proprietorship|    1|\n",
      "|            hospital|    1|\n",
      "|        occasionally|    1|\n",
      "|                 apa|    1|\n",
      "|                 500|    1|\n",
      "|            unproven|    1|\n",
      "|     entrepreneurial|    1|\n",
      "|               liked|    1|\n",
      "|               bleed|    1|\n",
      "|        maintainable|    1|\n",
      "|         frustrating|    1|\n",
      "|         willingness|    1|\n",
      "|               bears|    1|\n",
      "|               nicer|    1|\n",
      "|          capitalist|    1|\n",
      "|           achieving|    1|\n",
      "|               ethic|    1|\n",
      "|   misunderstandings|    1|\n",
      "|           exclusion|    1|\n",
      "|           preferred|    1|\n",
      "|       psychological|    1|\n",
      "|         respondents|    1|\n",
      "|           recognize|    1|\n",
      "|           oversight|    1|\n",
      "|             happily|    1|\n",
      "|            delivers|    1|\n",
      "|               angie|    1|\n",
      "|              arcane|    1|\n",
      "|        intelligence|    1|\n",
      "|        availability|    1|\n",
      "|              europe|    1|\n",
      "|          department|    1|\n",
      "|                 pop|    1|\n",
      "|          unprepared|    1|\n",
      "|           scenarios|    1|\n",
      "|           fortunate|    1|\n",
      "|             refined|    1|\n",
      "|               dress|    1|\n",
      "|          continuity|    1|\n",
      "|                 iag|    1|\n",
      "|            minority|    1|\n",
      "|           respected|    1|\n",
      "|               voice|    1|\n",
      "|                warm|    1|\n",
      "|                male|    1|\n",
      "|               saves|    1|\n",
      "|           personnel|    1|\n",
      "|           evaluates|    1|\n",
      "|              overly|    1|\n",
      "|            surprise|    1|\n",
      "|           factoring|    1|\n",
      "|               mined|    1|\n",
      "|              survey|    1|\n",
      "|           embarking|    1|\n",
      "|          subsidizes|    1|\n",
      "|              brutal|    1|\n",
      "|             drafted|    1|\n",
      "|               claim|    1|\n",
      "|               tests|    1|\n",
      "|              basing|    1|\n",
      "|            managers|    1|\n",
      "|       organizations|    1|\n",
      "|                tuck|    1|\n",
      "|              sudden|    1|\n",
      "|              convey|    1|\n",
      "|             mindful|    1|\n",
      "|               clock|    1|\n",
      "|         pessimistic|    1|\n",
      "|           rewarding|    1|\n",
      "|             failing|    1|\n",
      "|               screw|    1|\n",
      "|           correctly|    1|\n",
      "|               child|    1|\n",
      "|             hopeful|    1|\n",
      "|              sexist|    1|\n",
      "|            reflects|    1|\n",
      "|           depleting|    1|\n",
      "|         electronics|    1|\n",
      "|         observation|    1|\n",
      "|         differently|    1|\n",
      "|              driver|    1|\n",
      "|               force|    1|\n",
      "|           whirlwind|    1|\n",
      "|            seasonal|    1|\n",
      "|           magnitude|    1|\n",
      "|                 org|    1|\n",
      "|               water|    1|\n",
      "|        representing|    1|\n",
      "|            promoted|    1|\n",
      "|                2006|    1|\n",
      "|                 god|    1|\n",
      "|            centered|    1|\n",
      "|             abandon|    1|\n",
      "|            vigilant|    1|\n",
      "|         methodology|    1|\n",
      "|                sees|    1|\n",
      "|             degrees|    1|\n",
      "|             serious|    1|\n",
      "|             handful|    1|\n",
      "|             expects|    1|\n",
      "|              reveal|    1|\n",
      "|             builder|    1|\n",
      "|           insider11|    1|\n",
      "|             hallway|    1|\n",
      "|            disposal|    1|\n",
      "|             playing|    1|\n",
      "|              pursue|    1|\n",
      "|             binding|    1|\n",
      "|             hacking|    1|\n",
      "|               chump|    1|\n",
      "|          monitoring|    1|\n",
      "|          approaches|    1|\n",
      "|         distraction|    1|\n",
      "|                fool|    1|\n",
      "|                 ctr|    1|\n",
      "|           repelling|    1|\n",
      "|         acknowledge|    1|\n",
      "|                 sba|    1|\n",
      "|              victim|    1|\n",
      "|               rated|    1|\n",
      "|             centers|    1|\n",
      "|              digest|    1|\n",
      "|        marketplaces|    1|\n",
      "|        restaurateur|    1|\n",
      "|             equally|    1|\n",
      "|          originated|    1|\n",
      "|             germany|    1|\n",
      "|           delivered|    1|\n",
      "|              denial|    1|\n",
      "|       contingencies|    1|\n",
      "|                asks|    1|\n",
      "|             timothy|    1|\n",
      "|              random|    1|\n",
      "|          intimately|    1|\n",
      "|              backed|    1|\n",
      "|            lifetime|    1|\n",
      "|            replaced|    1|\n",
      "|           happening|    1|\n",
      "|              ragged|    1|\n",
      "|             trading|    1|\n",
      "|             express|    1|\n",
      "|                fuel|    1|\n",
      "|          understood|    1|\n",
      "|                dump|    1|\n",
      "|            inaction|    1|\n",
      "|               stamp|    1|\n",
      "|               yoast|    1|\n",
      "|        associations|    1|\n",
      "|             mindset|    1|\n",
      "|            unsolved|    1|\n",
      "|                 tim|    1|\n",
      "|             natural|    1|\n",
      "|          affordable|    1|\n",
      "|                 212|    1|\n",
      "|         specialized|    1|\n",
      "|         interacting|    1|\n",
      "|           nurturing|    1|\n",
      "|            pivoting|    1|\n",
      "|         replacement|    1|\n",
      "|              dangle|    1|\n",
      "|            cheating|    1|\n",
      "|             decades|    1|\n",
      "|               shirt|    1|\n",
      "|             reminds|    1|\n",
      "|               poll8|    1|\n",
      "|              fondly|    1|\n",
      "|               fault|    1|\n",
      "|              entity|    1|\n",
      "|          reclaiming|    1|\n",
      "|           releasing|    1|\n",
      "|     straightforward|    1|\n",
      "|              splash|    1|\n",
      "|              tender|    1|\n",
      "|         organically|    1|\n",
      "|             summary|    1|\n",
      "|            pleasant|    1|\n",
      "|             screens|    1|\n",
      "|               signs|    1|\n",
      "|             pushing|    1|\n",
      "|              agreed|    1|\n",
      "|               quits|    1|\n",
      "|          critically|    1|\n",
      "|iduskbn0e721m2014...|    1|\n",
      "|         regulations|    1|\n",
      "|           complaint|    1|\n",
      "|        architecture|    1|\n",
      "|             applies|    1|\n",
      "|          compressed|    1|\n",
      "|             applied|    1|\n",
      "|           reporters|    1|\n",
      "|               theft|    1|\n",
      "|              lining|    1|\n",
      "|               floor|    1|\n",
      "|          similarity|    1|\n",
      "|             entries|    1|\n",
      "|              awards|    1|\n",
      "|            promptly|    1|\n",
      "|                kick|    1|\n",
      "|             members|    1|\n",
      "|              silver|    1|\n",
      "|                rule|    1|\n",
      "|            emailing|    1|\n",
      "|           sustained|    1|\n",
      "|           beginners|    1|\n",
      "|          applicants|    1|\n",
      "|              errors|    1|\n",
      "|            referral|    1|\n",
      "|           reminding|    1|\n",
      "|           immediate|    1|\n",
      "|                scam|    1|\n",
      "|               lends|    1|\n",
      "|          relocation|    1|\n",
      "|               mails|    1|\n",
      "|              locale|    1|\n",
      "|             chasing|    1|\n",
      "|            explored|    1|\n",
      "|           employing|    1|\n",
      "|              static|    1|\n",
      "|               blurb|    1|\n",
      "|            graphics|    1|\n",
      "|               drops|    1|\n",
      "|           president|    1|\n",
      "|          proprietor|    1|\n",
      "|               tolls|    1|\n",
      "|        manufactured|    1|\n",
      "|            watching|    1|\n",
      "|                born|    1|\n",
      "|       discretionary|    1|\n",
      "|        abbreviation|    1|\n",
      "|             placing|    1|\n",
      "|              newest|    1|\n",
      "|           indicator|    1|\n",
      "|           publicize|    1|\n",
      "|             exclude|    1|\n",
      "|                docs|    1|\n",
      "|           secondary|    1|\n",
      "|                1000|    1|\n",
      "|             warning|    1|\n",
      "|           arbitrary|    1|\n",
      "|        brainwashing|    1|\n",
      "|        hopelessness|    1|\n",
      "|             editing|    1|\n",
      "|               fairs|    1|\n",
      "|             instill|    1|\n",
      "|              worded|    1|\n",
      "|          validation|    1|\n",
      "|               shift|    1|\n",
      "|            uncommon|    1|\n",
      "|               quote|    1|\n",
      "|                logs|    1|\n",
      "|              caters|    1|\n",
      "|              gender|    1|\n",
      "|              chores|    1|\n",
      "|              custom|    1|\n",
      "|           addictive|    1|\n",
      "|         directories|    1|\n",
      "|           surprises|    1|\n",
      "|               rebel|    1|\n",
      "|              barber|    1|\n",
      "|            securing|    1|\n",
      "|            generous|    1|\n",
      "|           intention|    1|\n",
      "|        periodically|    1|\n",
      "|         investments|    1|\n",
      "|             adapted|    1|\n",
      "|            divorced|    1|\n",
      "|               fkane|    1|\n",
      "|              sierra|    1|\n",
      "|            category|    1|\n",
      "|           accepting|    1|\n",
      "|        communicated|    1|\n",
      "|           mentality|    1|\n",
      "|           curiosity|    1|\n",
      "|             hitting|    1|\n",
      "|          recharging|    1|\n",
      "|                buys|    1|\n",
      "|          advantages|    1|\n",
      "|              flyers|    1|\n",
      "|          automation|    1|\n",
      "|            overseas|    1|\n",
      "|                1124|    1|\n",
      "|           splitting|    1|\n",
      "|             routine|    1|\n",
      "|                  w3|    1|\n",
      "|           billboard|    1|\n",
      "|     personalization|    1|\n",
      "|              crawls|    1|\n",
      "|              enjoys|    1|\n",
      "|                   e|    1|\n",
      "|          discarding|    1|\n",
      "|               motor|    1|\n",
      "|              billed|    1|\n",
      "|              gallup|    1|\n",
      "|        conservative|    1|\n",
      "|           vacations|    1|\n",
      "|               cycle|    1|\n",
      "|            finished|    1|\n",
      "|            aviation|    1|\n",
      "|               admit|    1|\n",
      "|           comparing|    1|\n",
      "|           concisely|    1|\n",
      "|              submit|    1|\n",
      "|                   j|    1|\n",
      "|                role|    1|\n",
      "|             forever|    1|\n",
      "|            immersed|    1|\n",
      "|                cram|    1|\n",
      "|          volumetric|    1|\n",
      "|           promoting|    1|\n",
      "|         volunteered|    1|\n",
      "|             impacts|    1|\n",
      "|               glass|    1|\n",
      "|           attaining|    1|\n",
      "|          remarkably|    1|\n",
      "|             dictate|    1|\n",
      "|            scouring|    1|\n",
      "|           budgeting|    1|\n",
      "|                late|    1|\n",
      "|             shocked|    1|\n",
      "|     recommendations|    1|\n",
      "|            absorbed|    1|\n",
      "|            catching|    1|\n",
      "|               acqui|    1|\n",
      "|                 dad|    1|\n",
      "|          scientific|    1|\n",
      "|             pricing|    1|\n",
      "|         notoriously|    1|\n",
      "|          revisiting|    1|\n",
      "|           describes|    1|\n",
      "|            released|    1|\n",
      "|               pizza|    1|\n",
      "|           stylistic|    1|\n",
      "|            preserve|    1|\n",
      "|             regards|    1|\n",
      "|             emulate|    1|\n",
      "|       differentiate|    1|\n",
      "|              delete|    1|\n",
      "|              expand|    1|\n",
      "|              valued|    1|\n",
      "|         negotiating|    1|\n",
      "|          derivative|    1|\n",
      "|          subsidiary|    1|\n",
      "|            pursuing|    1|\n",
      "|           september|    1|\n",
      "|            mitigate|    1|\n",
      "|             defeats|    1|\n",
      "|             prudent|    1|\n",
      "|               gloat|    1|\n",
      "|          consistent|    1|\n",
      "|            spinning|    1|\n",
      "|             passive|    1|\n",
      "|            mainstay|    1|\n",
      "|              browse|    1|\n",
      "|           unbounded|    1|\n",
      "|                ramp|    1|\n",
      "|            commuted|    1|\n",
      "|               crowd|    1|\n",
      "|     accomplishments|    1|\n",
      "|                dies|    1|\n",
      "|               spins|    1|\n",
      "|            finances|    1|\n",
      "|             checked|    1|\n",
      "|          brainstorm|    1|\n",
      "|              bridge|    1|\n",
      "|      substantiation|    1|\n",
      "|           brookings|    1|\n",
      "|          connecting|    1|\n",
      "|              astute|    1|\n",
      "|             remains|    1|\n",
      "|            practice|    1|\n",
      "|                 mom|    1|\n",
      "|           rendering|    1|\n",
      "|             knowing|    1|\n",
      "|              stable|    1|\n",
      "|            outdated|    1|\n",
      "|             noticed|    1|\n",
      "|                  05|    1|\n",
      "|             desired|    1|\n",
      "|                  w9|    1|\n",
      "|                  de|    1|\n",
      "|              scheme|    1|\n",
      "|            aversion|    1|\n",
      "|              lacked|    1|\n",
      "|           extorting|    1|\n",
      "|               stars|    1|\n",
      "|            competes|    1|\n",
      "|            informal|    1|\n",
      "|              braces|    1|\n",
      "|         appreciates|    1|\n",
      "|                 css|    1|\n",
      "|       helpareporter|    1|\n",
      "|              russia|    1|\n",
      "|                  68|    1|\n",
      "|           incurring|    1|\n",
      "|            domestic|    1|\n",
      "|                  60|    1|\n",
      "|           paralegal|    1|\n",
      "|                hung|    1|\n",
      "|             vehicle|    1|\n",
      "|            republic|    1|\n",
      "|                  49|    1|\n",
      "|            sticking|    1|\n",
      "|                 caf|    1|\n",
      "|              grumpy|    1|\n",
      "|               lasts|    1|\n",
      "|               scope|    1|\n",
      "|               segue|    1|\n",
      "|          challenges|    1|\n",
      "|              varies|    1|\n",
      "|            ulterior|    1|\n",
      "|           dominates|    1|\n",
      "|             pitfall|    1|\n",
      "|          multiplied|    1|\n",
      "|               bonds|    1|\n",
      "|             awesome|    1|\n",
      "|            gathered|    1|\n",
      "|           household|    1|\n",
      "|               weigh|    1|\n",
      "|           upholding|    1|\n",
      "|      conversational|    1|\n",
      "|            blocking|    1|\n",
      "|                   h|    1|\n",
      "|               heart|    1|\n",
      "|           repeating|    1|\n",
      "|              hassle|    1|\n",
      "|            personas|    1|\n",
      "|                york|    1|\n",
      "|               fully|    1|\n",
      "|                 104|    1|\n",
      "|       familiarizing|    1|\n",
      "|           frequency|    1|\n",
      "|                 max|    1|\n",
      "|              spam13|    1|\n",
      "|            children|    1|\n",
      "|      insurmountably|    1|\n",
      "|            average6|    1|\n",
      "|          perpetuity|    1|\n",
      "|       entertainment|    1|\n",
      "|                 png|    1|\n",
      "|              inform|    1|\n",
      "|          electrical|    1|\n",
      "|       participating|    1|\n",
      "|           telescope|    1|\n",
      "|                   b|    1|\n",
      "|             refines|    1|\n",
      "|              venues|    1|\n",
      "|          retweeting|    1|\n",
      "|               hired|    1|\n",
      "|          sponsoring|    1|\n",
      "|             freeing|    1|\n",
      "|                grew|    1|\n",
      "|             feeling|    1|\n",
      "|             dykstra|    1|\n",
      "|                deep|    1|\n",
      "|      unidirectional|    1|\n",
      "|             matches|    1|\n",
      "|         furthermore|    1|\n",
      "|         billionaire|    1|\n",
      "|                slim|    1|\n",
      "|            succinct|    1|\n",
      "|              builds|    1|\n",
      "|             strings|    1|\n",
      "|         extrapolate|    1|\n",
      "|            answered|    1|\n",
      "|              tricky|    1|\n",
      "|             painful|    1|\n",
      "|       complimentary|    1|\n",
      "|           published|    1|\n",
      "|               assed|    1|\n",
      "|           notorious|    1|\n",
      "|               inbox|    1|\n",
      "|              subset|    1|\n",
      "|               freed|    1|\n",
      "|                  65|    1|\n",
      "|     congratulations|    1|\n",
      "|             justify|    1|\n",
      "|          manageable|    1|\n",
      "|          proverbial|    1|\n",
      "|              jargon|    1|\n",
      "|                spec|    1|\n",
      "|              juices|    1|\n",
      "|                w3tc|    1|\n",
      "|             gaining|    1|\n",
      "|             opinion|    1|\n",
      "|           selecting|    1|\n",
      "|       conversations|    1|\n",
      "|                swat|    1|\n",
      "|              years2|    1|\n",
      "|                 312|    1|\n",
      "|           directing|    1|\n",
      "|             batched|    1|\n",
      "|         provisional|    1|\n",
      "|           penalties|    1|\n",
      "|                bars|    1|\n",
      "|              uniloc|    1|\n",
      "|              mantra|    1|\n",
      "|            mentions|    1|\n",
      "|         calculating|    1|\n",
      "|         philippines|    1|\n",
      "|           control10|    1|\n",
      "|          inadequate|    1|\n",
      "|              advise|    1|\n",
      "|            opposite|    1|\n",
      "|             bridges|    1|\n",
      "|               lined|    1|\n",
      "|           appeasing|    1|\n",
      "|           libraries|    1|\n",
      "|          separation|    1|\n",
      "|           indicates|    1|\n",
      "|          dimensions|    1|\n",
      "|                  19|    1|\n",
      "|            returned|    1|\n",
      "|                zoho|    1|\n",
      "|             subside|    1|\n",
      "|         unsolicited|    1|\n",
      "|          determined|    1|\n",
      "|           believing|    1|\n",
      "|               print|    1|\n",
      "|             hearing|    1|\n",
      "|               crown|    1|\n",
      "|            dreaming|    1|\n",
      "|               begin|    1|\n",
      "|             pirates|    1|\n",
      "|                 cdc|    1|\n",
      "|              worlds|    1|\n",
      "|              asleep|    1|\n",
      "|              canada|    1|\n",
      "|             fearing|    1|\n",
      "|              tahiti|    1|\n",
      "|             inbound|    1|\n",
      "|              deeply|    1|\n",
      "|          identified|    1|\n",
      "|                owns|    1|\n",
      "|           plentiful|    1|\n",
      "|          graciously|    1|\n",
      "|            contents|    1|\n",
      "|             climate|    1|\n",
      "|               ratio|    1|\n",
      "|            draining|    1|\n",
      "|                 faa|    1|\n",
      "|          complaints|    1|\n",
      "|              pirate|    1|\n",
      "|            shutdown|    1|\n",
      "|               facts|    1|\n",
      "|              versus|    1|\n",
      "|           collected|    1|\n",
      "|             catches|    1|\n",
      "|                tune|    1|\n",
      "|              corner|    1|\n",
      "|            achieves|    1|\n",
      "|                sink|    1|\n",
      "|               older|    1|\n",
      "|                trek|    1|\n",
      "|            withhold|    1|\n",
      "|       manufacturers|    1|\n",
      "|            cleaners|    1|\n",
      "|          partnering|    1|\n",
      "|           criticism|    1|\n",
      "|              attach|    1|\n",
      "|            whatsapp|    1|\n",
      "|        continuously|    1|\n",
      "|          continuous|    1|\n",
      "|               proud|    1|\n",
      "|                owes|    1|\n",
      "|           enjoyable|    1|\n",
      "|           revisited|    1|\n",
      "|        presentation|    1|\n",
      "|              lenses|    1|\n",
      "|          collateral|    1|\n",
      "|           proofread|    1|\n",
      "|           expansion|    1|\n",
      "|          overstated|    1|\n",
      "|           editorial|    1|\n",
      "|                 aws|    1|\n",
      "|                   r|    1|\n",
      "|                eyes|    1|\n",
      "|              waited|    1|\n",
      "|           regularly|    1|\n",
      "|             webpage|    1|\n",
      "|                star|    1|\n",
      "|             manages|    1|\n",
      "|                  vs|    1|\n",
      "|             caching|    1|\n",
      "|                  12|    1|\n",
      "|                roof|    1|\n",
      "|          indicative|    1|\n",
      "|               tenet|    1|\n",
      "|          preserving|    1|\n",
      "|         restriction|    1|\n",
      "|         merchandise|    1|\n",
      "|          medication|    1|\n",
      "|          progressed|    1|\n",
      "|               gauge|    1|\n",
      "|             directx|    1|\n",
      "|              placed|    1|\n",
      "|              injury|    1|\n",
      "|             manning|    1|\n",
      "|         memorabilia|    1|\n",
      "|             chapter|    1|\n",
      "|            quantify|    1|\n",
      "|             betting|    1|\n",
      "|               shops|    1|\n",
      "|               burst|    1|\n",
      "|            shortage|    1|\n",
      "|              crafts|    1|\n",
      "|             weather|    1|\n",
      "|              occurs|    1|\n",
      "|          foreigners|    1|\n",
      "|              crises|    1|\n",
      "|            consumed|    1|\n",
      "|          delivering|    1|\n",
      "|              stolen|    1|\n",
      "|            artisans|    1|\n",
      "|              opened|    1|\n",
      "|       brainstorming|    1|\n",
      "|                 aid|    1|\n",
      "|             goodbye|    1|\n",
      "|               ducks|    1|\n",
      "|          complicate|    1|\n",
      "|            settings|    1|\n",
      "|            incurred|    1|\n",
      "|            penalize|    1|\n",
      "|             payroll|    1|\n",
      "|             repairs|    1|\n",
      "|          reasonably|    1|\n",
      "|         certificate|    1|\n",
      "|         cornerstone|    1|\n",
      "|             fulfill|    1|\n",
      "|              clever|    1|\n",
      "|         requirments|    1|\n",
      "|         appointment|    1|\n",
      "|    disproportionate|    1|\n",
      "|                bing|    1|\n",
      "|            fatigued|    1|\n",
      "|              reacts|    1|\n",
      "|             disease|    1|\n",
      "|                asia|    1|\n",
      "|               cloud|    1|\n",
      "|              fixing|    1|\n",
      "|             risking|    1|\n",
      "|              enlist|    1|\n",
      "|              rumsey|    1|\n",
      "|                  13|    1|\n",
      "|             hurting|    1|\n",
      "|            weekends|    1|\n",
      "|              counts|    1|\n",
      "|        esthetically|    1|\n",
      "|            maritime|    1|\n",
      "|        municipality|    1|\n",
      "|                  07|    1|\n",
      "|                east|    1|\n",
      "|          engagement|    1|\n",
      "|                  14|    1|\n",
      "|           committed|    1|\n",
      "|           convinced|    1|\n",
      "|               shock|    1|\n",
      "|           disrupted|    1|\n",
      "|                aspx|    1|\n",
      "|           memorable|    1|\n",
      "|         communicate|    1|\n",
      "|            receipts|    1|\n",
      "|             culture|    1|\n",
      "|          wireframes|    1|\n",
      "|           contained|    1|\n",
      "|             thrived|    1|\n",
      "|        independence|    1|\n",
      "|           downsized|    1|\n",
      "|          assistants|    1|\n",
      "|             romance|    1|\n",
      "|             tactful|    1|\n",
      "|               alter|    1|\n",
      "|               silly|    1|\n",
      "|          extensible|    1|\n",
      "|                tone|    1|\n",
      "|             examine|    1|\n",
      "|        proofreading|    1|\n",
      "|        congratulate|    1|\n",
      "|                nasa|    1|\n",
      "|         compromises|    1|\n",
      "|              psyche|    1|\n",
      "|             focuses|    1|\n",
      "|          prioritize|    1|\n",
      "|             conform|    1|\n",
      "|       unsubscribing|    1|\n",
      "|           deadlines|    1|\n",
      "|                prod|    1|\n",
      "|              dashes|    1|\n",
      "|                 pro|    1|\n",
      "|            advanced|    1|\n",
      "|             fortune|    1|\n",
      "|             tropics|    1|\n",
      "|             advance|    1|\n",
      "|             cheaply|    1|\n",
      "|             suburbs|    1|\n",
      "|           tgeonetta|    1|\n",
      "|              barely|    1|\n",
      "|               fudge|    1|\n",
      "|           desirable|    1|\n",
      "|           conflicts|    1|\n",
      "|        unimaginably|    1|\n",
      "|              neomek|    1|\n",
      "|              plenty|    1|\n",
      "|             deviate|    1|\n",
      "|            identity|    1|\n",
      "|             outdoor|    1|\n",
      "|          unemployed|    1|\n",
      "|             history|    1|\n",
      "|            concrete|    1|\n",
      "|             entrust|    1|\n",
      "|         journalists|    1|\n",
      "|                 lap|    1|\n",
      "|             restate|    1|\n",
      "|          explicitly|    1|\n",
      "|              hosted|    1|\n",
      "|                 ons|    1|\n",
      "|             forcing|    1|\n",
      "|          complement|    1|\n",
      "|        improvements|    1|\n",
      "|            tricking|    1|\n",
      "|                 cup|    1|\n",
      "|               miles|    1|\n",
      "|             hobbies|    1|\n",
      "|            wardrobe|    1|\n",
      "|         environment|    1|\n",
      "|            interact|    1|\n",
      "|           exercises|    1|\n",
      "|             passage|    1|\n",
      "|               drawn|    1|\n",
      "|            paranoia|    1|\n",
      "|            talented|    1|\n",
      "|          imperative|    1|\n",
      "|              linked|    1|\n",
      "|             artists|    1|\n",
      "|            adaptive|    1|\n",
      "|                trip|    1|\n",
      "|         enforceable|    1|\n",
      "|         unavailable|    1|\n",
      "|                 opt|    1|\n",
      "|          occupation|    1|\n",
      "|             binging|    1|\n",
      "|                 usa|    1|\n",
      "|                hats|    1|\n",
      "|         distasteful|    1|\n",
      "|             happier|    1|\n",
      "|              hinges|    1|\n",
      "|           listening|    1|\n",
      "|             parking|    1|\n",
      "|               area5|    1|\n",
      "|                firm|    1|\n",
      "|            jeopardy|    1|\n",
      "|           completed|    1|\n",
      "|             alibaba|    1|\n",
      "|           specialty|    1|\n",
      "|            abstract|    1|\n",
      "|adsenseformobileapps|    1|\n",
      "|               acted|    1|\n",
      "|          ridiculous|    1|\n",
      "|               gates|    1|\n",
      "|            mailings|    1|\n",
      "|                 inc|    1|\n",
      "|               shots|    1|\n",
      "|      conservatively|    1|\n",
      "|           recurring|    1|\n",
      "|              tricks|    1|\n",
      "|             credits|    1|\n",
      "|              regret|    1|\n",
      "|                xxxx|    1|\n",
      "|           afterward|    1|\n",
      "|            luxuries|    1|\n",
      "|               added|    1|\n",
      "|          workaholic|    1|\n",
      "|            workweek|    1|\n",
      "|        distractions|    1|\n",
      "|          admittedly|    1|\n",
      "|            internal|    1|\n",
      "|           wonderful|    1|\n",
      "|          skepticism|    1|\n",
      "|                beer|    1|\n",
      "|                belt|    1|\n",
      "|             brought|    1|\n",
      "|           permanent|    1|\n",
      "|            multiply|    1|\n",
      "|          impressive|    1|\n",
      "|          paragraphs|    1|\n",
      "|            arriving|    1|\n",
      "|           appealing|    1|\n",
      "|           performed|    1|\n",
      "|          excitement|    1|\n",
      "|             noodles|    1|\n",
      "|           anecdotes|    1|\n",
      "|            withdraw|    1|\n",
      "|               unity|    1|\n",
      "|            visiting|    1|\n",
      "|            transfer|    1|\n",
      "|           amazingly|    1|\n",
      "|         interviewed|    1|\n",
      "|            overlook|    1|\n",
      "|          discussion|    1|\n",
      "|            burdened|    1|\n",
      "|              grades|    1|\n",
      "|         ineffective|    1|\n",
      "|            depleted|    1|\n",
      "|                jose|    1|\n",
      "|              supply|    1|\n",
      "|               tough|    1|\n",
      "|            flipside|    1|\n",
      "|                corp|    1|\n",
      "|              nearly|    1|\n",
      "|              native|    1|\n",
      "|               panic|    1|\n",
      "|                dhhs|    1|\n",
      "|              relied|    1|\n",
      "|                kits|    1|\n",
      "|             beliefs|    1|\n",
      "|          prototyped|    1|\n",
      "|              arises|    1|\n",
      "|              extort|    1|\n",
      "|             lowered|    1|\n",
      "|                 row|    1|\n",
      "|             founded|    1|\n",
      "|        frustrations|    1|\n",
      "|               pitch|    1|\n",
      "|              seeing|    1|\n",
      "|           replacing|    1|\n",
      "|            oriented|    1|\n",
      "|                lays|    1|\n",
      "|               dodgy|    1|\n",
      "|              spammy|    1|\n",
      "|             variety|    1|\n",
      "|               asked|    1|\n",
      "|          supporting|    1|\n",
      "|           undefined|    1|\n",
      "|           severance|    1|\n",
      "|                solo|    1|\n",
      "|            assembly|    1|\n",
      "|        manufacturer|    1|\n",
      "|                  93|    1|\n",
      "|             judging|    1|\n",
      "|                hold|    1|\n",
      "|             revolve|    1|\n",
      "|               quiet|    1|\n",
      "|                norm|    1|\n",
      "|            inviting|    1|\n",
      "|              status|    1|\n",
      "|              nudges|    1|\n",
      "|            proceeds|    1|\n",
      "|               lease|    1|\n",
      "|          membership|    1|\n",
      "|             alleged|    1|\n",
      "|         foreseeable|    1|\n",
      "|            distance|    1|\n",
      "|            sketches|    1|\n",
      "|           finalized|    1|\n",
      "|          legitimate|    1|\n",
      "|          population|    1|\n",
      "|               maker|    1|\n",
      "|             disable|    1|\n",
      "|         susceptible|    1|\n",
      "|               ramen|    1|\n",
      "|         unsubscribe|    1|\n",
      "|          rationally|    1|\n",
      "|           patenting|    1|\n",
      "|           testified|    1|\n",
      "|         hyphenation|    1|\n",
      "|                bots|    1|\n",
      "|          presumably|    1|\n",
      "|          equivalent|    1|\n",
      "|           landscape|    1|\n",
      "|            stunning|    1|\n",
      "|             vampire|    1|\n",
      "|             specify|    1|\n",
      "|       substantially|    1|\n",
      "|              latest|    1|\n",
      "|             joining|    1|\n",
      "|          committing|    1|\n",
      "|              wonder|    1|\n",
      "|            multiple|    1|\n",
      "|                myth|    1|\n",
      "|               flows|    1|\n",
      "|             roofing|    1|\n",
      "|             opening|    1|\n",
      "|           postcards|    1|\n",
      "|               edits|    1|\n",
      "|       subscriptions|    1|\n",
      "|              upbeat|    1|\n",
      "|               youth|    1|\n",
      "|            snippets|    1|\n",
      "|         advertorial|    1|\n",
      "|            wondered|    1|\n",
      "|        calculations|    1|\n",
      "|            rewarded|    1|\n",
      "|           teammates|    1|\n",
      "|          convenient|    1|\n",
      "|         contradicts|    1|\n",
      "|               tends|    1|\n",
      "|              austin|    1|\n",
      "|            stresses|    1|\n",
      "|           openforum|    1|\n",
      "|           conducted|    1|\n",
      "|          coordinate|    1|\n",
      "|               yahoo|    1|\n",
      "|         association|    1|\n",
      "|             reaches|    1|\n",
      "|            conflict|    1|\n",
      "|               salon|    1|\n",
      "|              garage|    1|\n",
      "|           newspaper|    1|\n",
      "|             coached|    1|\n",
      "|               novel|    1|\n",
      "|     unceremoniously|    1|\n",
      "|                 air|    1|\n",
      "|              robust|    1|\n",
      "|          intangible|    1|\n",
      "|            affected|    1|\n",
      "|            caffeine|    1|\n",
      "|                tens|    1|\n",
      "|              mainly|    1|\n",
      "|              notion|    1|\n",
      "|          innovation|    1|\n",
      "|            notifies|    1|\n",
      "|              retire|    1|\n",
      "|              tenure|    1|\n",
      "|                dead|    1|\n",
      "|              scares|    1|\n",
      "|          moderately|    1|\n",
      "|        metropolitan|    1|\n",
      "|                 law|    1|\n",
      "|            wasteful|    1|\n",
      "|             wallets|    1|\n",
      "|                 via|    1|\n",
      "|                fein|    1|\n",
      "|              secret|    1|\n",
      "|                  h1|    1|\n",
      "|           telephone|    1|\n",
      "|           retention|    1|\n",
      "|         overwhelmed|    1|\n",
      "|         constrained|    1|\n",
      "|                grab|    1|\n",
      "|              lowest|    1|\n",
      "|                pull|    1|\n",
      "|          requesting|    1|\n",
      "|              losses|    1|\n",
      "|        constructive|    1|\n",
      "|                spot|    1|\n",
      "|           seriously|    1|\n",
      "|               dries|    1|\n",
      "|           audiences|    1|\n",
      "|           deviation|    1|\n",
      "|             behaved|    1|\n",
      "|              arcade|    1|\n",
      "|               weeks|    1|\n",
      "|              handed|    1|\n",
      "|           qualified|    1|\n",
      "|                laid|    1|\n",
      "|              walked|    1|\n",
      "|           beautiful|    1|\n",
      "|            marketer|    1|\n",
      "|              killed|    1|\n",
      "|              summit|    1|\n",
      "|           dividends|    1|\n",
      "|              assess|    1|\n",
      "|               rough|    1|\n",
      "|            thousand|    1|\n",
      "|        transactions|    1|\n",
      "|           authorize|    1|\n",
      "|            december|    1|\n",
      "|                  vr|    1|\n",
      "|               speed|    1|\n",
      "|              loaded|    1|\n",
      "|         accompanies|    1|\n",
      "|          frequently|    1|\n",
      "|             burning|    1|\n",
      "|              strike|    1|\n",
      "|          conference|    1|\n",
      "|                foam|    1|\n",
      "|              derail|    1|\n",
      "|             contain|    1|\n",
      "|              sanity|    1|\n",
      "|          translates|    1|\n",
      "|            tweeting|    1|\n",
      "|             bizarre|    1|\n",
      "|        environments|    1|\n",
      "|              framed|    1|\n",
      "|              steers|    1|\n",
      "|                  15|    1|\n",
      "|          newsworthy|    1|\n",
      "|              unsure|    1|\n",
      "|       authoritative|    1|\n",
      "|            headache|    1|\n",
      "|           advocates|    1|\n",
      "|              daniel|    1|\n",
      "|             assumes|    1|\n",
      "|         landscapers|    1|\n",
      "|          represents|    1|\n",
      "|              menial|    1|\n",
      "|               newly|    1|\n",
      "|         indignities|    1|\n",
      "|             respect|    1|\n",
      "|              filled|    1|\n",
      "|           directors|    1|\n",
      "|             tuition|    1|\n",
      "|                guts|    1|\n",
      "|           highlight|    1|\n",
      "|            violated|    1|\n",
      "|         requirement|    1|\n",
      "|              alerts|    1|\n",
      "|                cave|    1|\n",
      "|          fraudulent|    1|\n",
      "|              escort|    1|\n",
      "|         transferred|    1|\n",
      "|           uncertain|    1|\n",
      "|            approval|    1|\n",
      "|             younger|    1|\n",
      "|       symmetrically|    1|\n",
      "|                host|    1|\n",
      "|            surfaces|    1|\n",
      "|        contributing|    1|\n",
      "|            virtuous|    1|\n",
      "|     businessinsider|    1|\n",
      "|          surprising|    1|\n",
      "|          absolutely|    1|\n",
      "|               batch|    1|\n",
      "|           preserved|    1|\n",
      "|               fares|    1|\n",
      "|              bought|    1|\n",
      "|             healthy|    1|\n",
      "|              burned|    1|\n",
      "|          programmer|    1|\n",
      "|                joke|    1|\n",
      "|              slowly|    1|\n",
      "|          exceptions|    1|\n",
      "|              device|    1|\n",
      "|              secure|    1|\n",
      "|              minded|    1|\n",
      "|              merely|    1|\n",
      "|            faceless|    1|\n",
      "|            reliever|    1|\n",
      "|               dealt|    1|\n",
      "|           unrelated|    1|\n",
      "|              habits|    1|\n",
      "|            flexible|    1|\n",
      "|             depicts|    1|\n",
      "|                dots|    1|\n",
      "|            retailer|    1|\n",
      "|           backwards|    1|\n",
      "|              retail|    1|\n",
      "|     confidentiality|    1|\n",
      "|        deliverables|    1|\n",
      "|            diligent|    1|\n",
      "|       determination|    1|\n",
      "|          achievable|    1|\n",
      "|             spotted|    1|\n",
      "|           powerless|    1|\n",
      "|           notebooks|    1|\n",
      "|           learnings|    1|\n",
      "|            pleasure|    1|\n",
      "|               frame|    1|\n",
      "|                bugs|    1|\n",
      "|             desktop|    1|\n",
      "|               frees|    1|\n",
      "|          protecting|    1|\n",
      "|               movie|    1|\n",
      "|                outs|    1|\n",
      "|            marriage|    1|\n",
      "|          unexpected|    1|\n",
      "|            composed|    1|\n",
      "|           invisible|    1|\n",
      "|             machine|    1|\n",
      "|              employ|    1|\n",
      "|        metaphorical|    1|\n",
      "|                  28|    1|\n",
      "|            relative|    1|\n",
      "|         millionaire|    1|\n",
      "|          generating|    1|\n",
      "|              fueled|    1|\n",
      "|             overall|    1|\n",
      "|          affiliated|    1|\n",
      "|               forum|    1|\n",
      "|         advancement|    1|\n",
      "|              years3|    1|\n",
      "|              opengl|    1|\n",
      "|              ending|    1|\n",
      "|            gambling|    1|\n",
      "|            smallest|    1|\n",
      "|             sorting|    1|\n",
      "|               twice|    1|\n",
      "|            calendar|    1|\n",
      "|                html|    1|\n",
      "|          meticulous|    1|\n",
      "|           radically|    1|\n",
      "|          unfinished|    1|\n",
      "|               plays|    1|\n",
      "|             needing|    1|\n",
      "|         endorsement|    1|\n",
      "|             welcome|    1|\n",
      "|          foreclosed|    1|\n",
      "|                  82|    1|\n",
      "|           announced|    1|\n",
      "|           religious|    1|\n",
      "|         highlighted|    1|\n",
      "|             offices|    1|\n",
      "|          landscaper|    1|\n",
      "|             promote|    1|\n",
      "|            salesman|    1|\n",
      "|             returns|    1|\n",
      "|                fiji|    1|\n",
      "|           delegated|    1|\n",
      "|                  18|    1|\n",
      "|          incentives|    1|\n",
      "|                file|    1|\n",
      "|           commodity|    1|\n",
      "|               angry|    1|\n",
      "|              catchy|    1|\n",
      "|             worries|    1|\n",
      "|              exists|    1|\n",
      "|             journey|    1|\n",
      "|           ultimatum|    1|\n",
      "|             riskier|    1|\n",
      "|           squatters|    1|\n",
      "|            deciding|    1|\n",
      "|             saddled|    1|\n",
      "|           moleskine|    1|\n",
      "|          geographic|    1|\n",
      "|              agenda|    1|\n",
      "|            friendly|    1|\n",
      "|               react|    1|\n",
      "|          journalism|    1|\n",
      "|             answers|    1|\n",
      "|                hurt|    1|\n",
      "|          oftentimes|    1|\n",
      "|              unsold|    1|\n",
      "|             parties|    1|\n",
      "|          benefits12|    1|\n",
      "|           overspend|    1|\n",
      "|          outsourced|    1|\n",
      "|          publishing|    1|\n",
      "|          combinator|    1|\n",
      "|           prevented|    1|\n",
      "|         discontinue|    1|\n",
      "|           subscribe|    1|\n",
      "|             educate|    1|\n",
      "|                  oh|    1|\n",
      "|               beach|    1|\n",
      "|           shrinking|    1|\n",
      "|          completing|    1|\n",
      "|       massachusetts|    1|\n",
      "|         publicizing|    1|\n",
      "|           tolerance|    1|\n",
      "|            exercise|    1|\n",
      "|            skillset|    1|\n",
      "|          concretely|    1|\n",
      "|          compensate|    1|\n",
      "|               guard|    1|\n",
      "|         questioning|    1|\n",
      "|        introductory|    1|\n",
      "|              armies|    1|\n",
      "|             seasick|    1|\n",
      "|           similarly|    1|\n",
      "|               gamed|    1|\n",
      "|            smashing|    1|\n",
      "|                  70|    1|\n",
      "|             studios|    1|\n",
      "|            national|    1|\n",
      "|             lessons|    1|\n",
      "|                race|    1|\n",
      "|            relaxing|    1|\n",
      "|         instructors|    1|\n",
      "|              niches|    1|\n",
      "|             cushion|    1|\n",
      "|                 125|    1|\n",
      "|           maximizes|    1|\n",
      "|            imported|    1|\n",
      "|              window|    1|\n",
      "|       undeliverable|    1|\n",
      "|               lives|    1|\n",
      "|               cease|    1|\n",
      "|             notable|    1|\n",
      "|              attack|    1|\n",
      "|        entrepreneur|    1|\n",
      "|          subsidized|    1|\n",
      "|            tenacity|    1|\n",
      "|            quantity|    1|\n",
      "|               scare|    1|\n",
      "|          everywhere|    1|\n",
      "|            instinct|    1|\n",
      "|             consume|    1|\n",
      "|               block|    1|\n",
      "|               blame|    1|\n",
      "|             concert|    1|\n",
      "|            patterns|    1|\n",
      "|              formed|    1|\n",
      "|              refund|    1|\n",
      "|              unused|    1|\n",
      "|          friendlier|    1|\n",
      "|           regarding|    1|\n",
      "|               award|    1|\n",
      "|                   k|    1|\n",
      "|              limits|    1|\n",
      "|             pockets|    1|\n",
      "|          forgivable|    1|\n",
      "|             iterate|    1|\n",
      "|                mile|    1|\n",
      "|          dealership|    1|\n",
      "|                date|    1|\n",
      "|               eight|    1|\n",
      "|             fullest|    1|\n",
      "|        unproductive|    1|\n",
      "|               lakes|    1|\n",
      "|             figured|    1|\n",
      "|            audience|    1|\n",
      "|             ratings|    1|\n",
      "|          collection|    1|\n",
      "|                 cat|    1|\n",
      "|          washington|    1|\n",
      "|           correlate|    1|\n",
      "|           graduated|    1|\n",
      "|        electricians|    1|\n",
      "|                tiny|    1|\n",
      "|              switch|    1|\n",
      "|             discuss|    1|\n",
      "|           reporting|    1|\n",
      "|               opens|    1|\n",
      "|       environmental|    1|\n",
      "|           disposing|    1|\n",
      "|               brand|    1|\n",
      "|            variance|    1|\n",
      "|                 man|    1|\n",
      "|           realities|    1|\n",
      "|             suspect|    1|\n",
      "|                lazy|    1|\n",
      "|           originate|    1|\n",
      "|             treated|    1|\n",
      "|                 dry|    1|\n",
      "|              artist|    1|\n",
      "|               badly|    1|\n",
      "|                lock|    1|\n",
      "|                bare|    1|\n",
      "|            homework|    1|\n",
      "|            improves|    1|\n",
      "|               doubt|    1|\n",
      "|           detection|    1|\n",
      "|         astronomers|    1|\n",
      "|           technique|    1|\n",
      "|                 alt|    1|\n",
      "|                dada|    1|\n",
      "|                 b2b|    1|\n",
      "|                feet|    1|\n",
      "|               draft|    1|\n",
      "|                bold|    1|\n",
      "|          specialist|    1|\n",
      "|              weekly|    1|\n",
      "|             himself|    1|\n",
      "|           obamacare|    1|\n",
      "|                   y|    1|\n",
      "|            starving|    1|\n",
      "|            dentists|    1|\n",
      "|         deductibles|    1|\n",
      "|               codes|    1|\n",
      "|               cults|    1|\n",
      "|      recommendation|    1|\n",
      "|                exam|    1|\n",
      "|                rise|    1|\n",
      "|          consulting|    1|\n",
      "|               talks|    1|\n",
      "|           returning|    1|\n",
      "|            distract|    1|\n",
      "|              booths|    1|\n",
      "|            diminish|    1|\n",
      "|             helpful|    1|\n",
      "|                rays|    1|\n",
      "|               tries|    1|\n",
      "|         substantial|    1|\n",
      "|          confidence|    1|\n",
      "|            brochure|    1|\n",
      "|             heading|    1|\n",
      "|              combat|    1|\n",
      "|           acquirers|    1|\n",
      "|         accelerator|    1|\n",
      "|             drained|    1|\n",
      "|          generation|    1|\n",
      "|          continuing|    1|\n",
      "|          characters|    1|\n",
      "|            realized|    1|\n",
      "|               reply|    1|\n",
      "|              liking|    1|\n",
      "|             issuing|    1|\n",
      "|            deserves|    1|\n",
      "|                fold|    1|\n",
      "|              thanks|    1|\n",
      "|                load|    1|\n",
      "|       encouragement|    1|\n",
      "|         constructed|    1|\n",
      "|           continued|    1|\n",
      "|               20081|    1|\n",
      "|              graphs|    1|\n",
      "|         distinction|    1|\n",
      "|          companions|    1|\n",
      "|           traveling|    1|\n",
      "|              spared|    1|\n",
      "|            traction|    1|\n",
      "|              houses|    1|\n",
      "|                2009|    1|\n",
      "|           treatment|    1|\n",
      "|          refinement|    2|\n",
      "|            devoting|    2|\n",
      "|          artificial|    2|\n",
      "|                wrap|    2|\n",
      "|       experimenting|    2|\n",
      "|           emergency|    2|\n",
      "|            activity|    2|\n",
      "|              hugely|    2|\n",
      "|            resulted|    2|\n",
      "|              finish|    2|\n",
      "|        prescription|    2|\n",
      "|             awarded|    2|\n",
      "|                prey|    2|\n",
      "|             honesty|    2|\n",
      "|                 bob|    2|\n",
      "|              waking|    2|\n",
      "|                 xxx|    2|\n",
      "|                poor|    2|\n",
      "|               shake|    2|\n",
      "|           relevance|    2|\n",
      "|          accustomed|    2|\n",
      "|           provision|    2|\n",
      "|                menu|    2|\n",
      "|                gone|    2|\n",
      "|            exposure|    2|\n",
      "|            involves|    2|\n",
      "|           therefore|    2|\n",
      "|           statistic|    2|\n",
      "|                seas|    2|\n",
      "|           establish|    2|\n",
      "|              devote|    2|\n",
      "|             acquire|    2|\n",
      "|             grossly|    2|\n",
      "|             imagine|    2|\n",
      "|             graphic|    2|\n",
      "|           tradeoffs|    2|\n",
      "|         compounding|    2|\n",
      "|           perceived|    2|\n",
      "|                wish|    2|\n",
      "|            payments|    2|\n",
      "|           predators|    2|\n",
      "|              escape|    2|\n",
      "|                exit|    2|\n",
      "|        concentrated|    2|\n",
      "|                 box|    2|\n",
      "|                puts|    2|\n",
      "|               penny|    2|\n",
      "|         eliminating|    2|\n",
      "|           temporary|    2|\n",
      "|          situations|    2|\n",
      "|                arms|    2|\n",
      "|             picture|    2|\n",
      "|                imdb|    2|\n",
      "|             loyalty|    2|\n",
      "|        interpreting|    2|\n",
      "|             extreme|    2|\n",
      "|               store|    2|\n",
      "|               guest|    2|\n",
      "|           estimated|    2|\n",
      "|            examples|    2|\n",
      "|                demo|    2|\n",
      "|              slices|    2|\n",
      "|          describing|    2|\n",
      "|               crash|    2|\n",
      "|           direction|    2|\n",
      "|           awareness|    2|\n",
      "|            striking|    2|\n",
      "|        indefinitely|    2|\n",
      "|             leaning|    2|\n",
      "|         spreadsheet|    2|\n",
      "|             medical|    2|\n",
      "|                 tie|    2|\n",
      "|          meaningful|    2|\n",
      "|          assumption|    2|\n",
      "|                fits|    2|\n",
      "|              doctor|    2|\n",
      "|            convince|    2|\n",
      "|           exception|    2|\n",
      "|              wanted|    2|\n",
      "|            thoughts|    2|\n",
      "|              served|    2|\n",
      "|               shark|    2|\n",
      "|                room|    2|\n",
      "|           challenge|    2|\n",
      "|             reports|    2|\n",
      "|             clearly|    2|\n",
      "|          announcing|    2|\n",
      "|               cobra|    2|\n",
      "|              bother|    2|\n",
      "|          inequities|    2|\n",
      "|             private|    2|\n",
      "|                zone|    2|\n",
      "|               hobby|    2|\n",
      "|              series|    2|\n",
      "|       bootstrapping|    2|\n",
      "|              except|    2|\n",
      "|             lawyers|    2|\n",
      "|                 win|    2|\n",
      "|             analyze|    2|\n",
      "|             precise|    2|\n",
      "|               young|    2|\n",
      "|         temporarily|    2|\n",
      "|          algorithms|    2|\n",
      "|               shell|    2|\n",
      "|            reliably|    2|\n",
      "|              earned|    2|\n",
      "|          impossible|    2|\n",
      "|              proved|    2|\n",
      "|           guarantee|    2|\n",
      "|          newspapers|    2|\n",
      "|            behavior|    2|\n",
      "|           dependent|    2|\n",
      "|               deals|    2|\n",
      "|        subscription|    2|\n",
      "|           increased|    2|\n",
      "|         capitalists|    2|\n",
      "|          contribute|    2|\n",
      "|                ebay|    2|\n",
      "|        technologies|    2|\n",
      "|            limiting|    2|\n",
      "|             easiest|    2|\n",
      "|               prior|    2|\n",
      "|              appeal|    2|\n",
      "|            precious|    2|\n",
      "|          experiment|    2|\n",
      "|                duty|    2|\n",
      "|        intimidating|    2|\n",
      "|          networking|    2|\n",
      "|            pitfalls|    2|\n",
      "|          discovered|    2|\n",
      "|          maximizing|    2|\n",
      "|               score|    2|\n",
      "|            refining|    2|\n",
      "|              prefer|    2|\n",
      "|           consuming|    2|\n",
      "|           meanwhile|    2|\n",
      "|             taxable|    2|\n",
      "|               prove|    2|\n",
      "|           workforce|    2|\n",
      "|         determining|    2|\n",
      "|         application|    2|\n",
      "|             premium|    2|\n",
      "|              lately|    2|\n",
      "|         considering|    2|\n",
      "|              define|    2|\n",
      "|            overcome|    2|\n",
      "|                says|    2|\n",
      "|              though|    2|\n",
      "|             default|    2|\n",
      "|          automobile|    2|\n",
      "|             printed|    2|\n",
      "|                fair|    2|\n",
      "|       consideration|    2|\n",
      "|                kill|    2|\n",
      "|           subsidies|    2|\n",
      "|          submitting|    2|\n",
      "|           reminders|    2|\n",
      "|              models|    2|\n",
      "|            sections|    2|\n",
      "|           precisely|    2|\n",
      "|              chosen|    2|\n",
      "|                gave|    2|\n",
      "|            straight|    2|\n",
      "|          shoestring|    2|\n",
      "|        restrictions|    2|\n",
      "|            politics|    2|\n",
      "|           connected|    2|\n",
      "|              filing|    2|\n",
      "|            rankings|    2|\n",
      "|              scales|    2|\n",
      "|            explicit|    2|\n",
      "|                auto|    2|\n",
      "|         unnecessary|    2|\n",
      "|            director|    2|\n",
      "|            pressure|    2|\n",
      "|             cheaper|    2|\n",
      "|                wait|    2|\n",
      "|         originating|    2|\n",
      "|             ranking|    2|\n",
      "|              geared|    2|\n",
      "|        moonlighting|    2|\n",
      "|             science|    2|\n",
      "|          separately|    2|\n",
      "|                 bid|    2|\n",
      "|             filling|    2|\n",
      "|              leaves|    2|\n",
      "|                 dpi|    2|\n",
      "|               proxy|    2|\n",
      "|           motivates|    2|\n",
      "|               teach|    2|\n",
      "|         established|    2|\n",
      "|             relying|    2|\n",
      "|               women|    2|\n",
      "|             budgets|    2|\n",
      "|            announce|    2|\n",
      "|               enter|    2|\n",
      "|            feelings|    2|\n",
      "|                 mvp|    2|\n",
      "|         demographic|    2|\n",
      "|               cards|    2|\n",
      "|      geographically|    2|\n",
      "|           trademark|    2|\n",
      "|            choosing|    2|\n",
      "|               teams|    2|\n",
      "|           validates|    2|\n",
      "|             approve|    2|\n",
      "|           malicious|    2|\n",
      "|           hyperbole|    2|\n",
      "|           integrate|    2|\n",
      "|               thank|    2|\n",
      "|              adjust|    2|\n",
      "|              hawaii|    2|\n",
      "|              behalf|    2|\n",
      "|             defined|    2|\n",
      "|              stands|    2|\n",
      "|              scroll|    2|\n",
      "|                 die|    2|\n",
      "|       powerlessness|    2|\n",
      "|              refine|    2|\n",
      "|             decided|    2|\n",
      "|            changing|    2|\n",
      "|            coursera|    2|\n",
      "|             feature|    2|\n",
      "|                bust|    2|\n",
      "|              tailor|    2|\n",
      "|               moved|    2|\n",
      "|             reasons|    2|\n",
      "|               clean|    2|\n",
      "|           attendees|    2|\n",
      "|            accounts|    2|\n",
      "|               feels|    2|\n",
      "|            breaking|    2|\n",
      "|                card|    2|\n",
      "|             leeches|    2|\n",
      "|                 gap|    2|\n",
      "|            vacation|    2|\n",
      "|          estimating|    2|\n",
      "|          underneath|    2|\n",
      "|          convention|    2|\n",
      "|            educated|    2|\n",
      "|              outlay|    2|\n",
      "|                join|    2|\n",
      "|        construction|    2|\n",
      "|             spammer|    2|\n",
      "|            polluted|    2|\n",
      "|       accomplishing|    2|\n",
      "|             waiting|    2|\n",
      "|            windfall|    2|\n",
      "|              mostly|    2|\n",
      "|            commonly|    2|\n",
      "|            thankful|    2|\n",
      "|          incredibly|    2|\n",
      "|              naming|    2|\n",
      "|              hardly|    2|\n",
      "|                 cut|    2|\n",
      "|              behave|    2|\n",
      "|             bankers|    2|\n",
      "|            envelope|    2|\n",
      "|               cache|    2|\n",
      "|             clarify|    2|\n",
      "|               funds|    2|\n",
      "|       realistically|    2|\n",
      "|           schedules|    2|\n",
      "|               adapt|    2|\n",
      "|         independent|    2|\n",
      "|             removes|    2|\n",
      "|               typed|    2|\n",
      "|                play|    2|\n",
      "|               looks|    2|\n",
      "|          disability|    2|\n",
      "|                ries|    2|\n",
      "|           practical|    2|\n",
      "|             possess|    2|\n",
      "|              member|    2|\n",
      "|               niosh|    2|\n",
      "|            adapting|    2|\n",
      "|             calling|    2|\n",
      "|          executives|    2|\n",
      "|             society|    2|\n",
      "|                whom|    2|\n",
      "|            external|    2|\n",
      "|          activities|    2|\n",
      "|                tank|    2|\n",
      "|           consumers|    2|\n",
      "|            complete|    2|\n",
      "|             changed|    2|\n",
      "|              wealth|    2|\n",
      "|       unfortunately|    2|\n",
      "|           increases|    2|\n",
      "|             payment|    2|\n",
      "|              raises|    2|\n",
      "|             demands|    2|\n",
      "|              tuning|    2|\n",
      "|           countless|    2|\n",
      "|              emerge|    2|\n",
      "|             fueling|    2|\n",
      "|        geographical|    2|\n",
      "|              street|    2|\n",
      "|             stopped|    2|\n",
      "|             creates|    2|\n",
      "|             barrier|    2|\n",
      "|            medicare|    2|\n",
      "|         transaction|    2|\n",
      "|               staff|    2|\n",
      "|           cleanings|    2|\n",
      "|               incur|    2|\n",
      "|          disclaimer|    2|\n",
      "|                slow|    2|\n",
      "|               crazy|    2|\n",
      "|                 hal|    2|\n",
      "|              videos|    2|\n",
      "|               fancy|    2|\n",
      "|           aerospace|    2|\n",
      "|                  3d|    2|\n",
      "|            speaking|    2|\n",
      "|                apps|    2|\n",
      "|                  29|    2|\n",
      "|             adsense|    2|\n",
      "|                copy|    2|\n",
      "|                luck|    2|\n",
      "|               slice|    2|\n",
      "|                rare|    2|\n",
      "|                toot|    2|\n",
      "|          smartphone|    2|\n",
      "|            previous|    2|\n",
      "|             revisit|    2|\n",
      "|             whoever|    2|\n",
      "|          integrated|    2|\n",
      "|             sitting|    2|\n",
      "|            figuring|    2|\n",
      "|            managing|    2|\n",
      "|                rafi|    2|\n",
      "|         meaningless|    2|\n",
      "|           computers|    2|\n",
      "|               tried|    2|\n",
      "|        ridiculously|    2|\n",
      "|                park|    2|\n",
      "|              ignore|    2|\n",
      "|               guilt|    2|\n",
      "|              pushed|    2|\n",
      "|               stand|    2|\n",
      "|       transitioning|    2|\n",
      "|               theme|    2|\n",
      "|          sufficient|    2|\n",
      "|            provider|    2|\n",
      "|              filter|    2|\n",
      "|          disturbing|    2|\n",
      "|            factored|    2|\n",
      "|              ticket|    2|\n",
      "|                 fix|    2|\n",
      "|            advisors|    2|\n",
      "|            contains|    2|\n",
      "|              sexism|    2|\n",
      "|           webmaster|    2|\n",
      "|             anymore|    2|\n",
      "|                  w2|    2|\n",
      "|             ferriss|    2|\n",
      "|                food|    2|\n",
      "|                harm|    2|\n",
      "|             pulling|    2|\n",
      "|            shipping|    2|\n",
      "|           practices|    2|\n",
      "|               exams|    2|\n",
      "|               treat|    2|\n",
      "|               minds|    2|\n",
      "|             housing|    2|\n",
      "|            patented|    2|\n",
      "|         comfortably|    2|\n",
      "|             tablets|    2|\n",
      "|            policies|    2|\n",
      "|             reuters|    2|\n",
      "|              region|    2|\n",
      "|         accountants|    2|\n",
      "|       instructional|    2|\n",
      "|             federal|    2|\n",
      "|             seconds|    2|\n",
      "|          seamlessly|    2|\n",
      "|          occasional|    2|\n",
      "|             decline|    2|\n",
      "|              closer|    2|\n",
      "|         disciplines|    2|\n",
      "|              orders|    2|\n",
      "|         immediately|    2|\n",
      "|             caution|    2|\n",
      "|          definitely|    2|\n",
      "|              issues|    2|\n",
      "|          contracted|    2|\n",
      "|               white|    2|\n",
      "|            original|    2|\n",
      "|                laws|    2|\n",
      "|        increasingly|    2|\n",
      "|              deeper|    2|\n",
      "|         essentially|    2|\n",
      "|           responses|    2|\n",
      "|             meeting|    2|\n",
      "|             glasses|    2|\n",
      "|                tags|    2|\n",
      "|                  55|    2|\n",
      "|            variants|    2|\n",
      "|               refer|    2|\n",
      "|                 arm|    2|\n",
      "|              spends|    2|\n",
      "|             organic|    2|\n",
      "|            frequent|    2|\n",
      "|               match|    2|\n",
      "|             posting|    2|\n",
      "|            invoices|    2|\n",
      "|               grown|    2|\n",
      "|                 ban|    2|\n",
      "|              wisdom|    2|\n",
      "|               relax|    2|\n",
      "|            founders|    2|\n",
      "|              editor|    2|\n",
      "|              spread|    2|\n",
      "|           political|    2|\n",
      "|               skill|    2|\n",
      "|             explore|    2|\n",
      "|             fallacy|    2|\n",
      "|                vast|    2|\n",
      "|           knowledge|    2|\n",
      "|          supportive|    2|\n",
      "|          complexity|    2|\n",
      "|               hands|    2|\n",
      "|             pressed|    2|\n",
      "|          commitment|    2|\n",
      "|             hanging|    2|\n",
      "|          healthcare|    2|\n",
      "|     acknowledgments|    2|\n",
      "|          automating|    2|\n",
      "|          california|    2|\n",
      "|               bonus|    2|\n",
      "|             secrets|    2|\n",
      "|                 tab|    2|\n",
      "|        consultation|    2|\n",
      "|         enthusiasts|    2|\n",
      "|              picked|    2|\n",
      "|             amateur|    2|\n",
      "|               carry|    2|\n",
      "|                pile|    2|\n",
      "|             doctors|    2|\n",
      "|             trapped|    2|\n",
      "|         engineering|    2|\n",
      "|          submission|    2|\n",
      "|        organization|    2|\n",
      "|              insert|    2|\n",
      "|              gospel|    2|\n",
      "|              worker|    2|\n",
      "|           universal|    2|\n",
      "|        housekeepers|    2|\n",
      "|                  pc|    2|\n",
      "|             reviews|    2|\n",
      "|                wear|    2|\n",
      "|            improved|    2|\n",
      "|           suppliers|    2|\n",
      "|         newsletters|    2|\n",
      "|         governments|    2|\n",
      "|                 bls|    2|\n",
      "|          measurable|    2|\n",
      "|             digging|    2|\n",
      "|               globe|    2|\n",
      "|               turns|    2|\n",
      "|              diving|    2|\n",
      "|              census|    2|\n",
      "|       professionals|    2|\n",
      "|         subscribers|    2|\n",
      "|             shelton|    2|\n",
      "|          likelihood|    2|\n",
      "|          television|    2|\n",
      "|             explain|    2|\n",
      "|             prevent|    2|\n",
      "|       circumstances|    2|\n",
      "|            periodic|    2|\n",
      "|             worried|    2|\n",
      "|              raised|    2|\n",
      "|           instagram|    2|\n",
      "|             experts|    2|\n",
      "|               uncle|    2|\n",
      "|             complex|    2|\n",
      "|            discount|    2|\n",
      "|            billable|    2|\n",
      "|                   x|    2|\n",
      "|               meets|    2|\n",
      "|                iraq|    2|\n",
      "|                 101|    2|\n",
      "|             helping|    2|\n",
      "|              serves|    2|\n",
      "|          disappears|    2|\n",
      "|                 him|    2|\n",
      "|            throwing|    2|\n",
      "|              orange|    2|\n",
      "|           variation|    2|\n",
      "|            magazine|    2|\n",
      "|             bonuses|    2|\n",
      "|           daughters|    2|\n",
      "|       intentionally|    2|\n",
      "|               tying|    2|\n",
      "|             exceeds|    2|\n",
      "|           bootstrap|    2|\n",
      "|            programs|    2|\n",
      "|             sharing|    2|\n",
      "|           embargoed|    2|\n",
      "|            gloating|    2|\n",
      "|          particular|    2|\n",
      "|        consistently|    2|\n",
      "|         alternative|    2|\n",
      "|                wife|    2|\n",
      "|             covered|    2|\n",
      "|           magazines|    2|\n",
      "|                aids|    2|\n",
      "|                keen|    2|\n",
      "|             attract|    2|\n",
      "|            handling|    2|\n",
      "|              belong|    2|\n",
      "|               loans|    2|\n",
      "|             wasting|    2|\n",
      "|             domains|    2|\n",
      "|            variable|    2|\n",
      "|               trend|    2|\n",
      "|             sources|    2|\n",
      "|              seemed|    2|\n",
      "|         projections|    2|\n",
      "|          specialize|    2|\n",
      "|           necessity|    2|\n",
      "|              forget|    2|\n",
      "|                2013|    2|\n",
      "|                fine|    2|\n",
      "|             totally|    2|\n",
      "|              visual|    2|\n",
      "|               mouth|    2|\n",
      "|           presented|    2|\n",
      "|           education|    2|\n",
      "|            recently|    2|\n",
      "|             biggest|    2|\n",
      "|               apart|    2|\n",
      "|         bookkeeping|    2|\n",
      "|           configure|    2|\n",
      "|          incubators|    2|\n",
      "|           construct|    2|\n",
      "|            profiles|    2|\n",
      "|             balance|    2|\n",
      "|               table|    2|\n",
      "|           improving|    2|\n",
      "|            reserved|    2|\n",
      "|              friend|    2|\n",
      "|        distributing|    2|\n",
      "|                  99|    2|\n",
      "|          copyrights|    2|\n",
      "|               udemy|    2|\n",
      "|          strategist|    2|\n",
      "|               guide|    2|\n",
      "|                body|    2|\n",
      "|            matching|    2|\n",
      "|              recent|    2|\n",
      "|             plugins|    2|\n",
      "|             offline|    2|\n",
      "|                 sum|    2|\n",
      "|             costing|    2|\n",
      "|          mitigation|    2|\n",
      "|                lens|    2|\n",
      "|            mohammed|    2|\n",
      "|             library|    2|\n",
      "|          navigation|    2|\n",
      "|       unnecessarily|    2|\n",
      "|                horn|    2|\n",
      "|                 bus|    2|\n",
      "|            ultimate|    2|\n",
      "|           reputable|    2|\n",
      "|               major|    2|\n",
      "|              riches|    2|\n",
      "|          congregate|    2|\n",
      "|             remnant|    2|\n",
      "|             visitor|    2|\n",
      "|                bear|    2|\n",
      "|              modify|    2|\n",
      "|                  ii|    2|\n",
      "|          subscriber|    2|\n",
      "|               nudge|    2|\n",
      "|              forbes|    2|\n",
      "|              author|    2|\n",
      "|             flowing|    2|\n",
      "|            dropping|    2|\n",
      "|             preface|    2|\n",
      "|              grants|    2|\n",
      "|           automatic|    2|\n",
      "|          leveraging|    2|\n",
      "|                 fee|    2|\n",
      "|            database|    2|\n",
      "|           influence|    2|\n",
      "|                 iii|    2|\n",
      "|               typos|    2|\n",
      "|                  75|    2|\n",
      "|               light|    2|\n",
      "|         integration|    2|\n",
      "|              events|    2|\n",
      "|           solutions|    2|\n",
      "|       sophisticated|    2|\n",
      "|           endeavors|    2|\n",
      "|              gather|    2|\n",
      "|             headers|    2|\n",
      "|                kept|    2|\n",
      "|             reached|    2|\n",
      "|                 lay|    2|\n",
      "|                head|    2|\n",
      "|           optimized|    2|\n",
      "|              copies|    2|\n",
      "|              embark|    2|\n",
      "|                mess|    2|\n",
      "|        silverlining|    2|\n",
      "|               split|    2|\n",
      "|            partners|    2|\n",
      "|               chaos|    2|\n",
      "|           gathering|    2|\n",
      "|                 bar|    2|\n",
      "|               weren|    2|\n",
      "|             staying|    2|\n",
      "|              screen|    2|\n",
      "|                  pm|    2|\n",
      "|          physically|    2|\n",
      "|           projected|    2|\n",
      "|            survivor|    2|\n",
      "|           depicting|    2|\n",
      "|           licensing|    2|\n",
      "|            versions|    2|\n",
      "|                etsy|    2|\n",
      "|        overestimate|    2|\n",
      "|               serve|    2|\n",
      "|             hostage|    2|\n",
      "|              dollar|    2|\n",
      "|              richer|    2|\n",
      "|         beneficiary|    2|\n",
      "|             employs|    2|\n",
      "|           followers|    2|\n",
      "|            consumer|    2|\n",
      "|              payoff|    2|\n",
      "|              horror|    2|\n",
      "|              nature|    2|\n",
      "|          overcoming|    2|\n",
      "|          deductions|    2|\n",
      "|               fresh|    2|\n",
      "|             despite|    2|\n",
      "|             inertia|    2|\n",
      "|                wave|    2|\n",
      "|            produced|    2|\n",
      "|          competitor|    2|\n",
      "|            mentally|    2|\n",
      "|              suffer|    2|\n",
      "|                desk|    2|\n",
      "|      advertisements|    2|\n",
      "|     representatives|    2|\n",
      "|            indicate|    2|\n",
      "|        helplessness|    2|\n",
      "|             doubled|    2|\n",
      "|              stages|    2|\n",
      "|         manufacture|    2|\n",
      "|             blanket|    2|\n",
      "|           executive|    2|\n",
      "|                  41|    2|\n",
      "|             choices|    2|\n",
      "|            somebody|    2|\n",
      "|               skies|    2|\n",
      "|             lightly|    2|\n",
      "|               party|    2|\n",
      "|              waters|    3|\n",
      "|               drain|    3|\n",
      "|           generates|    3|\n",
      "|              shared|    3|\n",
      "|           expertise|    3|\n",
      "|                seen|    3|\n",
      "|            includes|    3|\n",
      "|                 irs|    3|\n",
      "|              covers|    3|\n",
      "|               watch|    3|\n",
      "|              asking|    3|\n",
      "|               limit|    3|\n",
      "|            tempting|    3|\n",
      "|               noise|    3|\n",
      "|           publicity|    3|\n",
      "|              senior|    3|\n",
      "|            earnings|    3|\n",
      "|             streams|    3|\n",
      "|              listen|    3|\n",
      "|               ready|    3|\n",
      "|                code|    3|\n",
      "|             bedroom|    3|\n",
      "|          commercial|    3|\n",
      "|              michal|    3|\n",
      "|            addition|    3|\n",
      "|              inside|    3|\n",
      "|            ensuring|    3|\n",
      "|            download|    3|\n",
      "|                 app|    3|\n",
      "|              double|    3|\n",
      "|          incubation|    3|\n",
      "|         effectively|    3|\n",
      "|            property|    3|\n",
      "|               risks|    3|\n",
      "|         penetration|    3|\n",
      "|             install|    3|\n",
      "|               smart|    3|\n",
      "|              plugin|    3|\n",
      "|               merit|    3|\n",
      "|          throughout|    3|\n",
      "|                burn|    3|\n",
      "|           copyright|    3|\n",
      "|             clarity|    3|\n",
      "|             exactly|    3|\n",
      "|               exact|    3|\n",
      "|             patents|    3|\n",
      "|            allowing|    3|\n",
      "|               range|    3|\n",
      "|         sustainable|    3|\n",
      "|         measurement|    3|\n",
      "|          minimizing|    3|\n",
      "|                 eye|    3|\n",
      "|             careers|    3|\n",
      "|            learning|    3|\n",
      "|            wherever|    3|\n",
      "|           estimates|    3|\n",
      "|             monitor|    3|\n",
      "|            approach|    3|\n",
      "|            analysis|    3|\n",
      "|               worst|    3|\n",
      "|                note|    3|\n",
      "|           promising|    3|\n",
      "|            language|    3|\n",
      "|       inadvertently|    3|\n",
      "|                type|    3|\n",
      "|           simulator|    3|\n",
      "|        intellectual|    3|\n",
      "|         resignation|    3|\n",
      "|            agilesrc|    3|\n",
      "|         possibility|    3|\n",
      "|          extensions|    3|\n",
      "|             funding|    3|\n",
      "|             amounts|    3|\n",
      "|                  62|    3|\n",
      "|         introducing|    3|\n",
      "|              button|    3|\n",
      "|             weekend|    3|\n",
      "|              worthy|    3|\n",
      "|             version|    3|\n",
      "|               steps|    3|\n",
      "|                 hit|    3|\n",
      "|              clicks|    3|\n",
      "|              losing|    3|\n",
      "|               touch|    3|\n",
      "|              viewed|    3|\n",
      "|          commission|    3|\n",
      "|         surrounding|    3|\n",
      "|                  22|    3|\n",
      "|              afraid|    3|\n",
      "|                ties|    3|\n",
      "|          reputation|    3|\n",
      "|           obtaining|    3|\n",
      "|             billion|    3|\n",
      "|               ideal|    3|\n",
      "|            compared|    3|\n",
      "|           eliminate|    3|\n",
      "|               yield|    3|\n",
      "|      professionally|    3|\n",
      "|         downloading|    3|\n",
      "|            happened|    3|\n",
      "|           represent|    3|\n",
      "|         complicated|    3|\n",
      "|               cheap|    3|\n",
      "|             dummies|    3|\n",
      "|              school|    3|\n",
      "|            confused|    3|\n",
      "|             ensured|    3|\n",
      "|          replicated|    3|\n",
      "|            military|    3|\n",
      "|               broad|    3|\n",
      "|                 moz|    3|\n",
      "|             capture|    3|\n",
      "|           confident|    3|\n",
      "|         participate|    3|\n",
      "|              charge|    3|\n",
      "|         potentially|    3|\n",
      "|             failure|    3|\n",
      "|               earns|    3|\n",
      "|              brings|    3|\n",
      "|              fairly|    3|\n",
      "|        conventional|    3|\n",
      "|           generated|    3|\n",
      "|         necessarily|    3|\n",
      "|               lived|    3|\n",
      "|              levels|    3|\n",
      "|                   c|    3|\n",
      "|            millions|    3|\n",
      "|        announcement|    3|\n",
      "|                tips|    3|\n",
      "|              guilty|    3|\n",
      "|            barriers|    3|\n",
      "|                wary|    3|\n",
      "|            leverage|    3|\n",
      "|               farms|    3|\n",
      "|            informed|    3|\n",
      "|                feed|    3|\n",
      "|              taught|    3|\n",
      "|    entrepreneurship|    3|\n",
      "|             planned|    3|\n",
      "|             bargain|    3|\n",
      "|              unlike|    3|\n",
      "|              polish|    3|\n",
      "|             perform|    3|\n",
      "|              funded|    3|\n",
      "|          transition|    3|\n",
      "|             clicked|    3|\n",
      "|           offerings|    3|\n",
      "|               sizes|    3|\n",
      "|               shows|    3|\n",
      "|             trained|    3|\n",
      "|         acquisition|    3|\n",
      "|           abilities|    3|\n",
      "|             stories|    3|\n",
      "|              minute|    3|\n",
      "|              resist|    3|\n",
      "|            standard|    3|\n",
      "|              harder|    3|\n",
      "|                 art|    3|\n",
      "|                  11|    3|\n",
      "|          profitable|    3|\n",
      "|              prices|    3|\n",
      "|            constant|    3|\n",
      "|                 dig|    3|\n",
      "|         corporation|    3|\n",
      "|             portion|    3|\n",
      "|        alternatives|    3|\n",
      "|             augment|    3|\n",
      "|               likes|    3|\n",
      "|                 ego|    3|\n",
      "|       prescriptions|    3|\n",
      "|                eric|    3|\n",
      "|             economy|    3|\n",
      "|                sake|    3|\n",
      "|              extend|    3|\n",
      "|           interview|    3|\n",
      "|               below|    3|\n",
      "|                safe|    3|\n",
      "|         freelancing|    3|\n",
      "|             headset|    3|\n",
      "|      administration|    3|\n",
      "|         prospective|    3|\n",
      "|              former|    3|\n",
      "|            reaching|    3|\n",
      "|               cause|    3|\n",
      "|               radio|    3|\n",
      "|              cities|    3|\n",
      "|              detail|    3|\n",
      "|             central|    3|\n",
      "|               paper|    3|\n",
      "|         outsourcing|    3|\n",
      "|             shouldn|    3|\n",
      "|                hate|    3|\n",
      "|         importantly|    3|\n",
      "|           paperwork|    3|\n",
      "|              evolve|    3|\n",
      "|             updates|    3|\n",
      "|               night|    3|\n",
      "|             permits|    3|\n",
      "|       announcements|    3|\n",
      "|                  90|    3|\n",
      "|             council|    3|\n",
      "|              solves|    3|\n",
      "|              proper|    3|\n",
      "|             testing|    3|\n",
      "|             seattle|    3|\n",
      "|         responsibly|    3|\n",
      "|           primarily|    3|\n",
      "|                soft|    3|\n",
      "|               drive|    3|\n",
      "|             vesting|    3|\n",
      "|         distracting|    3|\n",
      "|         expectation|    3|\n",
      "|              annual|    3|\n",
      "|              wasted|    3|\n",
      "|          responsive|    3|\n",
      "|           replicate|    3|\n",
      "|            included|    3|\n",
      "|             aspects|    3|\n",
      "|           referrals|    3|\n",
      "|             improve|    3|\n",
      "|          trademarks|    3|\n",
      "|             missing|    3|\n",
      "|              metric|    3|\n",
      "|               peers|    3|\n",
      "|             obvious|    3|\n",
      "|          statements|    3|\n",
      "|              client|    3|\n",
      "|            proposed|    3|\n",
      "|        surprisingly|    3|\n",
      "|             peoples|    3|\n",
      "|          eventually|    3|\n",
      "|              moment|    3|\n",
      "|               tread|    3|\n",
      "|            sessions|    3|\n",
      "|         accompanied|    3|\n",
      "|               aaron|    3|\n",
      "|            mortgage|    3|\n",
      "|           literally|    3|\n",
      "|               quick|    3|\n",
      "|          connection|    3|\n",
      "|             regular|    3|\n",
      "|            publicly|    3|\n",
      "|             targets|    3|\n",
      "|           currently|    3|\n",
      "|            grabbing|    3|\n",
      "|            strongly|    3|\n",
      "|         accordingly|    3|\n",
      "|           employers|    3|\n",
      "|               daily|    3|\n",
      "|              ageism|    3|\n",
      "|          advertiser|    3|\n",
      "|              lasted|    3|\n",
      "|                2010|    3|\n",
      "|               truly|    3|\n",
      "|                true|    3|\n",
      "|              hourly|    3|\n",
      "|              behind|    3|\n",
      "|               amass|    3|\n",
      "|            decrease|    3|\n",
      "|                math|    3|\n",
      "|             twofold|    3|\n",
      "|           stressful|    3|\n",
      "|                 pre|    3|\n",
      "|               empty|    3|\n",
      "|              offset|    3|\n",
      "|         interaction|    3|\n",
      "|          university|    3|\n",
      "|               field|    3|\n",
      "|              rights|    3|\n",
      "|               legal|    3|\n",
      "|               power|    3|\n",
      "|                shop|    3|\n",
      "|             parents|    3|\n",
      "|              adding|    3|\n",
      "|           workplace|    3|\n",
      "|              beauty|    3|\n",
      "|              failed|    3|\n",
      "|         proposition|    3|\n",
      "|                ring|    3|\n",
      "|             smaller|    3|\n",
      "|         contracting|    3|\n",
      "|              tablet|    3|\n",
      "|                  27|    3|\n",
      "|      responsibility|    3|\n",
      "|                  46|    3|\n",
      "|            resource|    3|\n",
      "|         inexpensive|    3|\n",
      "|              equity|    3|\n",
      "|              bureau|    3|\n",
      "|             trigger|    3|\n",
      "|                 crm|    3|\n",
      "|         underserved|    3|\n",
      "|               catch|    3|\n",
      "|             depends|    3|\n",
      "|             student|    3|\n",
      "|              planet|    3|\n",
      "|          temptation|    3|\n",
      "|                fear|    3|\n",
      "|             virtual|    3|\n",
      "|                sent|    3|\n",
      "|              titles|    3|\n",
      "|           encourage|    3|\n",
      "|          livelihood|    3|\n",
      "|           platforms|    3|\n",
      "|               kinds|    3|\n",
      "|           genuinely|    3|\n",
      "|             talents|    3|\n",
      "|                walk|    3|\n",
      "|          restricted|    3|\n",
      "|              timely|    3|\n",
      "|             connect|    3|\n",
      "|            accident|    3|\n",
      "|           purchased|    3|\n",
      "|               agree|    3|\n",
      "|            achieved|    3|\n",
      "|           checklist|    3|\n",
      "|             effects|    3|\n",
      "|            referred|    3|\n",
      "|       knowledgeable|    3|\n",
      "|           targeting|    3|\n",
      "|         encouraging|    3|\n",
      "|            accepted|    3|\n",
      "|              manage|    3|\n",
      "|              aspect|    3|\n",
      "|            succeeds|    3|\n",
      "|              amazed|    3|\n",
      "|           encounter|    3|\n",
      "|              drives|    4|\n",
      "|               among|    4|\n",
      "|         competitors|    4|\n",
      "|             subject|    4|\n",
      "|        requirements|    4|\n",
      "|               trick|    4|\n",
      "|              places|    4|\n",
      "|           flowchart|    4|\n",
      "|            becoming|    4|\n",
      "|                 saw|    4|\n",
      "|                felt|    4|\n",
      "|               break|    4|\n",
      "|               helps|    4|\n",
      "|               basis|    4|\n",
      "|               parts|    4|\n",
      "|                rift|    4|\n",
      "|           contracts|    4|\n",
      "|             leading|    4|\n",
      "|            searched|    4|\n",
      "|              gamble|    4|\n",
      "|      specifications|    4|\n",
      "|             managed|    4|\n",
      "|          difference|    4|\n",
      "|              strong|    4|\n",
      "|             channel|    4|\n",
      "|            creation|    4|\n",
      "|             convert|    4|\n",
      "|            platform|    4|\n",
      "|           designing|    4|\n",
      "|               demos|    4|\n",
      "|           worthless|    4|\n",
      "|             involve|    4|\n",
      "|          accomplish|    4|\n",
      "|              widely|    4|\n",
      "|           producing|    4|\n",
      "|               bring|    4|\n",
      "|            response|    4|\n",
      "|             handled|    4|\n",
      "|           efficient|    4|\n",
      "|                spam|    4|\n",
      "|            clicking|    4|\n",
      "|               craft|    4|\n",
      "|             minutes|    4|\n",
      "|                 sky|    4|\n",
      "|             quarter|    4|\n",
      "|            together|    4|\n",
      "|            acquired|    4|\n",
      "|          belongings|    4|\n",
      "|                bill|    4|\n",
      "|           engineers|    4|\n",
      "|               notes|    4|\n",
      "|                lead|    4|\n",
      "|          retirement|    4|\n",
      "|           alongside|    4|\n",
      "|          evaluating|    4|\n",
      "|             license|    4|\n",
      "|           authority|    4|\n",
      "|          assessment|    4|\n",
      "|              source|    4|\n",
      "|             present|    4|\n",
      "|               trial|    4|\n",
      "|           extremely|    4|\n",
      "|          quickbooks|    4|\n",
      "|              couple|    4|\n",
      "|                gain|    4|\n",
      "|         individuals|    4|\n",
      "|               favor|    4|\n",
      "|           motivated|    4|\n",
      "|                love|    4|\n",
      "|              impact|    4|\n",
      "|               study|    4|\n",
      "|            reliable|    4|\n",
      "|           continues|    4|\n",
      "|             keyword|    4|\n",
      "|               throw|    4|\n",
      "|              active|    4|\n",
      "|                  co|    4|\n",
      "|             excited|    4|\n",
      "|               trust|    4|\n",
      "|       manufacturing|    4|\n",
      "|         flexibility|    4|\n",
      "|              public|    4|\n",
      "|             morning|    4|\n",
      "|             forward|    4|\n",
      "|              actual|    4|\n",
      "|            anywhere|    4|\n",
      "|              moving|    4|\n",
      "|         challenging|    4|\n",
      "|           worldwide|    4|\n",
      "|              degree|    4|\n",
      "|                hang|    4|\n",
      "|                fake|    4|\n",
      "|         experiences|    4|\n",
      "|             trouble|    4|\n",
      "|              update|    4|\n",
      "|       significantly|    4|\n",
      "|            optimize|    4|\n",
      "|            projects|    4|\n",
      "|               style|    4|\n",
      "|                2015|    4|\n",
      "|             changes|    4|\n",
      "|             letting|    4|\n",
      "|              oceans|    4|\n",
      "|              afford|    4|\n",
      "|                busy|    4|\n",
      "|             replace|    4|\n",
      "|              design|    4|\n",
      "|        unemployment|    4|\n",
      "|           dedicated|    4|\n",
      "|               tells|    4|\n",
      "|              showed|    4|\n",
      "|               rules|    4|\n",
      "|           involving|    4|\n",
      "|             solving|    4|\n",
      "|             purpose|    4|\n",
      "|         financially|    4|\n",
      "|             showing|    4|\n",
      "|            unlikely|    4|\n",
      "|          collecting|    4|\n",
      "|              format|    4|\n",
      "|              steady|    4|\n",
      "|                 eat|    4|\n",
      "|              report|    4|\n",
      "|                wide|    4|\n",
      "|             becomes|    4|\n",
      "|            remotely|    4|\n",
      "|               shown|    4|\n",
      "|            majority|    4|\n",
      "|              salary|    4|\n",
      "|             options|    4|\n",
      "|            american|    4|\n",
      "|               views|    4|\n",
      "|               keeps|    4|\n",
      "|                pool|    4|\n",
      "|           prospects|    4|\n",
      "|                team|    4|\n",
      "|               fired|    4|\n",
      "|              gotten|    4|\n",
      "|               aware|    4|\n",
      "|            detailed|    4|\n",
      "|         specializes|    4|\n",
      "|         competitive|    4|\n",
      "|             produce|    4|\n",
      "|                2011|    4|\n",
      "|              pocket|    4|\n",
      "|           developed|    4|\n",
      "|                  40|    4|\n",
      "|            startups|    4|\n",
      "|               built|    4|\n",
      "|                leap|    4|\n",
      "|          discipline|    4|\n",
      "|                tool|    4|\n",
      "|              carrot|    4|\n",
      "|            followed|    4|\n",
      "|           adventure|    4|\n",
      "|              signed|    4|\n",
      "|              decent|    4|\n",
      "|         identifying|    4|\n",
      "|            provided|    4|\n",
      "|             driving|    4|\n",
      "|          industries|    4|\n",
      "|             arrange|    4|\n",
      "|            engineer|    4|\n",
      "|               weird|    4|\n",
      "|             systems|    4|\n",
      "|          infringing|    4|\n",
      "|               issue|    4|\n",
      "|         discussions|    4|\n",
      "|                 ago|    4|\n",
      "|          definition|    4|\n",
      "|             putting|    4|\n",
      "|            familiar|    4|\n",
      "|               armed|    4|\n",
      "|              gaming|    4|\n",
      "|              mental|    4|\n",
      "|               china|    4|\n",
      "|              forces|    4|\n",
      "|            premiums|    4|\n",
      "|              modern|    4|\n",
      "|                drop|    4|\n",
      "|             execute|    4|\n",
      "|             closing|    4|\n",
      "|       independently|    4|\n",
      "|            expected|    4|\n",
      "|               surge|    4|\n",
      "|               guess|    4|\n",
      "|           objective|    4|\n",
      "|             offered|    4|\n",
      "|              narrow|    4|\n",
      "|              simple|    4|\n",
      "|                quiz|    4|\n",
      "|               faced|    4|\n",
      "|          conversion|    4|\n",
      "|              access|    4|\n",
      "|         maintenance|    4|\n",
      "|               title|    4|\n",
      "|              rather|    4|\n",
      "|                 gov|    4|\n",
      "|               speak|    4|\n",
      "|               count|    4|\n",
      "|                plus|    4|\n",
      "|               calls|    4|\n",
      "|             article|    4|\n",
      "|            contrast|    4|\n",
      "|           relations|    4|\n",
      "|             survive|    4|\n",
      "|               depth|    4|\n",
      "|            searches|    4|\n",
      "|             courses|    4|\n",
      "|               third|    4|\n",
      "|               posts|    4|\n",
      "|                cold|    4|\n",
      "|               clear|    4|\n",
      "|               labor|    4|\n",
      "|            exciting|    4|\n",
      "|                sold|    4|\n",
      "|               final|    4|\n",
      "|              wisely|    4|\n",
      "|             readers|    4|\n",
      "|           mailchimp|    4|\n",
      "|              visits|    4|\n",
      "|           mentioned|    4|\n",
      "|             protect|    4|\n",
      "|                pick|    4|\n",
      "|                 fit|    4|\n",
      "|                push|    4|\n",
      "|         responsible|    4|\n",
      "|            supposed|    4|\n",
      "|              effect|    4|\n",
      "|                pays|    4|\n",
      "|             compare|    4|\n",
      "|              retain|    4|\n",
      "|            positive|    5|\n",
      "|            received|    5|\n",
      "|        conversation|    5|\n",
      "|          evaluation|    5|\n",
      "|           statement|    5|\n",
      "|                 our|    5|\n",
      "|                   7|    5|\n",
      "|                rank|    5|\n",
      "|               waste|    5|\n",
      "|             thought|    5|\n",
      "|            economic|    5|\n",
      "|                farm|    5|\n",
      "|                hand|    5|\n",
      "|         conversions|    5|\n",
      "|               sells|    5|\n",
      "|             numbers|    5|\n",
      "|            negative|    5|\n",
      "|              states|    5|\n",
      "|                 age|    5|\n",
      "|          simulation|    5|\n",
      "|                step|    5|\n",
      "|                near|    5|\n",
      "|               space|    5|\n",
      "|           surprised|    5|\n",
      "|                mass|    5|\n",
      "|            referrer|    5|\n",
      "|             turning|    5|\n",
      "|                 300|    5|\n",
      "|               quite|    5|\n",
      "|           prototype|    5|\n",
      "|            focusing|    5|\n",
      "|            prepared|    5|\n",
      "|         salespeople|    5|\n",
      "|               raise|    5|\n",
      "|              second|    5|\n",
      "|             publish|    5|\n",
      "|                item|    5|\n",
      "|              backup|    5|\n",
      "|                fees|    5|\n",
      "|              sounds|    5|\n",
      "|             english|    5|\n",
      "|         interesting|    5|\n",
      "|               cases|    5|\n",
      "|               image|    5|\n",
      "|                post|    5|\n",
      "|              travel|    5|\n",
      "|                lack|    5|\n",
      "|          regardless|    5|\n",
      "|              wouldn|    5|\n",
      "|                 non|    5|\n",
      "|           competing|    5|\n",
      "|                wasn|    5|\n",
      "|             respond|    5|\n",
      "|              buying|    5|\n",
      "|          attainable|    5|\n",
      "|             ideally|    5|\n",
      "|              letter|    5|\n",
      "|          agreements|    5|\n",
      "|             locally|    5|\n",
      "|                fast|    5|\n",
      "|              honest|    5|\n",
      "|           somewhere|    5|\n",
      "|                vary|    5|\n",
      "|                   6|    5|\n",
      "|          creatively|    5|\n",
      "|              global|    5|\n",
      "|               areas|    5|\n",
      "|             actions|    5|\n",
      "|             florida|    5|\n",
      "|             devices|    5|\n",
      "|          placements|    5|\n",
      "|           discussed|    5|\n",
      "|              meetup|    5|\n",
      "|              oculus|    5|\n",
      "|              county|    5|\n",
      "|                  20|    5|\n",
      "|              coffee|    5|\n",
      "|         significant|    5|\n",
      "|               spare|    5|\n",
      "|               sleep|    5|\n",
      "|               cater|    5|\n",
      "|               tools|    5|\n",
      "|             written|    5|\n",
      "|          optimizing|    5|\n",
      "|              obtain|    5|\n",
      "|          importance|    5|\n",
      "|                five|    5|\n",
      "|               grows|    5|\n",
      "|             dealing|    5|\n",
      "|         exclusively|    5|\n",
      "|           developer|    5|\n",
      "|              trolls|    5|\n",
      "|              forums|    5|\n",
      "|            honestly|    5|\n",
      "|           placement|    5|\n",
      "|      indoctrination|    5|\n",
      "|            launched|    5|\n",
      "|            endeavor|    5|\n",
      "|              clouds|    5|\n",
      "|               lists|    5|\n",
      "|             program|    5|\n",
      "|               human|    5|\n",
      "|               group|    5|\n",
      "|              saying|    5|\n",
      "|               model|    5|\n",
      "|               risky|    5|\n",
      "|              relies|    5|\n",
      "|            hundreds|    5|\n",
      "|             focused|    5|\n",
      "|               lucky|    5|\n",
      "|              bigger|    5|\n",
      "|             project|    5|\n",
      "|             profits|    5|\n",
      "|           otherwise|    5|\n",
      "|              bottom|    5|\n",
      "|              allows|    5|\n",
      "|         publication|    5|\n",
      "|               knows|    5|\n",
      "|              remind|    5|\n",
      "|            avoiding|    5|\n",
      "|               seems|    5|\n",
      "|             compete|    5|\n",
      "|                 ten|    5|\n",
      "|         competition|    5|\n",
      "|                word|    5|\n",
      "|                fill|    5|\n",
      "|             details|    5|\n",
      "|            schedule|    5|\n",
      "|          storefront|    5|\n",
      "|            tracking|    5|\n",
      "|               gives|    5|\n",
      "|          misleading|    5|\n",
      "|         impressions|    5|\n",
      "|                 yes|    5|\n",
      "|            features|    5|\n",
      "|             telling|    5|\n",
      "|       communication|    5|\n",
      "|               owned|    5|\n",
      "|           paragraph|    5|\n",
      "|                seek|    5|\n",
      "|                went|    5|\n",
      "|             meaning|    5|\n",
      "|               worse|    5|\n",
      "|            minimize|    5|\n",
      "|                   2|    5|\n",
      "|          relatively|    5|\n",
      "|         recommended|    5|\n",
      "|             various|    5|\n",
      "|            headline|    5|\n",
      "|             hundred|    5|\n",
      "|                mail|    5|\n",
      "|               lunch|    5|\n",
      "|                debt|    5|\n",
      "|               above|    5|\n",
      "|        expectations|    5|\n",
      "|              remain|    5|\n",
      "|             visited|    5|\n",
      "|         appropriate|    5|\n",
      "|                task|    5|\n",
      "|              united|    5|\n",
      "|             sustain|    5|\n",
      "|              points|    5|\n",
      "|            whenever|    5|\n",
      "|            articles|    5|\n",
      "|           decisions|    5|\n",
      "|          simulators|    5|\n",
      "|             capital|    5|\n",
      "|              starts|    5|\n",
      "|                lets|    5|\n",
      "|           outsource|    5|\n",
      "|             minimal|    5|\n",
      "|             earning|    5|\n",
      "|                ends|    5|\n",
      "|              review|    5|\n",
      "|               fixed|    5|\n",
      "|           including|    5|\n",
      "|                sign|    5|\n",
      "|               topic|    5|\n",
      "|            discover|    5|\n",
      "|               total|    5|\n",
      "|                 his|    5|\n",
      "|          worthwhile|    5|\n",
      "|              factor|    5|\n",
      "|           incubator|    5|\n",
      "|               whole|    5|\n",
      "|                hope|    5|\n",
      "|               ahead|    6|\n",
      "|             markets|    6|\n",
      "|          associated|    6|\n",
      "|               sorts|    6|\n",
      "|               solve|    6|\n",
      "|           resulting|    6|\n",
      "|          reasonable|    6|\n",
      "|             somehow|    6|\n",
      "|               stick|    6|\n",
      "|       effectiveness|    6|\n",
      "|                knew|    6|\n",
      "|                 due|    6|\n",
      "|                 url|    6|\n",
      "|                base|    6|\n",
      "|          considered|    6|\n",
      "|               saved|    6|\n",
      "|                main|    6|\n",
      "|                mine|    6|\n",
      "|                 met|    6|\n",
      "|               basic|    6|\n",
      "|               items|    6|\n",
      "|                sale|    6|\n",
      "|               works|    6|\n",
      "|                goes|    6|\n",
      "|                 key|    6|\n",
      "|           structure|    6|\n",
      "|                rely|    6|\n",
      "|              depend|    6|\n",
      "|           liability|    6|\n",
      "|              choice|    6|\n",
      "|               allow|    6|\n",
      "|           advertise|    6|\n",
      "|           determine|    6|\n",
      "|              handle|    6|\n",
      "|            messages|    6|\n",
      "|             sending|    6|\n",
      "|         remarketing|    6|\n",
      "|          deductible|    6|\n",
      "|           unlimited|    6|\n",
      "|              choose|    6|\n",
      "|             finally|    6|\n",
      "|             hosting|    6|\n",
      "|               booth|    6|\n",
      "|           initially|    6|\n",
      "|            automate|    6|\n",
      "|             benefit|    6|\n",
      "|             concept|    6|\n",
      "|         comfortable|    6|\n",
      "|              rarely|    6|\n",
      "|                line|    6|\n",
      "|          techniques|    6|\n",
      "|         experienced|    6|\n",
      "|                sole|    6|\n",
      "|            globally|    6|\n",
      "|             partner|    6|\n",
      "|                lost|    6|\n",
      "|             silicon|    6|\n",
      "|                kind|    6|\n",
      "|             careful|    6|\n",
      "|                told|    6|\n",
      "|               story|    6|\n",
      "|       entrepreneurs|    6|\n",
      "|           commuting|    6|\n",
      "|             package|    6|\n",
      "|             control|    6|\n",
      "|                past|    6|\n",
      "|              manner|    6|\n",
      "|              became|    6|\n",
      "|                rest|    6|\n",
      "|                mean|    6|\n",
      "|             country|    6|\n",
      "|             million|    6|\n",
      "|            separate|    6|\n",
      "|           providing|    6|\n",
      "|              vision|    6|\n",
      "|                fail|    6|\n",
      "|              forced|    6|\n",
      "|               exist|    6|\n",
      "|               users|    6|\n",
      "|               board|    6|\n",
      "|             seeking|    6|\n",
      "|            comments|    6|\n",
      "|                rent|    6|\n",
      "|             largely|    6|\n",
      "|              shares|    6|\n",
      "|              assume|    6|\n",
      "|             collect|    6|\n",
      "|               stake|    6|\n",
      "|        distribution|    6|\n",
      "|               scary|    6|\n",
      "|          productive|    6|\n",
      "|             founder|    6|\n",
      "|            properly|    6|\n",
      "|                  tv|    6|\n",
      "|               forms|    6|\n",
      "|             keeping|    6|\n",
      "|           certainly|    6|\n",
      "|                 etc|    6|\n",
      "|         connections|    6|\n",
      "|              flight|    6|\n",
      "|              talked|    6|\n",
      "|            contract|    6|\n",
      "|               sound|    6|\n",
      "|                 llc|    6|\n",
      "|            designer|    6|\n",
      "|          especially|    6|\n",
      "|              reward|    6|\n",
      "|                fire|    6|\n",
      "|        dependencies|    6|\n",
      "|           qualities|    6|\n",
      "|                wall|    6|\n",
      "|             earlier|    7|\n",
      "|              safety|    7|\n",
      "|             certain|    7|\n",
      "|                   8|    7|\n",
      "|                  ok|    7|\n",
      "|                stay|    7|\n",
      "|           difficult|    7|\n",
      "|            describe|    7|\n",
      "|          additional|    7|\n",
      "|            location|    7|\n",
      "|                lots|    7|\n",
      "|               upper|    7|\n",
      "|               wants|    7|\n",
      "|                turn|    7|\n",
      "|           carefully|    7|\n",
      "|           wordpress|    7|\n",
      "|               taken|    7|\n",
      "|            continue|    7|\n",
      "|               heard|    7|\n",
      "|            dedicate|    7|\n",
      "|             mention|    7|\n",
      "|          protection|    7|\n",
      "|              energy|    7|\n",
      "|                zero|    7|\n",
      "|            meetings|    7|\n",
      "|          completely|    7|\n",
      "|               games|    7|\n",
      "|               track|    7|\n",
      "|                view|    7|\n",
      "|                save|    7|\n",
      "|               video|    7|\n",
      "|              single|    7|\n",
      "|            presence|    7|\n",
      "|             crucial|    7|\n",
      "|           thousands|    7|\n",
      "|                 bet|    7|\n",
      "|         maintaining|    7|\n",
      "|             mistake|    7|\n",
      "|              groups|    7|\n",
      "|                fund|    7|\n",
      "|             orlando|    7|\n",
      "|          freelancer|    7|\n",
      "|              spouse|    7|\n",
      "|             workers|    7|\n",
      "|             general|    7|\n",
      "|             leaving|    7|\n",
      "|         salesperson|    7|\n",
      "|            strategy|    7|\n",
      "|            everyone|    7|\n",
      "|               leads|    7|\n",
      "|               books|    7|\n",
      "|                kids|    7|\n",
      "|             further|    7|\n",
      "|              period|    7|\n",
      "|               trade|    7|\n",
      "|             defense|    7|\n",
      "|                upon|    7|\n",
      "|          standpoint|    7|\n",
      "|           ownership|    7|\n",
      "|                flow|    7|\n",
      "|             engines|    7|\n",
      "|               bunch|    7|\n",
      "|                open|    7|\n",
      "|            requires|    7|\n",
      "|         marketplace|    7|\n",
      "|              valley|    7|\n",
      "|         description|    7|\n",
      "|             setting|    7|\n",
      "|              reason|    7|\n",
      "|              images|    7|\n",
      "|                 sdk|    7|\n",
      "|            designed|    7|\n",
      "|              appear|    7|\n",
      "|              follow|    7|\n",
      "|           interests|    7|\n",
      "|           someplace|    7|\n",
      "|              dental|    7|\n",
      "|           addresses|    7|\n",
      "|                half|    7|\n",
      "|           generally|    7|\n",
      "|       understanding|    8|\n",
      "|             venture|    8|\n",
      "|               under|    8|\n",
      "|                form|    8|\n",
      "|             develop|    8|\n",
      "|                city|    8|\n",
      "|                tend|    8|\n",
      "|                2014|    8|\n",
      "|            evaluate|    8|\n",
      "|               visit|    8|\n",
      "|                 lpo|    8|\n",
      "|               three|    8|\n",
      "|              credit|    8|\n",
      "|                rich|    8|\n",
      "|                lean|    8|\n",
      "|                 top|    8|\n",
      "|                soon|    8|\n",
      "|                 old|    8|\n",
      "|              triton|    8|\n",
      "|              beyond|    8|\n",
      "|                took|    8|\n",
      "|             several|    8|\n",
      "|              common|    8|\n",
      "|               stage|    8|\n",
      "|           agreement|    8|\n",
      "|              easily|    8|\n",
      "|              anyone|    8|\n",
      "|                 add|    8|\n",
      "|            checking|    8|\n",
      "|            feedback|    8|\n",
      "|              useful|    8|\n",
      "|                user|    8|\n",
      "|               taxes|    8|\n",
      "|             editors|    8|\n",
      "|               write|    8|\n",
      "|             mission|    8|\n",
      "|            computer|    8|\n",
      "|               tasks|    8|\n",
      "|               worry|    8|\n",
      "|             message|    8|\n",
      "|       opportunities|    8|\n",
      "|        successfully|    8|\n",
      "|                 www|    8|\n",
      "|            identify|    8|\n",
      "|                2012|    8|\n",
      "|               leave|    8|\n",
      "|            security|    8|\n",
      "|                left|    8|\n",
      "|              notice|    8|\n",
      "|                gets|    8|\n",
      "|              higher|    8|\n",
      "|             matters|    8|\n",
      "|               haven|    8|\n",
      "|                bank|    8|\n",
      "|            reserves|    8|\n",
      "|                 car|    8|\n",
      "|             college|    9|\n",
      "|              demand|    9|\n",
      "|             reading|    9|\n",
      "|                hear|    9|\n",
      "|              stream|    9|\n",
      "|                 bit|    9|\n",
      "|           measuring|    9|\n",
      "|             achieve|    9|\n",
      "|            involved|    9|\n",
      "|              viable|    9|\n",
      "|              accept|    9|\n",
      "|             created|    9|\n",
      "|              across|    9|\n",
      "|                meet|    9|\n",
      "|             quality|    9|\n",
      "|              affect|    9|\n",
      "|              option|    9|\n",
      "|               phone|    9|\n",
      "|                deal|    9|\n",
      "|              mobile|    9|\n",
      "|                rate|    9|\n",
      "|              answer|    9|\n",
      "|               times|    9|\n",
      "|              simply|    9|\n",
      "|             clients|    9|\n",
      "|             savings|    9|\n",
      "|                   4|    9|\n",
      "|               share|    9|\n",
      "|            maximize|    9|\n",
      "|               stock|    9|\n",
      "|               goods|    9|\n",
      "|              amazon|    9|\n",
      "|               apply|    9|\n",
      "|              system|    9|\n",
      "|             minimum|    9|\n",
      "|             manager|    9|\n",
      "|                http|    9|\n",
      "|           following|    9|\n",
      "|          purchasing|    9|\n",
      "|                said|    9|\n",
      "|              cannot|    9|\n",
      "|             happens|    9|\n",
      "|               house|    9|\n",
      "|            creative|    9|\n",
      "|               alone|    9|\n",
      "|                tech|    9|\n",
      "|          ultimately|    9|\n",
      "|               fraud|    9|\n",
      "|                loan|    9|\n",
      "|             twitter|    9|\n",
      "|                boss|    9|\n",
      "|                 ceo|    9|\n",
      "|        relationship|    9|\n",
      "|              easier|    9|\n",
      "|           sometimes|    9|\n",
      "|            position|    9|\n",
      "|            increase|    9|\n",
      "|                text|    9|\n",
      "|                  he|    9|\n",
      "|              called|    9|\n",
      "|          government|    9|\n",
      "|           investors|   10|\n",
      "|             popular|   10|\n",
      "|           expensive|   10|\n",
      "|               along|   10|\n",
      "|             talking|   10|\n",
      "|              banner|   10|\n",
      "|               level|   10|\n",
      "|           according|   10|\n",
      "|               names|   10|\n",
      "|         traditional|   10|\n",
      "|                jobs|   10|\n",
      "|              unique|   10|\n",
      "|             process|   10|\n",
      "|             monthly|   10|\n",
      "|             friends|   10|\n",
      "|                 100|   10|\n",
      "|               takes|   10|\n",
      "|           employees|   10|\n",
      "|                kane|   10|\n",
      "|             receive|   10|\n",
      "|            exchange|   10|\n",
      "|                blog|   10|\n",
      "|          percentage|   10|\n",
      "|            thinking|   10|\n",
      "|               since|   10|\n",
      "|                lose|   10|\n",
      "|                  pr|   10|\n",
      "|           promotion|   10|\n",
      "|              beware|   10|\n",
      "|            maintain|   10|\n",
      "|           depending|   10|\n",
      "|               ended|   10|\n",
      "|              course|   10|\n",
      "|           necessary|   10|\n",
      "|               plans|   10|\n",
      "|            quitting|   10|\n",
      "|               known|   10|\n",
      "|             deliver|   10|\n",
      "|            question|   10|\n",
      "|            employee|   10|\n",
      "|         performance|   10|\n",
      "|            although|   10|\n",
      "|                last|   10|\n",
      "|          individual|   10|\n",
      "|             related|   10|\n",
      "|              anyhow|   10|\n",
      "|            starting|   10|\n",
      "|            purposes|   10|\n",
      "|               given|   10|\n",
      "|              domain|   10|\n",
      "|                odds|   10|\n",
      "|             expense|   10|\n",
      "|            training|   10|\n",
      "|               short|   10|\n",
      "|             mailing|   10|\n",
      "|                ever|   10|\n",
      "|                stop|   10|\n",
      "|           hopefully|   10|\n",
      "|           situation|   11|\n",
      "|             include|   11|\n",
      "|             startup|   11|\n",
      "|             average|   11|\n",
      "|                high|   11|\n",
      "|           countries|   11|\n",
      "|            visitors|   11|\n",
      "|                 cpa|   11|\n",
      "|              expect|   11|\n",
      "|               goals|   11|\n",
      "|                call|   11|\n",
      "|                came|   11|\n",
      "|           basically|   11|\n",
      "|            research|   11|\n",
      "|               today|   11|\n",
      "|          contractor|   11|\n",
      "|          accounting|   11|\n",
      "|             willing|   11|\n",
      "|           technical|   11|\n",
      "|           searching|   11|\n",
      "|             service|   11|\n",
      "|             running|   11|\n",
      "|           recommend|   11|\n",
      "|               ocean|   11|\n",
      "|            paycheck|   11|\n",
      "|              highly|   11|\n",
      "|          technology|   11|\n",
      "|             learned|   11|\n",
      "|               close|   11|\n",
      "|              during|   11|\n",
      "|               bills|   11|\n",
      "|               front|   11|\n",
      "|            linkedin|   11|\n",
      "|               frank|   11|\n",
      "|             rewards|   11|\n",
      "|                   9|   11|\n",
      "|              patent|   11|\n",
      "|              larger|   11|\n",
      "|               bound|   11|\n",
      "|          interested|   11|\n",
      "|             reality|   11|\n",
      "|                test|   11|\n",
      "|            directly|   11|\n",
      "|           launching|   11|\n",
      "|                 act|   11|\n",
      "|              invest|   11|\n",
      "|             between|   11|\n",
      "|             section|   11|\n",
      "|             digital|   11|\n",
      "|          statistics|   11|\n",
      "|    responsibilities|   11|\n",
      "|               early|   11|\n",
      "|                seem|   11|\n",
      "|                size|   11|\n",
      "|        publications|   11|\n",
      "|               scale|   12|\n",
      "|         freelancers|   12|\n",
      "|          everything|   12|\n",
      "|             growing|   12|\n",
      "|         opportunity|   12|\n",
      "|               links|   12|\n",
      "|           automated|   12|\n",
      "|               comes|   12|\n",
      "|             network|   12|\n",
      "|           advantage|   12|\n",
      "|              stress|   12|\n",
      "|                link|   12|\n",
      "|              target|   12|\n",
      "|               cover|   12|\n",
      "|              budget|   12|\n",
      "|          experience|   12|\n",
      "|                  am|   12|\n",
      "|              needed|   12|\n",
      "|           community|   12|\n",
      "|            valuable|   12|\n",
      "|           realistic|   12|\n",
      "|               makes|   12|\n",
      "|             against|   12|\n",
      "|           equipment|   12|\n",
      "|              emails|   12|\n",
      "|            channels|   12|\n",
      "|              almost|   12|\n",
      "|           resources|   12|\n",
      "|              basics|   12|\n",
      "|                move|   12|\n",
      "|            licenses|   12|\n",
      "|              chance|   12|\n",
      "|            decision|   12|\n",
      "|             limited|   12|\n",
      "|               sense|   12|\n",
      "|            coverage|   13|\n",
      "|                   5|   13|\n",
      "|              myself|   13|\n",
      "|             support|   13|\n",
      "|              direct|   13|\n",
      "|                days|   13|\n",
      "|               avoid|   13|\n",
      "|                   1|   13|\n",
      "|              toward|   13|\n",
      "|          personally|   13|\n",
      "|            purchase|   13|\n",
      "|                tied|   13|\n",
      "|                 net|   13|\n",
      "|                send|   13|\n",
      "|                 yet|   13|\n",
      "|              unless|   13|\n",
      "|                   3|   13|\n",
      "|              itself|   13|\n",
      "|              ensure|   13|\n",
      "|            interest|   13|\n",
      "|                 bad|   13|\n",
      "|             looking|   13|\n",
      "|           questions|   13|\n",
      "|               price|   13|\n",
      "|               lower|   13|\n",
      "|                game|   13|\n",
      "|              engine|   13|\n",
      "|             ongoing|   13|\n",
      "|                 put|   13|\n",
      "|              longer|   13|\n",
      "|               enjoy|   13|\n",
      "|                made|   13|\n",
      "|              matter|   13|\n",
      "|                huge|   13|\n",
      "|              owners|   13|\n",
      "|              happen|   13|\n",
      "|                tell|   14|\n",
      "|                read|   14|\n",
      "|             commute|   14|\n",
      "|          management|   14|\n",
      "|             outside|   14|\n",
      "|                both|   14|\n",
      "|              either|   14|\n",
      "|           effective|   14|\n",
      "|                face|   14|\n",
      "|                fact|   14|\n",
      "|            estimate|   14|\n",
      "|             realize|   14|\n",
      "|              figure|   14|\n",
      "|               pages|   14|\n",
      "|             require|   14|\n",
      "|              trends|   14|\n",
      "|              become|   14|\n",
      "|                news|   14|\n",
      "|               stuff|   14|\n",
      "|            targeted|   14|\n",
      "|             address|   14|\n",
      "|       automatically|   14|\n",
      "|            required|   14|\n",
      "|               yours|   15|\n",
      "|            problems|   15|\n",
      "|               reach|   15|\n",
      "|                 far|   15|\n",
      "|              future|   15|\n",
      "|             quickly|   15|\n",
      "|              entire|   15|\n",
      "|                 seo|   15|\n",
      "|               check|   15|\n",
      "|                 low|   15|\n",
      "|             writing|   15|\n",
      "|              return|   15|\n",
      "|             ability|   15|\n",
      "|        optimization|   15|\n",
      "|             nothing|   15|\n",
      "|           freelance|   15|\n",
      "|                 buy|   15|\n",
      "|               happy|   15|\n",
      "|            keywords|   15|\n",
      "|                didn|   15|\n",
      "|            entirely|   15|\n",
      "|                area|   16|\n",
      "|            releases|   16|\n",
      "|                earn|   16|\n",
      "|             contact|   16|\n",
      "|                next|   16|\n",
      "|           analytics|   16|\n",
      "|              giving|   16|\n",
      "|           investing|   16|\n",
      "|               until|   16|\n",
      "|            creating|   16|\n",
      "|            offering|   16|\n",
      "|              worked|   16|\n",
      "|            networks|   16|\n",
      "|               state|   16|\n",
      "|            planning|   16|\n",
      "|             believe|   16|\n",
      "|              offers|   16|\n",
      "|                 did|   16|\n",
      "|        professional|   16|\n",
      "|          themselves|   16|\n",
      "|                  50|   16|\n",
      "|            generate|   16|\n",
      "|             content|   17|\n",
      "|               wrong|   17|\n",
      "|             efforts|   17|\n",
      "|             finding|   17|\n",
      "|                mind|   17|\n",
      "|              lawyer|   17|\n",
      "|           financial|   17|\n",
      "|              coming|   17|\n",
      "|              skills|   17|\n",
      "|               owner|   17|\n",
      "|            existing|   17|\n",
      "|           corporate|   17|\n",
      "|                 two|   17|\n",
      "|            spending|   17|\n",
      "|              amount|   17|\n",
      "|              profit|   17|\n",
      "|              hiring|   17|\n",
      "|               media|   18|\n",
      "|          developers|   18|\n",
      "|              change|   18|\n",
      "|               month|   18|\n",
      "|               value|   18|\n",
      "|                risk|   18|\n",
      "|             display|   18|\n",
      "|               sites|   18|\n",
      "|              pretty|   18|\n",
      "|                each|   18|\n",
      "|                used|   18|\n",
      "|             started|   18|\n",
      "|                   d|   18|\n",
      "|              within|   18|\n",
      "|              taking|   19|\n",
      "|            industry|   19|\n",
      "|          investment|   19|\n",
      "|              always|   19|\n",
      "|             landing|   19|\n",
      "|               again|   19|\n",
      "|                show|   19|\n",
      "|              person|   19|\n",
      "|            physical|   19|\n",
      "|                goal|   19|\n",
      "|               doesn|   19|\n",
      "|         development|   19|\n",
      "|                 com|   19|\n",
      "|               great|   19|\n",
      "|                cash|   19|\n",
      "|          accountant|   19|\n",
      "|             current|   19|\n",
      "|                hour|   19|\n",
      "|              nobody|   19|\n",
      "|                path|   19|\n",
      "|               based|   19|\n",
      "|              decide|   19|\n",
      "|                away|   19|\n",
      "|               maybe|   19|\n",
      "|                ways|   19|\n",
      "|             dollars|   20|\n",
      "|           attention|   20|\n",
      "|             usually|   20|\n",
      "|            benefits|   20|\n",
      "|                ones|   20|\n",
      "|                 ask|   20|\n",
      "|             example|   20|\n",
      "|                aren|   20|\n",
      "|                hire|   20|\n",
      "|                term|   20|\n",
      "|            services|   20|\n",
      "|         contractors|   20|\n",
      "|               terms|   20|\n",
      "|               niche|   20|\n",
      "|              around|   20|\n",
      "|            facebook|   20|\n",
      "|                sort|   20|\n",
      "|                grow|   21|\n",
      "|                quit|   21|\n",
      "|             whether|   21|\n",
      "|               click|   21|\n",
      "|           different|   21|\n",
      "|                down|   21|\n",
      "|               costs|   21|\n",
      "|          developing|   21|\n",
      "|            campaign|   21|\n",
      "|               spent|   21|\n",
      "|           available|   21|\n",
      "|               place|   21|\n",
      "|              action|   21|\n",
      "|                list|   22|\n",
      "|               focus|   22|\n",
      "|               learn|   22|\n",
      "|                able|   22|\n",
      "|               extra|   22|\n",
      "|               found|   22|\n",
      "|             problem|   22|\n",
      "|                name|   22|\n",
      "|              paying|   22|\n",
      "|               later|   22|\n",
      "|             metrics|   23|\n",
      "|            anything|   23|\n",
      "|                 got|   23|\n",
      "|                feel|   23|\n",
      "|            remember|   23|\n",
      "|                  us|   23|\n",
      "|                paid|   23|\n",
      "|              months|   23|\n",
      "|                 off|   23|\n",
      "|             already|   23|\n",
      "|                 say|   23|\n",
      "|                  10|   23|\n",
      "|           campaigns|   23|\n",
      "|             success|   24|\n",
      "|              sundog|   24|\n",
      "|              result|   24|\n",
      "|               needs|   24|\n",
      "|             provide|   24|\n",
      "|                data|   24|\n",
      "|               build|   24|\n",
      "|              income|   24|\n",
      "|               world|   24|\n",
      "|               right|   24|\n",
      "|                care|   24|\n",
      "|              launch|   24|\n",
      "|           insurance|   24|\n",
      "|                 000|   24|\n",
      "|                less|   25|\n",
      "|              trying|   25|\n",
      "|              making|   25|\n",
      "|             results|   25|\n",
      "|               using|   25|\n",
      "|                 why|   25|\n",
      "|                been|   25|\n",
      "|                live|   25|\n",
      "|         information|   25|\n",
      "|                week|   25|\n",
      "|              effort|   26|\n",
      "|             perhaps|   26|\n",
      "|             another|   26|\n",
      "|            whatever|   26|\n",
      "|               often|   26|\n",
      "|              likely|   26|\n",
      "|                case|   26|\n",
      "|            consider|   26|\n",
      "|                 set|   26|\n",
      "|            internet|   26|\n",
      "|              family|   26|\n",
      "|             initial|   27|\n",
      "|               never|   27|\n",
      "|               ideas|   27|\n",
      "|                 run|   27|\n",
      "|                 isn|   27|\n",
      "|            relevant|   27|\n",
      "|             measure|   27|\n",
      "|                home|   27|\n",
      "|             similar|   27|\n",
      "|          successful|   27|\n",
      "|              living|   27|\n",
      "|                full|   27|\n",
      "|                look|   28|\n",
      "|                 too|   28|\n",
      "|                 ___|   28|\n",
      "|             because|   28|\n",
      "|                done|   28|\n",
      "|               thing|   29|\n",
      "|               offer|   29|\n",
      "|                here|   29|\n",
      "|                 had|   29|\n",
      "|                 tax|   29|\n",
      "|                same|   29|\n",
      "|                must|   29|\n",
      "|              little|   29|\n",
      "|              office|   29|\n",
      "|                side|   30|\n",
      "|               large|   30|\n",
      "|                does|   30|\n",
      "|             account|   30|\n",
      "|               means|   30|\n",
      "|            possible|   30|\n",
      "|              having|   30|\n",
      "|                   m|   30|\n",
      "|                come|   30|\n",
      "|             adwords|   30|\n",
      "|               least|   31|\n",
      "|              career|   31|\n",
      "|            websites|   31|\n",
      "|                give|   31|\n",
      "|           companies|   31|\n",
      "|                life|   31|\n",
      "|             getting|   31|\n",
      "|                sell|   31|\n",
      "|                real|   31|\n",
      "|            actually|   32|\n",
      "|                 let|   32|\n",
      "|               order|   32|\n",
      "|             release|   32|\n",
      "|                easy|   32|\n",
      "|                talk|   32|\n",
      "|               being|   32|\n",
      "|               every|   32|\n",
      "|                part|   33|\n",
      "|               after|   33|\n",
      "|             traffic|   33|\n",
      "|               email|   33|\n",
      "|            building|   33|\n",
      "|             selling|   33|\n",
      "|                else|   33|\n",
      "|                  me|   34|\n",
      "|                back|   34|\n",
      "|                 per|   34|\n",
      "|            specific|   35|\n",
      "|                 web|   35|\n",
      "|                free|   35|\n",
      "|          understand|   35|\n",
      "|              health|   35|\n",
      "|               hours|   35|\n",
      "|              social|   36|\n",
      "|               would|   36|\n",
      "|                 its|   36|\n",
      "|             however|   36|\n",
      "|             instead|   37|\n",
      "|              others|   37|\n",
      "|               point|   37|\n",
      "|               which|   37|\n",
      "|            expenses|   38|\n",
      "|           potential|   38|\n",
      "|              things|   38|\n",
      "|               years|   38|\n",
      "|            employed|   38|\n",
      "|                site|   38|\n",
      "|                were|   39|\n",
      "|                 won|   39|\n",
      "|                book|   39|\n",
      "|               local|   39|\n",
      "|               worth|   39|\n",
      "|              growth|   39|\n",
      "|                 few|   40|\n",
      "|             without|   40|\n",
      "|                 has|   40|\n",
      "|              number|   41|\n",
      "|         advertising|   41|\n",
      "|            customer|   41|\n",
      "|             freedom|   41|\n",
      "|                page|   42|\n",
      "|           marketing|   42|\n",
      "|                 big|   42|\n",
      "|              really|   42|\n",
      "|                keep|   42|\n",
      "|                such|   43|\n",
      "|               spend|   43|\n",
      "|               while|   43|\n",
      "|                  we|   43|\n",
      "|               press|   43|\n",
      "|              advice|   44|\n",
      "|                best|   44|\n",
      "|             revenue|   44|\n",
      "|               start|   44|\n",
      "|            employer|   44|\n",
      "|           lifestyle|   44|\n",
      "|                 pay|   45|\n",
      "|              create|   45|\n",
      "|                  go|   45|\n",
      "|               could|   46|\n",
      "|               small|   47|\n",
      "|                 try|   47|\n",
      "|                hard|   47|\n",
      "|          businesses|   47|\n",
      "|                long|   47|\n",
      "|                 now|   48|\n",
      "|                 see|   48|\n",
      "|            personal|   48|\n",
      "|              better|   49|\n",
      "|                 end|   49|\n",
      "|                year|   49|\n",
      "|                help|   50|\n",
      "|              online|   50|\n",
      "|               doing|   51|\n",
      "|               first|   51|\n",
      "|                know|   52|\n",
      "|                cost|   53|\n",
      "|               where|   53|\n",
      "|                 way|   55|\n",
      "|                 lot|   55|\n",
      "|               going|   55|\n",
      "|                take|   55|\n",
      "|              search|   56|\n",
      "|           something|   56|\n",
      "|             through|   57|\n",
      "|                very|   58|\n",
      "|               think|   58|\n",
      "|           important|   58|\n",
      "|             working|   58|\n",
      "|                 use|   58|\n",
      "|              enough|   59|\n",
      "|                once|   59|\n",
      "|                then|   59|\n",
      "|                idea|   60|\n",
      "|            software|   60|\n",
      "|                over|   62|\n",
      "|             someone|   62|\n",
      "|                 any|   62|\n",
      "|              google|   63|\n",
      "|                plan|   64|\n",
      "|                sure|   65|\n",
      "|               still|   65|\n",
      "|              market|   66|\n",
      "|                well|   66|\n",
      "|            products|   67|\n",
      "|               those|   68|\n",
      "|              should|   69|\n",
      "|                  ad|   70|\n",
      "|              before|   70|\n",
      "|                most|   70|\n",
      "|               might|   70|\n",
      "|                many|   71|\n",
      "|                good|   72|\n",
      "|                 day|   73|\n",
      "|                 ads|   75|\n",
      "|          employment|   75|\n",
      "|                  no|   76|\n",
      "|            probably|   76|\n",
      "|               other|   78|\n",
      "|                like|   78|\n",
      "|            yourself|   78|\n",
      "|                into|   79|\n",
      "|                only|   79|\n",
      "|               sales|   80|\n",
      "|                find|   81|\n",
      "|               these|   82|\n",
      "|                 was|   85|\n",
      "|               money|   86|\n",
      "|                 who|   88|\n",
      "|                 job|   90|\n",
      "|                much|   90|\n",
      "|                also|   91|\n",
      "|                than|   92|\n",
      "|                  ve|   95|\n",
      "|                 one|  100|\n",
      "|                when|  102|\n",
      "|                even|  104|\n",
      "|                 may|  107|\n",
      "|                make|  108|\n",
      "|             website|  109|\n",
      "|                self|  111|\n",
      "|                  ll|  114|\n",
      "|                some|  121|\n",
      "|             company|  122|\n",
      "|                want|  122|\n",
      "|                  by|  122|\n",
      "|               their|  122|\n",
      "|           customers|  123|\n",
      "|                 get|  123|\n",
      "|                 don|  133|\n",
      "|                 all|  137|\n",
      "|                 own|  140|\n",
      "|                just|  142|\n",
      "|                  so|  143|\n",
      "|                work|  144|\n",
      "|              people|  145|\n",
      "|                 new|  153|\n",
      "|                 out|  161|\n",
      "|               there|  162|\n",
      "|                 how|  163|\n",
      "|                them|  166|\n",
      "|                from|  166|\n",
      "|                need|  174|\n",
      "|                  up|  177|\n",
      "|                  an|  178|\n",
      "|             product|  182|\n",
      "|                more|  200|\n",
      "|               about|  202|\n",
      "|                 not|  203|\n",
      "|                  do|  207|\n",
      "|                  re|  214|\n",
      "|                  my|  215|\n",
      "|                  at|  220|\n",
      "|                what|  229|\n",
      "|                will|  231|\n",
      "|                they|  234|\n",
      "|                 but|  242|\n",
      "|                time|  255|\n",
      "|                  or|  278|\n",
      "|                this|  280|\n",
      "|                   t|  301|\n",
      "|                with|  315|\n",
      "|                have|  321|\n",
      "|                  as|  343|\n",
      "|                  be|  369|\n",
      "|                 can|  376|\n",
      "|            business|  383|\n",
      "|                   i|  387|\n",
      "|                   s|  391|\n",
      "|                  if|  411|\n",
      "|                 are|  424|\n",
      "|                  on|  428|\n",
      "|                 for|  537|\n",
      "|                  is|  560|\n",
      "|                  in|  616|\n",
      "|                  it|  649|\n",
      "|                that|  747|\n",
      "|                 and|  934|\n",
      "|                  of|  970|\n",
      "|                   a| 1191|\n",
      "|                 the| 1292|\n",
      "|                your| 1420|\n",
      "|                  to| 1828|\n",
      "|                 you| 1878|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "\n",
    "\n",
    "\n",
    "# Split using a regular expression that extracts words\n",
    "words = book.select(func.explode(func.split(book.value, \"\\\\W+\")).alias(\"word\"))\n",
    "wordsWithoutEmptyString = words.filter(words.word != \"\")\n",
    "\n",
    "# Normalize everything to lowercase\n",
    "lowercaseWords = wordsWithoutEmptyString.select(func.lower(wordsWithoutEmptyString.word).alias(\"word\"))\n",
    "\n",
    "# Count up the occurrences of each word\n",
    "wordCounts = lowercaseWords.groupBy(\"word\").count()\n",
    "\n",
    "# Sort by counts\n",
    "wordCountsSorted = wordCounts.sort(\"count\")\n",
    "\n",
    "# Show the results.\n",
    "wordCountsSorted.show(wordCountsSorted.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets try to do it using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting a session, loading the data, and inferring the schema\n",
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()\n",
    "book = spark.read.option('header', 'true').option('inferSchema', 'true')\\\n",
    ".text('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.createOrReplaceTempView('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Self-Employment: ...|\n",
      "|Achieving Financi...|\n",
      "|       By Frank Kane|\n",
      "|                    |\n",
      "|                    |\n",
      "|                    |\n",
      "|Copyright � 2015 ...|\n",
      "|All rights reserv...|\n",
      "|                    |\n",
      "|                    |\n",
      "|            CONTENTS|\n",
      "|          Disclaimer|\n",
      "|             Preface|\n",
      "|Part I: Making th...|\n",
      "|  Overcoming Inertia|\n",
      "|     Fear of Failure|\n",
      "|Career Indoctrina...|\n",
      "|The Carrot on a S...|\n",
      "|      Ego Protection|\n",
      "|Your Employer as ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select *\n",
    "from book\n",
    "'''\n",
    "\n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, each row is a string sentence.\n",
    "- Breaking it down to words and counting everything using sql will be tough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Temperature per station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stationID: string (nullable = true)\n",
      " |-- date: integer (nullable = true)\n",
      " |-- measure_type: string (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MinTemperatures\").getOrCreate()\n",
    "\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"stationID\", StringType(), True), \\\n",
    "                     StructField(\"date\", IntegerType(), True), \\\n",
    "                     StructField(\"measure_type\", StringType(), True), \\\n",
    "                     StructField(\"temperature\", FloatType(), True)])\n",
    "\n",
    "# // Read the file as dataframe\n",
    "df = spark.read.schema(schema).csv(\"1800.csv\")\n",
    "df.printSchema()                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then we can do the excersise using rdd commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|  stationID|min(temperature)|\n",
      "+-----------+----------------+\n",
      "|ITE00100554|          -148.0|\n",
      "|EZE00100082|          -135.0|\n",
      "+-----------+----------------+\n",
      "\n",
      "ITE00100554\t5.36F\n",
      "EZE00100082\t7.70F\n"
     ]
    }
   ],
   "source": [
    "# Filter out all but TMIN entries\n",
    "minTemps = df.filter(df.measure_type == \"TMIN\")\n",
    "\n",
    "# Select only stationID and temperature\n",
    "stationTemps = minTemps.select(\"stationID\", \"temperature\")\n",
    "\n",
    "# Aggregate to find minimum temperature for every station\n",
    "minTempsByStation = stationTemps.groupBy(\"stationID\").min(\"temperature\")\n",
    "minTempsByStation.show()\n",
    "\n",
    "# Convert temperature to fahrenheit and sort the dataset\n",
    "minTempsByStationF = minTempsByStation.withColumn(\"temperature\",\n",
    "                                                  func.round(func.col(\"min(temperature)\") * 0.1 * (9.0 / 5.0) + 32.0, 2))\\\n",
    "                                                  .select(\"stationID\", \"temperature\").sort(\"temperature\")\n",
    "                                                  \n",
    "# Collect, format, and print the results\n",
    "results = minTempsByStationF.collect()\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
    "    \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- or by using sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------+-----------+\n",
      "|  stationID|    date|measure_type|temperature|\n",
      "+-----------+--------+------------+-----------+\n",
      "|ITE00100554|18000101|        TMAX|      -75.0|\n",
      "|ITE00100554|18000101|        TMIN|     -148.0|\n",
      "|GM000010962|18000101|        PRCP|        0.0|\n",
      "|EZE00100082|18000101|        TMAX|      -86.0|\n",
      "|EZE00100082|18000101|        TMIN|     -135.0|\n",
      "|ITE00100554|18000102|        TMAX|      -60.0|\n",
      "|ITE00100554|18000102|        TMIN|     -125.0|\n",
      "|GM000010962|18000102|        PRCP|        0.0|\n",
      "|EZE00100082|18000102|        TMAX|      -44.0|\n",
      "|EZE00100082|18000102|        TMIN|     -130.0|\n",
      "|ITE00100554|18000103|        TMAX|      -23.0|\n",
      "|ITE00100554|18000103|        TMIN|      -46.0|\n",
      "|GM000010962|18000103|        PRCP|        4.0|\n",
      "|EZE00100082|18000103|        TMAX|      -10.0|\n",
      "|EZE00100082|18000103|        TMIN|      -73.0|\n",
      "|ITE00100554|18000104|        TMAX|        0.0|\n",
      "|ITE00100554|18000104|        TMIN|      -13.0|\n",
      "|GM000010962|18000104|        PRCP|        0.0|\n",
      "|EZE00100082|18000104|        TMAX|      -55.0|\n",
      "|EZE00100082|18000104|        TMIN|      -74.0|\n",
      "+-----------+--------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select *\n",
    "from df\n",
    "'''\n",
    "result = spark.sql(query)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|min_temp|  stationID|\n",
      "+--------+-----------+\n",
      "|  -148.0|ITE00100554|\n",
      "|     0.0|GM000010962|\n",
      "|  -135.0|EZE00100082|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select \n",
    "    min(temperature) as min_temp,\n",
    "    stationID\n",
    "from df\n",
    "group by stationID\n",
    "'''\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most popular Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- movieID: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "\n",
    "# Create a Spark Session\n",
    "spark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\n",
    "\n",
    "# Create schema when reading u.data\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "\n",
    "# Load up movie data as dataframe\n",
    "moviesDF = spark.read.option(\"sep\", \"\\t\").schema(schema).csv(\"u.data\")\n",
    "moviesDF.printSchema()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By Using RDD Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|movieID|count|\n",
      "+-------+-----+\n",
      "|     50|  583|\n",
      "|    258|  509|\n",
      "|    100|  508|\n",
      "|    181|  507|\n",
      "|    294|  485|\n",
      "|    286|  481|\n",
      "|    288|  478|\n",
      "|      1|  452|\n",
      "|    300|  431|\n",
      "|    121|  429|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some SQL-style magic to sort all movies by popularity in one line!\n",
    "topMovieIDs = moviesDF.groupBy(\"movieID\").count().orderBy(func.desc(\"count\"))\n",
    "\n",
    "# Grab the top 10\n",
    "topMovieIDs.show(10)\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets do the same in sql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF.createOrReplaceTempView('moviesDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|movieID|count|\n",
      "+-------+-----+\n",
      "|     50|  583|\n",
      "|    258|  509|\n",
      "|    100|  508|\n",
      "|    181|  507|\n",
      "|    294|  485|\n",
      "|    286|  481|\n",
      "|    288|  478|\n",
      "|      1|  452|\n",
      "|    300|  431|\n",
      "|    121|  429|\n",
      "|    174|  420|\n",
      "|    127|  413|\n",
      "|     56|  394|\n",
      "|      7|  392|\n",
      "|     98|  390|\n",
      "|    237|  384|\n",
      "|    117|  378|\n",
      "|    172|  367|\n",
      "|    222|  365|\n",
      "|    204|  350|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select \n",
    "    movieID,\n",
    "    count(*) as count\n",
    "from moviesDF\n",
    "group by movieID\n",
    "order by count(*) desc\n",
    "'''\n",
    "\n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most popular Movie + Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------------------+\n",
      "|movieID|count|movieTitle                   |\n",
      "+-------+-----+-----------------------------+\n",
      "|50     |583  |Star Wars (1977)             |\n",
      "|258    |509  |Contact (1997)               |\n",
      "|100    |508  |Fargo (1996)                 |\n",
      "|181    |507  |Return of the Jedi (1983)    |\n",
      "|294    |485  |Liar Liar (1997)             |\n",
      "|286    |481  |English Patient, The (1996)  |\n",
      "|288    |478  |Scream (1996)                |\n",
      "|1      |452  |Toy Story (1995)             |\n",
      "|300    |431  |Air Force One (1997)         |\n",
      "|121    |429  |Independence Day (ID4) (1996)|\n",
      "+-------+-----+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Sep  7 15:28:00 2020\n",
    "\n",
    "@author: Frank\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "import codecs\n",
    "\n",
    "# Creating a dictionary object\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    # CHANGE THIS TO THE PATH TO YOUR u.ITEM FILE:\n",
    "    with codecs.open(\"u.item\", \"r\", encoding='ISO-8859-1', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\n",
    "\n",
    "# Broadcasting the dictionary to the Spark Driver\n",
    "nameDict = spark.sparkContext.broadcast(loadMovieNames())\n",
    "\n",
    "# Create schema when reading u.data\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "\n",
    "\n",
    "# Load up movie data as dataframe\n",
    "moviesDF = spark.read.option(\"sep\", \"\\t\").schema(schema).csv(\"u.data\")\n",
    "movieCounts = moviesDF.groupBy(\"movieID\").count()\n",
    "\n",
    "\n",
    "# Create a user-defined function to look up movie names from our broadcasted dictionary\n",
    "def lookupName(movieID):\n",
    "    return nameDict.value[movieID]\n",
    "\n",
    "# Retrieve the movie names from the broadcasted dictionary\n",
    "lookupNameUDF = func.udf(lookupName)\n",
    "\n",
    "# Add a movieTitle column using our new udf\n",
    "moviesWithNames = movieCounts.withColumn(\"movieTitle\", lookupNameUDF(func.col(\"movieID\")))\n",
    "\n",
    "# Sort the results\n",
    "sortedMoviesWithNames = moviesWithNames.orderBy(func.desc(\"count\"))\n",
    "\n",
    "# Grab the top 10\n",
    "sortedMoviesWithNames.show(10, False)\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superhero Social Graph1 - Find the most popular superhero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTAIN AMERICA is the most popular superhero with 1933 co-appearances.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MostPopularSuperhero\").getOrCreate()\n",
    "\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"id\", IntegerType(), True), \\\n",
    "                     StructField(\"name\", StringType(), True)])\n",
    "\n",
    "names = spark.read.schema(schema).option(\"sep\", \" \").csv(\"marvel_names.txt\")\n",
    "\n",
    "lines = spark.read.text(\"marvel_graph.txt\")\n",
    "\n",
    "# Small tweak vs. what's shown in the video: we trim each line of whitespace as that could\n",
    "# throw off the counts.\n",
    "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
    "    .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
    "    .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
    "    \n",
    "mostPopular = connections.sort(func.col(\"connections\").desc()).first()\n",
    "\n",
    "mostPopularName = names.filter(func.col(\"id\") == mostPopular[0]).select(\"name\").first()\n",
    "\n",
    "print(mostPopularName[0] + \" is the most popular superhero with \" + str(mostPopular[1]) + \" co-appearances.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superhero Social Graph2 - Find the least popular superhero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following characters have only 0 connection(s):\n",
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|        BERSERKER II|\n",
      "|              BLARE/|\n",
      "|MARVEL BOY II/MARTIN|\n",
      "|MARVEL BOY/MARTIN BU|\n",
      "|      GIURESCU, RADU|\n",
      "|       CLUMSY FOULUP|\n",
      "|              FENRIS|\n",
      "|              RANDAK|\n",
      "|           SHARKSKIN|\n",
      "|     CALLAHAN, DANNY|\n",
      "|         DEATHCHARGE|\n",
      "|                RUNE|\n",
      "|         SEA LEOPARD|\n",
      "|         RED WOLF II|\n",
      "|              ZANTOR|\n",
      "|JOHNSON, LYNDON BAIN|\n",
      "|          LUNATIK II|\n",
      "|                KULL|\n",
      "|GERVASE, LADY ALYSSA|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MostObscureSuperheroes\").getOrCreate()\n",
    "\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"id\", IntegerType(), True), \\\n",
    "                     StructField(\"name\", StringType(), True)])\n",
    "\n",
    "names = spark.read.schema(schema).option(\"sep\", \" \").csv(\"marvel_names.txt\")\n",
    "\n",
    "lines = spark.read.text(\"marvel_graph.txt\")\n",
    "\n",
    "# Small tweak vs. what's shown in the video: we trim whitespace from each line as this\n",
    "# could throw the counts off by one.\n",
    "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
    "    .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
    "    .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
    "    \n",
    "minConnectionCount = connections.agg(func.min(\"connections\")).first()[0]\n",
    "\n",
    "minConnections = connections.filter(func.col(\"connections\") == minConnectionCount)\n",
    "\n",
    "minConnectionsWithNames = minConnections.join(names, \"id\")\n",
    "\n",
    "print(\"The following characters have only \" + str(minConnectionCount) + \" connection(s):\")\n",
    "\n",
    "minConnectionsWithNames.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superhero Social Graph3 - Find the degrees of connections + Accumelators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BFS iteration# 1\n",
      "Processing 8330 values.\n",
      "Running BFS iteration# 2\n",
      "Processing 220615 values.\n",
      "Hit the target character! From 1 different direction(s).\n"
     ]
    }
   ],
   "source": [
    "#Boilerplate stuff:\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"DegreesOfSeparation\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# The characters we wish to find the degree of separation between:\n",
    "startCharacterID = 5306 #SpiderMan\n",
    "targetCharacterID = 14  #ADAM 3,031 (who?)\n",
    "\n",
    "# Our accumulator, used to signal when we find the target character during\n",
    "# our BFS traversal.\n",
    "hitCounter = sc.accumulator(0)\n",
    "\n",
    "def convertToBFS(line):\n",
    "    fields = line.split()\n",
    "    heroID = int(fields[0])\n",
    "    connections = []\n",
    "    for connection in fields[1:]:\n",
    "        connections.append(int(connection))\n",
    "\n",
    "    color = 'WHITE'\n",
    "    distance = 9999\n",
    "\n",
    "    if (heroID == startCharacterID):\n",
    "        color = 'GRAY'\n",
    "        distance = 0\n",
    "\n",
    "    return (heroID, (connections, distance, color))\n",
    "\n",
    "\n",
    "def createStartingRdd():\n",
    "    inputFile = sc.textFile(\"marvel_graph.txt\")\n",
    "    return inputFile.map(convertToBFS)\n",
    "\n",
    "def bfsMap(node):\n",
    "    characterID = node[0]\n",
    "    data = node[1]\n",
    "    connections = data[0]\n",
    "    distance = data[1]\n",
    "    color = data[2]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    #If this node needs to be expanded...\n",
    "    if (color == 'GRAY'):\n",
    "        for connection in connections:\n",
    "            newCharacterID = connection\n",
    "            newDistance = distance + 1\n",
    "            newColor = 'GRAY'\n",
    "            if (targetCharacterID == connection):\n",
    "                hitCounter.add(1)\n",
    "\n",
    "            newEntry = (newCharacterID, ([], newDistance, newColor))\n",
    "            results.append(newEntry)\n",
    "\n",
    "        #We've processed this node, so color it black\n",
    "        color = 'BLACK'\n",
    "\n",
    "    #Emit the input node so we don't lose it.\n",
    "    results.append( (characterID, (connections, distance, color)) )\n",
    "    return results\n",
    "\n",
    "def bfsReduce(data1, data2):\n",
    "    edges1 = data1[0]\n",
    "    edges2 = data2[0]\n",
    "    distance1 = data1[1]\n",
    "    distance2 = data2[1]\n",
    "    color1 = data1[2]\n",
    "    color2 = data2[2]\n",
    "\n",
    "    distance = 9999\n",
    "    color = color1\n",
    "    edges = []\n",
    "\n",
    "    # See if one is the original node with its connections.\n",
    "    # If so preserve them.\n",
    "    if (len(edges1) > 0):\n",
    "        edges.extend(edges1)\n",
    "    if (len(edges2) > 0):\n",
    "        edges.extend(edges2)\n",
    "\n",
    "    # Preserve minimum distance\n",
    "    if (distance1 < distance):\n",
    "        distance = distance1\n",
    "\n",
    "    if (distance2 < distance):\n",
    "        distance = distance2\n",
    "\n",
    "    # Preserve darkest color\n",
    "    if (color1 == 'WHITE' and (color2 == 'GRAY' or color2 == 'BLACK')):\n",
    "        color = color2\n",
    "\n",
    "    if (color1 == 'GRAY' and color2 == 'BLACK'):\n",
    "        color = color2\n",
    "\n",
    "    if (color2 == 'WHITE' and (color1 == 'GRAY' or color1 == 'BLACK')):\n",
    "        color = color1\n",
    "\n",
    "    if (color2 == 'GRAY' and color1 == 'BLACK'):\n",
    "        color = color1\n",
    "\n",
    "    return (edges, distance, color)\n",
    "\n",
    "\n",
    "#Main program here:\n",
    "iterationRdd = createStartingRdd()\n",
    "\n",
    "for iteration in range(0, 10):\n",
    "    print(\"Running BFS iteration# \" + str(iteration+1))\n",
    "\n",
    "    # Create new vertices as needed to darken or reduce distances in the\n",
    "    # reduce stage. If we encounter the node we're looking for as a GRAY\n",
    "    # node, increment our accumulator to signal that we're done.\n",
    "    mapped = iterationRdd.flatMap(bfsMap)\n",
    "\n",
    "    # Note that mapped.count() action here forces the RDD to be evaluated, and\n",
    "    # that's the only reason our accumulator is actually updated.\n",
    "    print(\"Processing \" + str(mapped.count()) + \" values.\")\n",
    "\n",
    "    if (hitCounter.value > 0):\n",
    "        print(\"Hit the target character! From \" + str(hitCounter.value) \\\n",
    "            + \" different direction(s).\")\n",
    "        break\n",
    "\n",
    "    # Reducer combines data for each character ID, preserving the darkest\n",
    "    # color and shortest path.\n",
    "    iterationRdd = mapped.reduceByKey(bfsReduce)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Recommend similar movies based on ratings + Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 84>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Sort by quality score.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m results \u001b[38;5;241m=\u001b[39m filteredResults\u001b[38;5;241m.\u001b[39msort(func\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 similar movies for \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mgetMovieName\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovieNames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmovieID\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# Display the similarity result that isn't the movie we're looking at\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     similarMovieID \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mmovie1\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mgetMovieName\u001b[1;34m(movieNames, movieId)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetMovieName\u001b[39m(movieNames, movieId):\n\u001b[1;32m---> 33\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmovieNames\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovieID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmovieId\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovieTitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "import sys\n",
    "\n",
    "def computeCosineSimilarity(spark, data):\n",
    "    # Compute xx, xy and yy columns\n",
    "    pairScores = data \\\n",
    "      .withColumn(\"xx\", func.col(\"rating1\") * func.col(\"rating1\")) \\\n",
    "      .withColumn(\"yy\", func.col(\"rating2\") * func.col(\"rating2\")) \\\n",
    "      .withColumn(\"xy\", func.col(\"rating1\") * func.col(\"rating2\")) \n",
    "\n",
    "    # Compute numerator, denominator and numPairs columns\n",
    "    calculateSimilarity = pairScores \\\n",
    "      .groupBy(\"movie1\", \"movie2\") \\\n",
    "      .agg( \\\n",
    "        func.sum(func.col(\"xy\")).alias(\"numerator\"), \\\n",
    "        (func.sqrt(func.sum(func.col(\"xx\"))) * func.sqrt(func.sum(func.col(\"yy\")))).alias(\"denominator\"), \\\n",
    "        func.count(func.col(\"xy\")).alias(\"numPairs\")\n",
    "      )\n",
    "\n",
    "    # Calculate score and select only needed columns (movie1, movie2, score, numPairs)\n",
    "    result = calculateSimilarity \\\n",
    "      .withColumn(\"score\", \\\n",
    "        func.when(func.col(\"denominator\") != 0, func.col(\"numerator\") / func.col(\"denominator\")) \\\n",
    "          .otherwise(0) \\\n",
    "      ).select(\"movie1\", \"movie2\", \"score\", \"numPairs\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Get movie name by given movie id \n",
    "def getMovieName(movieNames, movieId):\n",
    "    result = movieNames.filter(func.col(\"movieID\") == movieId) \\\n",
    "        .select(\"movieTitle\").collect()[0]\n",
    "\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MovieSimilarities\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "movieNamesSchema = StructType([ \\\n",
    "                               StructField(\"movieID\", IntegerType(), True), \\\n",
    "                               StructField(\"movieTitle\", StringType(), True) \\\n",
    "                               ])\n",
    "    \n",
    "moviesSchema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "    \n",
    "    \n",
    "# Create a broadcast dataset of movieID and movieTitle.\n",
    "# Apply ISO-885901 charset\n",
    "movieNames = spark.read \\\n",
    "      .option(\"sep\", \"|\") \\\n",
    "      .option(\"charset\", \"ISO-8859-1\") \\\n",
    "      .schema(movieNamesSchema) \\\n",
    "      .csv(\"u.item\")\n",
    "\n",
    "# Load up movie data as dataset\n",
    "movies = spark.read \\\n",
    "      .option(\"sep\", \"\\t\") \\\n",
    "      .schema(moviesSchema) \\\n",
    "      .csv(\"u.data\")\n",
    "\n",
    "\n",
    "ratings = movies.select(\"userId\", \"movieId\", \"rating\")\n",
    "\n",
    "# Emit every movie rated together by the same user.\n",
    "# Self-join to find every combination.\n",
    "# Select movie pairs and rating pairs\n",
    "moviePairs = ratings.alias(\"ratings1\") \\\n",
    "      .join(ratings.alias(\"ratings2\"), (func.col(\"ratings1.userId\") == func.col(\"ratings2.userId\")) \\\n",
    "            & (func.col(\"ratings1.movieId\") < func.col(\"ratings2.movieId\"))) \\\n",
    "      .select(func.col(\"ratings1.movieId\").alias(\"movie1\"), \\\n",
    "        func.col(\"ratings2.movieId\").alias(\"movie2\"), \\\n",
    "        func.col(\"ratings1.rating\").alias(\"rating1\"), \\\n",
    "        func.col(\"ratings2.rating\").alias(\"rating2\"))\n",
    "\n",
    "\n",
    "moviePairSimilarities = computeCosineSimilarity(spark, moviePairs).cache()\n",
    "\n",
    "if (len(sys.argv) > 1):\n",
    "    scoreThreshold = 0.97\n",
    "    coOccurrenceThreshold = 50.0\n",
    "\n",
    "    movieID = sys.argv[1]\n",
    "\n",
    "    # Filter for movies with this sim that are \"good\" as defined by\n",
    "    # our quality thresholds above\n",
    "    filteredResults = moviePairSimilarities.filter( \\\n",
    "        ((func.col(\"movie1\") == movieID) | (func.col(\"movie2\") == movieID)) & \\\n",
    "          (func.col(\"score\") > scoreThreshold) & (func.col(\"numPairs\") > coOccurrenceThreshold))\n",
    "\n",
    "    # Sort by quality score.\n",
    "    results = filteredResults.sort(func.col(\"score\").desc()).take(10)\n",
    "    \n",
    "    print (\"Top 10 similar movies for \" + getMovieName(movieNames, movieID))\n",
    "    \n",
    "    for result in results:\n",
    "        # Display the similarity result that isn't the movie we're looking at\n",
    "        similarMovieID = result.movie1\n",
    "        if (similarMovieID == movieID):\n",
    "            similarMovieID = result.movie2\n",
    "        \n",
    "        print(getMovieName(movieNames, similarMovieID) + \"\\tscore: \" \\\n",
    "              + str(result.score) + \"\\tstrength: \" + str(result.numPairs))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend similar movies based on ratings + Partitioning and running on clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from math import sqrt\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"movies.dat\",  encoding='ascii', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            fields = line.split(\"::\")\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "def makePairs( userRatings ):\n",
    "    ratings = userRatings[1]\n",
    "    (movie1, rating1) = ratings[0]\n",
    "    (movie2, rating2) = ratings[1]\n",
    "    return ((movie1, movie2), (rating1, rating2))\n",
    "\n",
    "def filterDuplicates( userRatings ):\n",
    "    ratings = userRatings[1]\n",
    "    (movie1, rating1) = ratings[0]\n",
    "    (movie2, rating2) = ratings[1]\n",
    "    return movie1 < movie2\n",
    "\n",
    "def computeCosineSimilarity(ratingPairs):\n",
    "    numPairs = 0\n",
    "    sum_xx = sum_yy = sum_xy = 0\n",
    "    for ratingX, ratingY in ratingPairs:\n",
    "        sum_xx += ratingX * ratingX\n",
    "        sum_yy += ratingY * ratingY\n",
    "        sum_xy += ratingX * ratingY\n",
    "        numPairs += 1\n",
    "\n",
    "    numerator = sum_xy\n",
    "    denominator = sqrt(sum_xx) * sqrt(sum_yy)\n",
    "\n",
    "    score = 0\n",
    "    if (denominator):\n",
    "        score = (numerator / (float(denominator)))\n",
    "\n",
    "    return (score, numPairs)\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "print(\"\\nLoading movie names...\")\n",
    "nameDict = loadMovieNames()\n",
    "\n",
    "data = sc.textFile(\"s3n://sundog-spark/ml-1m/ratings.dat\")\n",
    "\n",
    "# Map ratings to key / value pairs: user ID => movie ID, rating\n",
    "ratings = data.map(lambda l: l.split(\"::\")).map(lambda l: (int(l[0]), (int(l[1]), float(l[2]))))\n",
    "\n",
    "# Emit every movie rated together by the same user.\n",
    "# Self-join to find every combination.\n",
    "ratingsPartitioned = ratings.partitionBy(100)\n",
    "joinedRatings = ratingsPartitioned.join(ratingsPartitioned)\n",
    "\n",
    "# At this point our RDD consists of userID => ((movieID, rating), (movieID, rating))\n",
    "\n",
    "# Filter out duplicate pairs\n",
    "uniqueJoinedRatings = joinedRatings.filter(filterDuplicates)\n",
    "\n",
    "# Now key by (movie1, movie2) pairs.\n",
    "moviePairs = uniqueJoinedRatings.map(makePairs).partitionBy(100)\n",
    "\n",
    "# We now have (movie1, movie2) => (rating1, rating2)\n",
    "# Now collect all ratings for each movie pair and compute similarity\n",
    "moviePairRatings = moviePairs.groupByKey()\n",
    "\n",
    "# We now have (movie1, movie2) = > (rating1, rating2), (rating1, rating2) ...\n",
    "# Can now compute similarities.\n",
    "moviePairSimilarities = moviePairRatings.mapValues(computeCosineSimilarity).persist()\n",
    "\n",
    "# Save the results if desired\n",
    "moviePairSimilarities.sortByKey()\n",
    "moviePairSimilarities.saveAsTextFile(\"movie-sims\")\n",
    "\n",
    "# Extract similarities for the movie we care about that are \"good\".\n",
    "if (len(sys.argv) > 1):\n",
    "\n",
    "    scoreThreshold = 0.97\n",
    "    coOccurenceThreshold = 50\n",
    "\n",
    "    movieID = int(sys.argv[1])\n",
    "\n",
    "    # Filter for movies with this sim that are \"good\" as defined by\n",
    "    # our quality thresholds above\n",
    "    filteredResults = moviePairSimilarities.filter(lambda pairSim: \\\n",
    "        (pairSim[0][0] == movieID or pairSim[0][1] == movieID) \\\n",
    "        and pairSim[1][0] > scoreThreshold and pairSim[1][1] > coOccurenceThreshold)\n",
    "\n",
    "    # Sort by quality score.\n",
    "    results = filteredResults.map(lambda pairSim: (pairSim[1], pairSim[0])).sortByKey(ascending = False).take(10)\n",
    "\n",
    "    print(\"Top 10 similar movies for \" + nameDict[movieID])\n",
    "    for result in results:\n",
    "        (sim, pair) = result\n",
    "        # Display the similarity result that isn't the movie we're looking at\n",
    "        similarMovieID = pair[0]\n",
    "        if (similarMovieID == movieID):\n",
    "            similarMovieID = pair[1]\n",
    "        print(nameDict[similarMovieID] + \"\\tscore: \" + str(sim[0]) + \"\\tstrength: \" + str(sim[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-3U7IV4I:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DegreesOfSeparation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=DegreesOfSeparation>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>__SparkML__<ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training recommendation model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 38>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m model \u001b[38;5;241m=\u001b[39m als\u001b[38;5;241m.\u001b[39mfit(ratings)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Manually construct a dataframe of the user ID's we want recs for\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m userID \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m userSchema \u001b[38;5;241m=\u001b[39m StructType([StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserID\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m)])\n\u001b[0;32m     40\u001b[0m users \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([[userID,]], userSchema)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '-f'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    # CHANGE THIS TO THE PATH TO YOUR u.ITEM FILE:\n",
    "    with codecs.open(\"u.ITEM\", \"r\", encoding='ISO-8859-1', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ALSExample\").getOrCreate()\n",
    "    \n",
    "moviesSchema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "    \n",
    "names = loadMovieNames()\n",
    "    \n",
    "ratings = spark.read.option(\"sep\", \"\\t\").schema(moviesSchema) \\\n",
    "    .csv(\"u.data\")\n",
    "    \n",
    "print(\"Training recommendation model...\")\n",
    "\n",
    "als = ALS().setMaxIter(5).setRegParam(0.01).setUserCol(\"userID\").setItemCol(\"movieID\") \\\n",
    "    .setRatingCol(\"rating\")\n",
    "    \n",
    "model = als.fit(ratings)\n",
    "\n",
    "# Manually construct a dataframe of the user ID's we want recs for\n",
    "userID = int(sys.argv[1])\n",
    "userSchema = StructType([StructField(\"userID\", IntegerType(), True)])\n",
    "users = spark.createDataFrame([[userID,]], userSchema)\n",
    "\n",
    "recommendations = model.recommendForUserSubset(users, 10).collect()\n",
    "\n",
    "print(\"Top 10 recommendations for user ID \" + str(userID))\n",
    "\n",
    "for userRecs in recommendations:\n",
    "    myRecs = userRecs[1]  #userRecs is (userID, [Row(movieId, rating), Row(movieID, rating)...])\n",
    "    for rec in myRecs: #my Recs is just the column of recs for the user\n",
    "        movie = rec[0] #For each rec in the list, extract the movie ID and rating\n",
    "        rating = rec[1]\n",
    "        movieName = names[movie]\n",
    "        print(movieName + str(rating))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2.4404683061101764, -3.54)\n",
      "(-2.3125552232173225, -3.23)\n",
      "(-2.0496227750486797, -2.89)\n",
      "(-1.8222217387947177, -2.58)\n",
      "(-1.6943086559018645, -2.54)\n",
      "(-1.6587772439871826, -2.43)\n",
      "(-1.7227337854336098, -2.36)\n",
      "(-1.5521830082431383, -2.27)\n",
      "(-1.523757878711393, -2.22)\n",
      "(-1.4029510782014758, -1.96)\n",
      "(-1.3745259486697305, -1.94)\n",
      "(-1.2963568424574312, -1.91)\n",
      "(-1.3176756896062403, -1.91)\n",
      "(-1.381632231052667, -1.91)\n",
      "(-1.4029510782014758, -1.87)\n",
      "(-1.1897626067133866, -1.83)\n",
      "(-1.3105694072233038, -1.82)\n",
      "(-1.3034631248403676, -1.8)\n",
      "(-1.225294018628068, -1.79)\n",
      "(-1.175550041947514, -1.74)\n",
      "(-1.1115935005010873, -1.68)\n",
      "(-1.033424394288788, -1.67)\n",
      "(-1.1400186300328325, -1.66)\n",
      "(-1.1613374771816414, -1.66)\n",
      "(-1.2110814538621955, -1.66)\n",
      "(-1.3034631248403676, -1.64)\n",
      "(-1.2181877362451319, -1.61)\n",
      "(-1.0902746533522785, -1.6)\n",
      "(-1.12580606526696, -1.59)\n",
      "(-1.147124912415769, -1.59)\n",
      "(-1.1684437595645776, -1.58)\n",
      "(-1.012105547139979, -1.57)\n",
      "(-1.0547432414375968, -1.54)\n",
      "(-0.9765741352252976, -1.5)\n",
      "(-0.9765741352252976, -1.48)\n",
      "(-0.9978929823741064, -1.46)\n",
      "(-1.0476369590546606, -1.42)\n",
      "(-0.9197238761618072, -1.4)\n",
      "(-0.9339364409276798, -1.4)\n",
      "(-1.033424394288788, -1.4)\n",
      "(-0.9339364409276798, -1.39)\n",
      "(-0.869979899481253, -1.38)\n",
      "(-0.9552552880764887, -1.37)\n",
      "(-0.9694678528423614, -1.36)\n",
      "(-0.8415547699495077, -1.35)\n",
      "(-0.9197238761618072, -1.34)\n",
      "(-0.9410427233106161, -1.34)\n",
      "(-0.7847045108860174, -1.3)\n",
      "(-0.9694678528423614, -1.3)\n",
      "(-0.8060233580348262, -1.29)\n",
      "(-1.0405306766717242, -1.29)\n",
      "(-0.8415547699495077, -1.27)\n",
      "(-0.8273422051836351, -1.26)\n",
      "(-0.848661052332444, -1.25)\n",
      "(-0.869979899481253, -1.25)\n",
      "(-0.9339364409276798, -1.25)\n",
      "(-0.7704919461201448, -1.24)\n",
      "(-0.7704919461201448, -1.23)\n",
      "(-0.7847045108860174, -1.2)\n",
      "(-0.848661052332444, -1.2)\n",
      "(-0.962361570459425, -1.2)\n",
      "(-0.6852165575249091, -1.16)\n",
      "(-0.7918107932689538, -1.16)\n",
      "(-0.8841924642471256, -1.16)\n",
      "(-0.8131296404177625, -1.14)\n",
      "(-0.7775982285030811, -1.12)\n",
      "(-0.7349605342054633, -1.09)\n",
      "(-0.7491730989713359, -1.09)\n",
      "(-0.8344484875665714, -1.09)\n",
      "(-0.7775982285030811, -1.08)\n",
      "(-0.7918107932689538, -1.08)\n",
      "(-0.8273422051836351, -1.04)\n",
      "(-0.7562793813542722, -1.03)\n",
      "(-0.6781102751419728, -1.02)\n",
      "(-0.7633856637372085, -1.01)\n",
      "(-0.8273422051836351, -1.01)\n",
      "(-0.614153733695546, -1.0)\n",
      "(-0.7704919461201448, -0.99)\n",
      "(-0.635472580844355, -0.98)\n",
      "(-0.7349605342054633, -0.98)\n",
      "(-0.5573034746320557, -0.97)\n",
      "(-0.5715160393979283, -0.95)\n",
      "(-0.5928348865467371, -0.95)\n",
      "(-0.6781102751419728, -0.95)\n",
      "(-0.706535404673718, -0.94)\n",
      "(-0.7491730989713359, -0.93)\n",
      "(-0.5004532155685651, -0.91)\n",
      "(-0.6070474513126097, -0.91)\n",
      "(-0.7207479694395906, -0.91)\n",
      "(-0.5501971922491193, -0.9)\n",
      "(-0.6283662984614187, -0.9)\n",
      "(-0.543090909866183, -0.88)\n",
      "(-0.614153733695546, -0.87)\n",
      "(-0.564409757014992, -0.85)\n",
      "(-0.6283662984614187, -0.84)\n",
      "(-0.7349605342054633, -0.82)\n",
      "(-0.5146657803344378, -0.81)\n",
      "(-0.5857286041638008, -0.81)\n",
      "(-0.6070474513126097, -0.81)\n",
      "(-0.5146657803344378, -0.8)\n",
      "(-0.564409757014992, -0.8)\n",
      "(-0.564409757014992, -0.8)\n",
      "(-0.6425788632272913, -0.79)\n",
      "(-0.46492180365388364, -0.78)\n",
      "(-0.6710039927590364, -0.78)\n",
      "(-0.6638977103761001, -0.77)\n",
      "(-0.46492180365388364, -0.76)\n",
      "(-0.5928348865467371, -0.76)\n",
      "(-0.5075594979515015, -0.75)\n",
      "(-0.564409757014992, -0.75)\n",
      "(-0.5928348865467371, -0.75)\n",
      "(-0.6567914279931639, -0.74)\n",
      "(-0.379646415058648, -0.73)\n",
      "(-0.4436029565050747, -0.73)\n",
      "(-0.47202808603681995, -0.73)\n",
      "(-0.41517782697332944, -0.72)\n",
      "(-0.3867526974415843, -0.71)\n",
      "(-0.4364966741221384, -0.71)\n",
      "(-0.47913436841975626, -0.71)\n",
      "(-0.5075594979515015, -0.71)\n",
      "(-0.3867526974415843, -0.7)\n",
      "(-0.5359846274832467, -0.7)\n",
      "(-0.543090909866183, -0.7)\n",
      "(-0.4009652622074568, -0.69)\n",
      "(-0.5146657803344378, -0.69)\n",
      "(-0.5217720627173741, -0.69)\n",
      "(-0.6212600160784824, -0.68)\n",
      "(-0.4578155212709473, -0.65)\n",
      "(-0.5146657803344378, -0.65)\n",
      "(-0.6212600160784824, -0.65)\n",
      "(-0.3441150031439664, -0.63)\n",
      "(-0.42228410935626576, -0.62)\n",
      "(-0.4436029565050747, -0.61)\n",
      "(-0.5217720627173741, -0.61)\n",
      "(-0.24462704978285812, -0.6)\n",
      "(-0.3725401326757117, -0.6)\n",
      "(-0.3938589798245206, -0.6)\n",
      "(-0.379646415058648, -0.59)\n",
      "(-0.4436029565050747, -0.59)\n",
      "(-0.46492180365388364, -0.59)\n",
      "(-0.2943710264634123, -0.57)\n",
      "(-0.379646415058648, -0.57)\n",
      "(-0.287264744080476, -0.56)\n",
      "(-0.5501971922491193, -0.56)\n",
      "(-0.450709238888011, -0.55)\n",
      "(-0.4862406508026925, -0.55)\n",
      "(-0.2801584616975397, -0.54)\n",
      "(-0.3441150031439664, -0.54)\n",
      "(-0.35832756790983905, -0.53)\n",
      "(-0.4364966741221384, -0.53)\n",
      "(-0.23752076739992184, -0.51)\n",
      "(-0.3014773088463486, -0.51)\n",
      "(-0.2588396145487308, -0.49)\n",
      "(-0.4578155212709473, -0.49)\n",
      "(-0.3867526974415843, -0.48)\n",
      "(-0.4862406508026925, -0.48)\n",
      "(-0.3299024383780938, -0.47)\n",
      "(-0.3085835912292849, -0.46)\n",
      "(-0.450709238888011, -0.46)\n",
      "(-0.2233082026340492, -0.45)\n",
      "(-0.31568987361222123, -0.44)\n",
      "(-0.35122128552690274, -0.44)\n",
      "(-0.32279615599515754, -0.42)\n",
      "(-0.3441150031439664, -0.42)\n",
      "(-0.2233082026340492, -0.41)\n",
      "(-0.15224537880468617, -0.4)\n",
      "(-0.2090956378681766, -0.4)\n",
      "(-0.2588396145487308, -0.4)\n",
      "(-0.3938589798245206, -0.4)\n",
      "(-0.2659458969316671, -0.39)\n",
      "(-0.31568987361222123, -0.39)\n",
      "(-0.11671396689000464, -0.38)\n",
      "(-0.17356422595349508, -0.35)\n",
      "(-0.1806705083364314, -0.35)\n",
      "(-0.3299024383780938, -0.35)\n",
      "(-0.2233082026340492, -0.34)\n",
      "(-0.2233082026340492, -0.34)\n",
      "(-0.2090956378681766, -0.33)\n",
      "(-0.14513909642174985, -0.32)\n",
      "(-0.17356422595349508, -0.32)\n",
      "(-0.1806705083364314, -0.31)\n",
      "(-0.12382024927294094, -0.3)\n",
      "(-0.2162019202511129, -0.3)\n",
      "(-0.24462704978285812, -0.3)\n",
      "(-0.2801584616975397, -0.3)\n",
      "(-0.287264744080476, -0.3)\n",
      "(-0.2943710264634123, -0.3)\n",
      "(-0.23752076739992184, -0.29)\n",
      "(-0.1877767907193677, -0.28)\n",
      "(-0.20198935548524027, -0.28)\n",
      "(-0.23041448501698553, -0.28)\n",
      "(-0.25173333216579447, -0.28)\n",
      "(-0.10960768450706833, -0.27)\n",
      "(-0.379646415058648, -0.26)\n",
      "(-0.20198935548524027, -0.25)\n",
      "(-0.2730521793146034, -0.25)\n",
      "(-0.038544860677705274, -0.24)\n",
      "(-0.1806705083364314, -0.24)\n",
      "(-0.1877767907193677, -0.24)\n",
      "(-0.20198935548524027, -0.24)\n",
      "(-0.20198935548524027, -0.23)\n",
      "(-0.20198935548524027, -0.22)\n",
      "(-0.2801584616975397, -0.22)\n",
      "(-0.2233082026340492, -0.21)\n",
      "(-0.23041448501698553, -0.21)\n",
      "(-0.038544860677705274, -0.2)\n",
      "(-0.10960768450706833, -0.19)\n",
      "(-0.14513909642174985, -0.19)\n",
      "(-0.3085835912292849, -0.19)\n",
      "(0.01119911600284886, -0.18)\n",
      "(-0.05986370782651419, -0.18)\n",
      "(-0.0669699902094505, -0.18)\n",
      "(-0.09539511974119573, -0.18)\n",
      "(-0.09539511974119573, -0.16)\n",
      "(-0.15935166118762248, -0.15)\n",
      "(-0.010119731145960056, -0.14)\n",
      "(-0.08828883735825942, -0.14)\n",
      "(-0.20198935548524027, -0.14)\n",
      "(-0.05986370782651419, -0.13)\n",
      "(-0.10960768450706833, -0.13)\n",
      "(0.02541168076872147, -0.12)\n",
      "(-0.1877767907193677, -0.12)\n",
      "(-0.024332295911832665, -0.11)\n",
      "(-0.0811825549753231, -0.11)\n",
      "(-0.1877767907193677, -0.11)\n",
      "(-0.0030134487630237503, -0.1)\n",
      "(-0.031438578294768975, -0.1)\n",
      "(-0.0669699902094505, -0.1)\n",
      "(-0.0740762725923868, -0.1)\n",
      "(-0.0030134487630237503, -0.09)\n",
      "(-0.11671396689000464, -0.09)\n",
      "(0.0680493750663393, -0.07)\n",
      "(0.01119911600284886, -0.07)\n",
      "(-0.010119731145960056, -0.07)\n",
      "(-0.12382024927294094, -0.05)\n",
      "(-0.15935166118762248, -0.04)\n",
      "(0.08936822221514822, -0.02)\n",
      "(0.032517963151657774, -0.02)\n",
      "(0.032517963151657774, -0.02)\n",
      "(-0.05986370782651419, -0.01)\n",
      "(0.10358078698102084, 0.0)\n",
      "(0.07515565744927562, -0.0)\n",
      "(0.0538368103004667, 0.0)\n",
      "(0.046730527917530386, -0.0)\n",
      "(-0.09539511974119573, 0.0)\n",
      "(0.11068706936395713, 0.01)\n",
      "(0.046730527917530386, 0.01)\n",
      "(0.02541168076872147, 0.02)\n",
      "(0.004092833619912555, 0.02)\n",
      "(-0.0030134487630237503, 0.02)\n",
      "(0.07515565744927562, 0.03)\n",
      "(0.032517963151657774, 0.03)\n",
      "(-0.01722601352889636, 0.03)\n",
      "(-0.0030134487630237503, 0.04)\n",
      "(0.0538368103004667, 0.05)\n",
      "(0.018305398385785165, 0.05)\n",
      "(0.004092833619912555, 0.05)\n",
      "(0.13911219889570237, 0.08)\n",
      "(0.060943092683403, 0.08)\n",
      "(0.046730527917530386, 0.08)\n",
      "(-0.0030134487630237503, 0.08)\n",
      "(0.10358078698102084, 0.09)\n",
      "(0.08226193983221192, 0.09)\n",
      "(0.032517963151657774, 0.09)\n",
      "(0.13200591651276605, 0.1)\n",
      "(-0.010119731145960056, 0.1)\n",
      "(0.10358078698102084, 0.11)\n",
      "(0.0680493750663393, 0.11)\n",
      "(0.0538368103004667, 0.11)\n",
      "(0.01119911600284886, 0.11)\n",
      "(0.1675373284274476, 0.12)\n",
      "(-0.0030134487630237503, 0.12)\n",
      "(0.11068706936395713, 0.13)\n",
      "(0.08226193983221192, 0.13)\n",
      "(0.0538368103004667, 0.13)\n",
      "(0.224387587490938, 0.14)\n",
      "(0.08226193983221192, 0.14)\n",
      "(0.2528127170226832, 0.15)\n",
      "(0.19596245795919282, 0.15)\n",
      "(0.14621848127863868, 0.15)\n",
      "(0.07515565744927562, 0.17)\n",
      "(0.0680493750663393, 0.17)\n",
      "(0.018305398385785165, 0.17)\n",
      "(0.28123784655442846, 0.18)\n",
      "(0.15332476366157496, 0.18)\n",
      "(0.13200591651276605, 0.18)\n",
      "(0.0680493750663393, 0.18)\n",
      "(0.14621848127863868, 0.19)\n",
      "(0.0680493750663393, 0.19)\n",
      "(0.1888561755762565, 0.2)\n",
      "(0.2599189994056195, 0.21)\n",
      "(0.1675373284274476, 0.21)\n",
      "(0.13200591651276605, 0.21)\n",
      "(0.13200591651276605, 0.21)\n",
      "(0.23149386987387433, 0.22)\n",
      "(0.224387587490938, 0.22)\n",
      "(0.17464361081038388, 0.23)\n",
      "(0.20306874034212913, 0.24)\n",
      "(0.19596245795919282, 0.24)\n",
      "(0.16043104604451128, 0.24)\n",
      "(0.17464361081038388, 0.25)\n",
      "(0.13911219889570237, 0.25)\n",
      "(0.24570643463974695, 0.26)\n",
      "(0.23860015225681064, 0.27)\n",
      "(0.2528127170226832, 0.28)\n",
      "(0.1817498931933202, 0.29)\n",
      "(0.1817498931933202, 0.29)\n",
      "(0.3238755408520463, 0.3)\n",
      "(0.1888561755762565, 0.3)\n",
      "(0.15332476366157496, 0.3)\n",
      "(0.224387587490938, 0.31)\n",
      "(0.224387587490938, 0.31)\n",
      "(0.2172813051080017, 0.31)\n",
      "(0.27413156417149215, 0.32)\n",
      "(0.2172813051080017, 0.32)\n",
      "(0.1817498931933202, 0.33)\n",
      "(0.12489963412982975, 0.33)\n",
      "(0.2883441289373648, 0.34)\n",
      "(0.19596245795919282, 0.34)\n",
      "(0.24570643463974695, 0.35)\n",
      "(0.30966297608617366, 0.36)\n",
      "(0.33808810561791885, 0.37)\n",
      "(0.2599189994056195, 0.37)\n",
      "(0.23860015225681064, 0.37)\n",
      "(0.3309818232349826, 0.39)\n",
      "(0.30255669370323734, 0.39)\n",
      "(0.39493836468140936, 0.41)\n",
      "(0.31676925846910997, 0.41)\n",
      "(0.21017502272506539, 0.42)\n",
      "(0.33808810561791885, 0.43)\n",
      "(0.26702528178855583, 0.44)\n",
      "(0.4660011885107724, 0.45)\n",
      "(0.3736195175326004, 0.45)\n",
      "(0.29545041132030103, 0.45)\n",
      "(0.224387587490938, 0.45)\n",
      "(0.31676925846910997, 0.46)\n",
      "(0.24570643463974695, 0.47)\n",
      "(0.4162572118302182, 0.48)\n",
      "(0.4162572118302182, 0.48)\n",
      "(0.3736195175326004, 0.49)\n",
      "(0.3238755408520463, 0.49)\n",
      "(0.26702528178855583, 0.49)\n",
      "(0.4660011885107724, 0.5)\n",
      "(0.44468234136196344, 0.5)\n",
      "(0.34519438800085517, 0.5)\n",
      "(0.224387587490938, 0.51)\n",
      "(0.4020446470643457, 0.53)\n",
      "(0.31676925846910997, 0.53)\n",
      "(0.20306874034212913, 0.53)\n",
      "(0.3665132351496641, 0.54)\n",
      "(0.30255669370323734, 0.54)\n",
      "(0.34519438800085517, 0.55)\n",
      "(0.31676925846910997, 0.55)\n",
      "(0.4375760589790271, 0.58)\n",
      "(0.4944263180425176, 0.59)\n",
      "(0.4304697765960908, 0.59)\n",
      "(0.4020446470643457, 0.61)\n",
      "(0.34519438800085517, 0.61)\n",
      "(0.4020446470643457, 0.63)\n",
      "(0.34519438800085517, 0.64)\n",
      "(0.45889490612783607, 0.65)\n",
      "(0.4304697765960908, 0.65)\n",
      "(0.38783208229847305, 0.65)\n",
      "(0.3238755408520463, 0.65)\n",
      "(0.4731074708937087, 0.66)\n",
      "(0.5228514475742628, 0.67)\n",
      "(0.6010205537865622, 0.68)\n",
      "(0.5299577299571991, 0.68)\n",
      "(0.45889490612783607, 0.68)\n",
      "(0.45178862374489975, 0.68)\n",
      "(0.6223394009353711, 0.69)\n",
      "(0.5797017066377533, 0.69)\n",
      "(0.5228514475742628, 0.69)\n",
      "(0.44468234136196344, 0.69)\n",
      "(0.4304697765960908, 0.69)\n",
      "(0.5868079890206895, 0.7)\n",
      "(0.38072579991553673, 0.7)\n",
      "(0.5797017066377533, 0.72)\n",
      "(0.5797017066377533, 0.72)\n",
      "(0.572595424254817, 0.73)\n",
      "(0.4873200356595813, 0.73)\n",
      "(0.45889490612783607, 0.74)\n",
      "(0.4162572118302182, 0.74)\n",
      "(0.6081268361694985, 0.75)\n",
      "(0.4304697765960908, 0.75)\n",
      "(0.572595424254817, 0.76)\n",
      "(0.5512765771060081, 0.76)\n",
      "(0.4873200356595813, 0.77)\n",
      "(0.5797017066377533, 0.78)\n",
      "(0.572595424254817, 0.78)\n",
      "(0.5797017066377533, 0.81)\n",
      "(0.5157451651913265, 0.81)\n",
      "(0.5939142714036258, 0.82)\n",
      "(0.5868079890206895, 0.84)\n",
      "(0.5512765771060081, 0.85)\n",
      "(0.5299577299571991, 0.85)\n",
      "(0.7715713309770336, 0.86)\n",
      "(0.6294456833183074, 0.87)\n",
      "(0.5939142714036258, 0.89)\n",
      "(0.6862959423817978, 0.9)\n",
      "(0.664977095232989, 0.9)\n",
      "(0.64365824808418, 0.9)\n",
      "(0.5157451651913265, 0.9)\n",
      "(0.6862959423817978, 0.93)\n",
      "(0.664977095232989, 0.93)\n",
      "(0.6081268361694985, 0.93)\n",
      "(0.5654891418718807, 0.93)\n",
      "(0.6365519657012437, 0.95)\n",
      "(0.6152331185524348, 0.95)\n",
      "(0.7502524838282246, 0.96)\n",
      "(0.6010205537865622, 0.96)\n",
      "(0.7005085071476704, 0.97)\n",
      "(0.64365824808418, 0.97)\n",
      "(0.6081268361694985, 0.97)\n",
      "(0.8213153076575875, 0.98)\n",
      "(0.6507645304671164, 0.98)\n",
      "(0.7147210719135431, 0.99)\n",
      "(0.7715713309770336, 1.0)\n",
      "(0.7431462014452883, 1.0)\n",
      "(0.6934022247647341, 1.0)\n",
      "(0.6934022247647341, 1.01)\n",
      "(0.6720833776159252, 1.01)\n",
      "(0.64365824808418, 1.03)\n",
      "(0.7502524838282246, 1.04)\n",
      "(0.6365519657012437, 1.04)\n",
      "(0.7715713309770336, 1.05)\n",
      "(0.6294456833183074, 1.05)\n",
      "(0.572595424254817, 1.05)\n",
      "(0.7928901781258425, 1.06)\n",
      "(0.7928901781258425, 1.06)\n",
      "(0.7502524838282246, 1.06)\n",
      "(0.7928901781258425, 1.08)\n",
      "(0.7715713309770336, 1.08)\n",
      "(0.7928901781258425, 1.09)\n",
      "(0.8568467195722691, 1.11)\n",
      "(0.8284215900405238, 1.11)\n",
      "(0.7857838957429062, 1.12)\n",
      "(0.7573587662111609, 1.14)\n",
      "(1.0202912143798042, 1.15)\n",
      "(0.8710592843381417, 1.16)\n",
      "(0.8639530019552054, 1.17)\n",
      "(0.9279095434016322, 1.22)\n",
      "(0.8426341548063965, 1.24)\n",
      "(0.9492283905504412, 1.25)\n",
      "(0.9634409553163138, 1.26)\n",
      "(0.9350158257845685, 1.26)\n",
      "(0.8355278724234602, 1.27)\n",
      "(0.8568467195722691, 1.28)\n",
      "(0.9492283905504412, 1.31)\n",
      "(1.062928908677422, 1.32)\n",
      "(0.9634409553163138, 1.35)\n",
      "(1.0416100615286132, 1.36)\n",
      "(1.0060786496139316, 1.37)\n",
      "(0.9492283905504412, 1.38)\n",
      "(0.9847598024651226, 1.39)\n",
      "(0.8639530019552054, 1.41)\n",
      "(1.013184931996868, 1.44)\n",
      "(0.9065906962528233, 1.44)\n",
      "(1.013184931996868, 1.48)\n",
      "(1.0345037791456768, 1.49)\n",
      "(1.1482042972726578, 1.5)\n",
      "(1.1410980148897216, 1.5)\n",
      "(0.8852718491040144, 1.5)\n",
      "(1.1482042972726578, 1.51)\n",
      "(1.1197791677409126, 1.51)\n",
      "(1.062928908677422, 1.54)\n",
      "(1.2619048153996386, 1.59)\n",
      "(1.1695231444214667, 1.61)\n",
      "(1.0060786496139316, 1.64)\n",
      "(1.1197791677409126, 1.65)\n",
      "(1.1837357091873393, 1.69)\n",
      "(1.3471802039948744, 1.72)\n",
      "(1.0842477558262311, 1.74)\n",
      "(1.3329676392290017, 1.76)\n",
      "(1.2761173801655112, 1.8)\n",
      "(1.205054556336148, 1.8)\n",
      "(1.311648792080193, 1.82)\n",
      "(1.2121608387190845, 1.83)\n",
      "(1.3329676392290017, 1.86)\n",
      "(1.3329676392290017, 1.86)\n",
      "(1.2832236625484477, 1.9)\n",
      "(1.3045425096972565, 1.91)\n",
      "(1.4893058516536004, 1.93)\n",
      "(1.3898178982924922, 1.96)\n",
      "(1.4679870045047916, 1.98)\n",
      "(1.5816875226317726, 2.08)\n",
      "(1.5603686754829635, 2.14)\n",
      "(1.6385377816952627, 2.31)\n",
      "(1.5319435459512183, 2.33)\n",
      "(1.7096006055246258, 2.39)\n",
      "(1.8019822765027977, 2.48)\n",
      "(1.851726253183352, 2.56)\n",
      "(1.9156827946297788, 2.62)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a SparkSession (Note, the config section is only for Windows!)\n",
    "    spark = SparkSession.builder.appName(\"LinearRegression\").getOrCreate()\n",
    "\n",
    "    # Load up our data and convert it to the format MLLib expects.\n",
    "    inputLines = spark.sparkContext.textFile(\"regression.txt\")\n",
    "    data = inputLines.map(lambda x: x.split(\",\")).map(lambda x: (float(x[0]), Vectors.dense(float(x[1]))))\n",
    "\n",
    "    # Convert this RDD to a DataFrame\n",
    "    colNames = [\"label\", \"features\"]\n",
    "    df = data.toDF(colNames)\n",
    "\n",
    "    # Note, there are lots of cases where you can avoid going from an RDD to a DataFrame.\n",
    "    # Perhaps you're importing data from a real database. Or you are using structured streaming\n",
    "    # to get your data.\n",
    "\n",
    "    # Let's split our data into training data and testing data\n",
    "    trainTest = df.randomSplit([0.5, 0.5])\n",
    "    trainingDF = trainTest[0]\n",
    "    testDF = trainTest[1]\n",
    "\n",
    "    # Now create our linear regression model\n",
    "    lir = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "    # Train the model using our training data\n",
    "    model = lir.fit(trainingDF)\n",
    "\n",
    "    # Now see if we can predict values in our test data.\n",
    "    # Generate predictions using our linear regression model for all features in our\n",
    "    # test dataframe:\n",
    "    fullPredictions = model.transform(testDF).cache()\n",
    "\n",
    "    # Extract the predictions and the \"known\" correct labels.\n",
    "    predictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "    labels = fullPredictions.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "    # Zip them together\n",
    "    predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "    # Print out the predicted and actual values for each point\n",
    "    for prediction in predictionAndLabel:\n",
    "      print(prediction)\n",
    "\n",
    "\n",
    "    # Stop the session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39.68108108108107, 7.6)\n",
      "(19.200000000000003, 11.2)\n",
      "(17.73333333333333, 12.2)\n",
      "(17.216666666666665, 12.8)\n",
      "(23.9962962962963, 12.8)\n",
      "(20.96, 12.9)\n",
      "(15.3, 13.4)\n",
      "(23.9962962962963, 13.7)\n",
      "(23.9962962962963, 13.8)\n",
      "(15.3, 14.4)\n",
      "(17.216666666666665, 14.7)\n",
      "(17.216666666666665, 15.4)\n",
      "(17.73333333333333, 15.5)\n",
      "(17.216666666666665, 15.9)\n",
      "(23.9962962962963, 16.1)\n",
      "(17.73333333333333, 17.4)\n",
      "(17.216666666666665, 18.6)\n",
      "(17.73333333333333, 18.8)\n",
      "(23.9962962962963, 19.0)\n",
      "(18.25, 20.7)\n",
      "(29.49230769230769, 21.3)\n",
      "(23.9962962962963, 21.5)\n",
      "(29.49230769230769, 21.7)\n",
      "(23.9962962962963, 21.8)\n",
      "(15.3, 22.1)\n",
      "(20.96, 22.3)\n",
      "(29.500000000000014, 22.8)\n",
      "(23.9962962962963, 22.9)\n",
      "(29.49230769230769, 23.1)\n",
      "(23.9962962962963, 23.2)\n",
      "(29.49230769230769, 23.5)\n",
      "(23.9962962962963, 23.7)\n",
      "(33.53333333333334, 23.9)\n",
      "(23.9962962962963, 24.7)\n",
      "(17.73333333333333, 24.7)\n",
      "(23.9962962962963, 24.8)\n",
      "(29.49230769230769, 25.3)\n",
      "(22.250000000000004, 25.3)\n",
      "(23.9962962962963, 25.7)\n",
      "(39.68108108108107, 26.5)\n",
      "(39.68108108108107, 26.5)\n",
      "(23.9962962962963, 26.5)\n",
      "(23.9962962962963, 26.6)\n",
      "(42.3375, 26.9)\n",
      "(23.9962962962963, 27.0)\n",
      "(23.9962962962963, 27.0)\n",
      "(33.53333333333334, 27.3)\n",
      "(17.216666666666665, 27.3)\n",
      "(23.9962962962963, 27.3)\n",
      "(29.49230769230769, 27.7)\n",
      "(23.9962962962963, 27.7)\n",
      "(23.9962962962963, 27.7)\n",
      "(23.9962962962963, 28.1)\n",
      "(29.49230769230769, 28.4)\n",
      "(29.49230769230769, 28.4)\n",
      "(42.3375, 28.5)\n",
      "(41.391666666666666, 28.5)\n",
      "(29.49230769230769, 28.6)\n",
      "(29.49230769230769, 29.3)\n",
      "(20.96, 29.3)\n",
      "(29.49230769230769, 29.5)\n",
      "(41.391666666666666, 29.8)\n",
      "(41.391666666666666, 30.0)\n",
      "(29.49230769230769, 30.7)\n",
      "(29.500000000000014, 30.7)\n",
      "(23.9962962962963, 30.8)\n",
      "(23.9962962962963, 31.1)\n",
      "(29.49230769230769, 31.3)\n",
      "(29.49230769230769, 31.3)\n",
      "(29.49230769230769, 31.7)\n",
      "(41.391666666666666, 32.4)\n",
      "(42.3375, 32.9)\n",
      "(39.68108108108107, 32.9)\n",
      "(42.3375, 33.1)\n",
      "(41.391666666666666, 33.1)\n",
      "(39.68108108108107, 34.0)\n",
      "(41.391666666666666, 34.1)\n",
      "(41.391666666666666, 34.2)\n",
      "(41.391666666666666, 34.2)\n",
      "(41.391666666666666, 34.7)\n",
      "(41.391666666666666, 35.1)\n",
      "(41.391666666666666, 35.3)\n",
      "(29.49230769230769, 35.6)\n",
      "(32.97142857142857, 36.2)\n",
      "(32.97142857142857, 36.3)\n",
      "(32.97142857142857, 36.5)\n",
      "(33.53333333333334, 36.7)\n",
      "(32.97142857142857, 36.8)\n",
      "(39.68108108108107, 37.0)\n",
      "(39.68108108108107, 37.4)\n",
      "(39.68108108108107, 37.4)\n",
      "(41.391666666666666, 37.4)\n",
      "(39.68108108108107, 37.5)\n",
      "(39.68108108108107, 37.5)\n",
      "(39.68108108108107, 37.8)\n",
      "(49.339999999999996, 37.9)\n",
      "(42.3375, 37.9)\n",
      "(39.68108108108107, 38.1)\n",
      "(41.391666666666666, 38.1)\n",
      "(41.391666666666666, 38.3)\n",
      "(41.391666666666666, 38.4)\n",
      "(45.75, 38.5)\n",
      "(32.97142857142857, 38.6)\n",
      "(42.3375, 39.3)\n",
      "(39.68108108108107, 39.3)\n",
      "(39.68108108108107, 39.4)\n",
      "(39.68108108108107, 39.6)\n",
      "(42.3375, 39.7)\n",
      "(38.2, 40.0)\n",
      "(32.97142857142857, 40.2)\n",
      "(39.68108108108107, 40.3)\n",
      "(41.391666666666666, 40.3)\n",
      "(52.02, 40.5)\n",
      "(32.97142857142857, 40.6)\n",
      "(41.391666666666666, 40.8)\n",
      "(71.43333333333332, 40.9)\n",
      "(42.3375, 40.9)\n",
      "(78.30000000000001, 41.0)\n",
      "(41.391666666666666, 41.1)\n",
      "(48.85999999999999, 41.2)\n",
      "(22.250000000000004, 41.2)\n",
      "(42.3375, 41.4)\n",
      "(71.43333333333332, 41.6)\n",
      "(39.68108108108107, 42.0)\n",
      "(39.68108108108107, 42.0)\n",
      "(39.68108108108107, 42.2)\n",
      "(39.68108108108107, 42.2)\n",
      "(45.75, 42.3)\n",
      "(39.68108108108107, 42.4)\n",
      "(39.68108108108107, 42.5)\n",
      "(39.68108108108107, 42.5)\n",
      "(39.68108108108107, 42.5)\n",
      "(42.3375, 42.5)\n",
      "(48.666666666666664, 42.7)\n",
      "(45.75, 43.2)\n",
      "(39.68108108108107, 43.4)\n",
      "(39.68108108108107, 43.8)\n",
      "(39.68108108108107, 44.0)\n",
      "(48.85999999999999, 44.5)\n",
      "(52.02, 44.7)\n",
      "(52.02, 44.8)\n",
      "(52.02, 44.9)\n",
      "(52.02, 45.2)\n",
      "(42.3375, 45.3)\n",
      "(36.700000000000045, 45.5)\n",
      "(41.391666666666666, 46.0)\n",
      "(42.3375, 46.1)\n",
      "(48.85999999999999, 46.2)\n",
      "(20.96, 46.6)\n",
      "(42.3375, 46.6)\n",
      "(41.391666666666666, 46.6)\n",
      "(48.85999999999999, 46.7)\n",
      "(39.68108108108107, 46.8)\n",
      "(42.3375, 47.0)\n",
      "(42.3375, 47.1)\n",
      "(42.3375, 47.3)\n",
      "(39.68108108108107, 47.3)\n",
      "(39.68108108108107, 47.4)\n",
      "(47.699999999999996, 47.7)\n",
      "(71.43333333333332, 47.7)\n",
      "(42.3375, 48.1)\n",
      "(42.3375, 48.2)\n",
      "(32.97142857142857, 48.5)\n",
      "(42.3375, 48.5)\n",
      "(47.699999999999996, 49.0)\n",
      "(39.68108108108107, 49.3)\n",
      "(39.68108108108107, 49.3)\n",
      "(47.699999999999996, 49.7)\n",
      "(57.17272727272728, 50.0)\n",
      "(42.3375, 50.2)\n",
      "(49.339999999999996, 50.4)\n",
      "(48.85999999999999, 51.0)\n",
      "(39.68108108108107, 51.0)\n",
      "(42.3375, 51.6)\n",
      "(49.339999999999996, 51.8)\n",
      "(49.339999999999996, 52.2)\n",
      "(42.3375, 52.5)\n",
      "(48.85999999999999, 53.0)\n",
      "(48.85999999999999, 53.3)\n",
      "(36.700000000000045, 53.9)\n",
      "(57.17272727272728, 54.4)\n",
      "(39.68108108108107, 54.8)\n",
      "(36.700000000000045, 55.0)\n",
      "(48.85999999999999, 55.0)\n",
      "(42.3375, 55.1)\n",
      "(49.339999999999996, 55.2)\n",
      "(32.97142857142857, 55.3)\n",
      "(42.3375, 55.5)\n",
      "(42.3375, 55.9)\n",
      "(49.339999999999996, 56.2)\n",
      "(42.3375, 56.8)\n",
      "(36.700000000000045, 57.1)\n",
      "(42.3375, 57.4)\n",
      "(57.17272727272728, 58.1)\n",
      "(57.17272727272728, 58.1)\n",
      "(49.339999999999996, 58.8)\n",
      "(57.17272727272728, 59.0)\n",
      "(48.85999999999999, 59.6)\n",
      "(49.339999999999996, 60.7)\n",
      "(57.17272727272728, 62.1)\n",
      "(57.17272727272728, 62.2)\n",
      "(71.43333333333332, 63.3)\n",
      "(78.30000000000001, 63.3)\n",
      "(48.666666666666664, 67.7)\n",
      "(71.43333333333332, 70.1)\n",
      "(41.391666666666666, 78.0)\n",
      "(42.3375, 117.5)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a SparkSession (Note, the config section is only for Windows!)\n",
    "    spark = SparkSession.builder.appName(\"DecisionTree\").getOrCreate()\n",
    "\n",
    "    \n",
    "    # Load up data as dataframe\n",
    "    data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "        .csv(\"realestate.csv\")\n",
    "\n",
    "    assembler = VectorAssembler().setInputCols([\"HouseAge\", \"DistanceToMRT\", \\\n",
    "                               \"NumberConvenienceStores\"]).setOutputCol(\"features\")\n",
    "    \n",
    "    df = assembler.transform(data).select(\"PriceOfUnitArea\", \"features\")\n",
    "\n",
    "    # Let's split our data into training data and testing data\n",
    "    trainTest = df.randomSplit([0.5, 0.5])\n",
    "    trainingDF = trainTest[0]\n",
    "    testDF = trainTest[1]\n",
    "\n",
    "    # Now create our decision tree\n",
    "    dtr = DecisionTreeRegressor().setFeaturesCol(\"features\").setLabelCol(\"PriceOfUnitArea\")\n",
    "\n",
    "    # Train the model using our training data\n",
    "    model = dtr.fit(trainingDF)\n",
    "\n",
    "    # Now see if we can predict values in our test data.\n",
    "    # Generate predictions using our decision tree model for all features in our\n",
    "    # test dataframe:\n",
    "    fullPredictions = model.transform(testDF).cache()\n",
    "\n",
    "    # Extract the predictions and the \"known\" correct labels.\n",
    "    predictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "    labels = fullPredictions.select(\"PriceOfUnitArea\").rdd.map(lambda x: x[0])\n",
    "\n",
    "    # Zip them together\n",
    "    predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "    # Print out the predicted and actual values for each point\n",
    "    for prediction in predictionAndLabel:\n",
    "        print(prediction)\n",
    "\n",
    "\n",
    "    # Stop the session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>__Spark Streaming__<ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1277.text.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:567)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:268)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:164)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:164)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:34)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:196)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:210)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.text(DataStreamReader.scala:343)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.warehouse.dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:///C:/tmp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStructuredStreaming\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Monitor the logs directory for new log data, and read in the raw lines as accessLines\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m accessLines \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Parse out the common log format to a DataFrame\u001b[39;00m\n\u001b[0;32m     21\u001b[0m contentSizeExp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)$\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\streaming.py:702\u001b[0m, in \u001b[0;36mDataStreamReader.text\u001b[1;34m(self, path, wholetext, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    696\u001b[0m     wholetext\u001b[38;5;241m=\u001b[39mwholetext,\n\u001b[0;32m    697\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m    698\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m    699\u001b[0m     recursiveFileLookup\u001b[38;5;241m=\u001b[39mrecursiveFileLookup,\n\u001b[0;32m    700\u001b[0m )\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath can be only a single string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1277.text.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:567)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:268)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:164)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:164)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:34)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:196)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:210)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.text(DataStreamReader.scala:343)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Dec 18 09:15:05 2019\n",
    "\n",
    "@author: Frank\n",
    "\"\"\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Create a SparkSession (the config bit is only for Windows!)\n",
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"file:///C:/tmp\").appName(\"StructuredStreaming\").getOrCreate()\n",
    "\n",
    "# Monitor the logs directory for new log data, and read in the raw lines as accessLines\n",
    "accessLines = spark.readStream.text(\"logs\")\n",
    "\n",
    "# Parse out the common log format to a DataFrame\n",
    "contentSizeExp = r'\\s(\\d+)$'\n",
    "statusExp = r'\\s(\\d{3})\\s'\n",
    "generalExp = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "timeExp = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "hostExp = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "\n",
    "logsDF = accessLines.select(regexp_extract('value', hostExp, 1).alias('host'),\n",
    "                         regexp_extract('value', timeExp, 1).alias('timestamp'),\n",
    "                         regexp_extract('value', generalExp, 1).alias('method'),\n",
    "                         regexp_extract('value', generalExp, 2).alias('endpoint'),\n",
    "                         regexp_extract('value', generalExp, 3).alias('protocol'),\n",
    "                         regexp_extract('value', statusExp, 1).cast('integer').alias('status'),\n",
    "                         regexp_extract('value', contentSizeExp, 1).cast('integer').alias('content_size'))\n",
    "\n",
    "# Keep a running count of every access by status code\n",
    "statusCountsDF = logsDF.groupBy(logsDF.status).count()\n",
    "\n",
    "# Kick off our streaming query, dumping results to the console\n",
    "query = ( statusCountsDF.writeStream.outputMode(\"complete\").format(\"console\").queryName(\"counts\").start() )\n",
    "\n",
    "# Run forever until terminated\n",
    "query.awaitTermination()\n",
    "\n",
    "# Cleanly shut down the session\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1282.text.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:567)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:268)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:164)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:164)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:34)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:196)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:210)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.text(DataStreamReader.scala:343)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStructuredStreaming\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Monitor the logs directory for new log data, and read in the raw lines as accessLines\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m accessLines \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Parse out the common log format to a DataFrame\u001b[39;00m\n\u001b[0;32m     12\u001b[0m contentSizeExp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)$\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\streaming.py:702\u001b[0m, in \u001b[0;36mDataStreamReader.text\u001b[1;34m(self, path, wholetext, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    696\u001b[0m     wholetext\u001b[38;5;241m=\u001b[39mwholetext,\n\u001b[0;32m    697\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m    698\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m    699\u001b[0m     recursiveFileLookup\u001b[38;5;241m=\u001b[39mrecursiveFileLookup,\n\u001b[0;32m    700\u001b[0m )\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath can be only a single string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1282.text.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:567)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:268)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:164)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:164)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:118)\r\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:34)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:196)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:210)\r\n\tat org.apache.spark.sql.streaming.DataStreamReader.text(DataStreamReader.scala:343)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "# Create a SparkSession (the config bit is only for Windows!)\n",
    "spark = SparkSession.builder.appName(\"StructuredStreaming\").getOrCreate()\n",
    "\n",
    "# Monitor the logs directory for new log data, and read in the raw lines as accessLines\n",
    "accessLines = spark.readStream.text(\"logs\")\n",
    "\n",
    "# Parse out the common log format to a DataFrame\n",
    "contentSizeExp = r'\\s(\\d+)$'\n",
    "statusExp = r'\\s(\\d{3})\\s'\n",
    "generalExp = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "timeExp = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "hostExp = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "\n",
    "logsDF = accessLines.select(func.regexp_extract('value', hostExp, 1).alias('host'),\n",
    "                         func.regexp_extract('value', timeExp, 1).alias('timestamp'),\n",
    "                         func.regexp_extract('value', generalExp, 1).alias('method'),\n",
    "                         func.regexp_extract('value', generalExp, 2).alias('endpoint'),\n",
    "                         func.regexp_extract('value', generalExp, 3).alias('protocol'),\n",
    "                         func.regexp_extract('value', statusExp, 1).cast('integer').alias('status'),\n",
    "                         func.regexp_extract('value', contentSizeExp, 1).cast('integer').alias('content_size'))\n",
    "\n",
    "logsDF2 = logsDF.withColumn(\"eventTime\", func.current_timestamp())\n",
    "\n",
    "# Keep a running count of endpoints\n",
    "endpointCounts = logsDF2.groupBy(func.window(func.col(\"eventTime\"), \\\n",
    "      \"30 seconds\", \"10 seconds\"), func.col(\"endpoint\")).count()\n",
    "\n",
    "sortedEndpointCounts = endpointCounts.orderBy(func.col(\"count\").desc())\n",
    "\n",
    "# Display the stream to the console\n",
    "query = sortedEndpointCounts.writeStream.outputMode(\"complete\").format(\"console\") \\\n",
    "      .queryName(\"counts\").start()\n",
    "\n",
    "# Wait until we terminate the scripts\n",
    "query.awaitTermination()\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>__Coursera - SparkSQL__<ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL is a Spark module for structured data processing. It is sed to query structured data inside Spark programs, using either SQL or a familiar DataFrame API.\n",
    "\n",
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Load a data file into a dataframe\n",
    "*   Create a Table View for the dataframe\n",
    "*   Run basic SQL queries and aggregate data on the table view\n",
    "*   Create a Pandas UDF to perform columnar operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we are going to be using Python and Spark (PySpark). These libraries should be installed in your lab environment or in SN Labs. Pandas is a popular data science package for Python. In this lab, we use Pandas to load a CSV file from disc to a pandas dataframe in memory. PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the spark context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\user\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.2)\n",
      "Requirement already satisfied: findspark in c:\\users\\user\\anaconda3\\lib\\site-packages (1.4.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pyarrow==0.14.1 (from versions: 0.9.0, 0.10.0, 0.11.0, 0.11.1, 0.12.0, 0.12.1, 0.13.0, 0.14.0, 0.15.1, 0.16.0, 0.17.0, 0.17.1, 1.0.0, 1.0.1, 2.0.0, 3.0.0, 4.0.0, 4.0.1, 5.0.0, 6.0.0, 6.0.1)\n",
      "ERROR: No matching distribution found for pyarrow==0.14.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Collecting numpy==1.19.5\n",
      "  Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "Successfully installed numpy-1.19.5\n"
     ]
    }
   ],
   "source": [
    "# Installing required packages\n",
    "!pip install pyspark\n",
    "!pip install findspark\n",
    "!pip install pyarrow==0.14.1\n",
    "!pip install pandas\n",
    "!pip install numpy==1.19.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 -  Spark session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and initialize the Spark session needed to load the data frames and operate on it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Creating the spark session and context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bbecaf5a89c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Creating a spark context class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Creating a spark session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "# Creating a spark context class\n",
    "sc = SparkContext()\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Initialize Spark session\n",
    "\n",
    "To work with dataframes we just need to verify that the spark session instance has been created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://karthiks-mbp.attlocal.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<PySpark.sql.session.SparkSession at 0x7fce55585dd8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Loading the Data and creating a table view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will first read the CSV file into a Pandas Dataframe and then read it into a Spark Dataframe\n",
    "Pandas is a library used for data manipulation and analysis. The Pandas library offers data structures and operations for creating and manipulating Data Series and DataFrame objects. Data can be imported from various data sources, e.g., Numpy arrays, Python dictionaries, and CSV files. Pandas allows you to manipulate, organize and display the data.\n",
    "\n",
    "To create a Spark DataFrame we load an external DataFrame, called `mtcars`. This DataFrame includes 32 observations on 11 variables:\n",
    "\n",
    "| colIndex | colName | units/description                        |\n",
    "| :------: | :------ | :--------------------------------------- |\n",
    "|   [, 1]  | mpg     | Miles per gallon                         |\n",
    "|   [, 2]  | cyl     | Number of cylinders                      |\n",
    "|   [, 3]  | disp    | Displacement (cu.in.)                    |\n",
    "|   [, 4]  | hp      | Gross horsepower                         |\n",
    "|   [, 5]  | drat    | Rear axle ratio                          |\n",
    "|   [, 6]  | wt      | Weight (lb/1000)                         |\n",
    "|   [, 7]  | qsec    | 1/4 mile time                            |\n",
    "|   [, 8]  | vs      | V/S                                      |\n",
    "|   [, 9]  | am      | Transmission (0 = automatic, 1 = manual) |\n",
    "|   [,10]  | gear    | Number of forward gears                  |\n",
    "|   [,11]  | carb    | Number of carburetors                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Load data into a Pandas DataFrame.\n",
    "\n",
    "Pandas has a convenient function to load CSV data from a URL directly into a pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file using `read_csv` function in pandas\n",
    "mtcars = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/mtcars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mazda RX4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mazda RX4 Wag</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datsun 710</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hornet 4 Drive</td>\n",
       "      <td>21.4</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.215</td>\n",
       "      <td>19.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hornet Sportabout</td>\n",
       "      <td>18.7</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.440</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
       "0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
       "1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
       "2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
       "3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3   \n",
       "4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3   \n",
       "\n",
       "   carb  \n",
       "0     4  \n",
       "1     4  \n",
       "2     1  \n",
       "3     1  \n",
       "4     2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview a few records\n",
    "mtcars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars.rename( columns={'Unnamed: 0':'name'}, inplace=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Loading data into a Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `createDataFrame` function to load the data into a spark dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(mtcars) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the schema of the loaded spark dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- mpg: double (nullable = true)\n",
      " |-- cyl: long (nullable = true)\n",
      " |-- disp: double (nullable = true)\n",
      " |-- hp: long (nullable = true)\n",
      " |-- drat: double (nullable = true)\n",
      " |-- wt: double (nullable = true)\n",
      " |-- qsec: double (nullable = true)\n",
      " |-- vs: long (nullable = true)\n",
      " |-- am: long (nullable = true)\n",
      " |-- gear: long (nullable = true)\n",
      " |-- carb: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Create a Table View\n",
    "\n",
    "Creating a table view in Spark SQL is required to run SQL queries programmatically on a DataFrame. A view is a temporary table to run SQL queries. A Temporary view provides local scope within the current Spark session. In this example we create a temporary view using the `createTempView()` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createTempView(\"cars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Running SQL queries and aggregating data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a table view, we can run queries similar to querying a SQL table. We perform similar operations to the ones in the DataFrames notebook. Note the difference here however is that we use the SQL queries directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+-----+---+----+------------------+-----+---+---+----+----+\n",
      "|               name| mpg|cyl| disp| hp|drat|                wt| qsec| vs| am|gear|carb|\n",
      "+-------------------+----+---+-----+---+----+------------------+-----+---+---+----+----+\n",
      "|          Mazda RX4|21.0|  6|160.0|110| 3.9|              2.62|16.46|  0|  1|   4|   4|\n",
      "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|             2.875|17.02|  0|  1|   4|   4|\n",
      "|         Datsun 710|22.8|  4|108.0| 93|3.85|              2.32|18.61|  1|  1|   4|   1|\n",
      "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|             3.215|19.44|  1|  0|   3|   1|\n",
      "|  Hornet Sportabout|18.7|  8|360.0|175|3.15|              3.44|17.02|  0|  0|   3|   2|\n",
      "|            Valiant|18.1|  6|225.0|105|2.76|              3.46|20.22|  1|  0|   3|   1|\n",
      "|         Duster 360|14.3|  8|360.0|245|3.21|              3.57|15.84|  0|  0|   3|   4|\n",
      "|          Merc 240D|24.4|  4|146.7| 62|3.69|              3.19| 20.0|  1|  0|   4|   2|\n",
      "|           Merc 230|22.8|  4|140.8| 95|3.92|              3.15| 22.9|  1|  0|   4|   2|\n",
      "|           Merc 280|19.2|  6|167.6|123|3.92|              3.44| 18.3|  1|  0|   4|   4|\n",
      "|          Merc 280C|17.8|  6|167.6|123|3.92|              3.44| 18.9|  1|  0|   4|   4|\n",
      "|         Merc 450SE|16.4|  8|275.8|180|3.07|              4.07| 17.4|  0|  0|   3|   3|\n",
      "|         Merc 450SL|17.3|  8|275.8|180|3.07|              3.73| 17.6|  0|  0|   3|   3|\n",
      "|        Merc 450SLC|15.2|  8|275.8|180|3.07|              3.78| 18.0|  0|  0|   3|   3|\n",
      "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93|              5.25|17.98|  0|  0|   3|   4|\n",
      "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.4239999999999995|17.82|  0|  0|   3|   4|\n",
      "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|             5.345|17.42|  0|  0|   3|   4|\n",
      "|           Fiat 128|32.4|  4| 78.7| 66|4.08|               2.2|19.47|  1|  1|   4|   1|\n",
      "|        Honda Civic|30.4|  4| 75.7| 52|4.93|             1.615|18.52|  1|  1|   4|   2|\n",
      "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|             1.835| 19.9|  1|  1|   4|   1|\n",
      "+-------------------+----+---+-----+---+----+------------------+-----+---+---+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing the whole table\n",
    "spark.sql(\"SELECT * FROM cars\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| mpg|\n",
      "+----+\n",
      "|21.0|\n",
      "|21.0|\n",
      "|22.8|\n",
      "|21.4|\n",
      "|18.7|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing a specific column\n",
    "spark.sql(\"SELECT mpg FROM cars\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "|       name| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
      "+-----------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "| Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
      "|  Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|\n",
      "|   Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|\n",
      "|   Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|\n",
      "|Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|\n",
      "+-----------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic filtering query to determine cars that have a high mileage and low cylinder count\n",
    "spark.sql(\"SELECT * FROM cars where mpg>20 AND cyl < 6\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|count(1)|cyl|\n",
      "+--------+---+\n",
      "|       7|  6|\n",
      "|      14|  8|\n",
      "|      11|  4|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregating data and grouping by cylinders\n",
    "spark.sql(\"SELECT count(*), cyl from cars GROUP BY cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Create a Pandas UDF to apply a columnar operation\n",
    "\n",
    "Apache Spark has become the de-facto standard in processing big data. To enable data scientists to leverage the value of big data, Spark added a Python API in version 0.7, with support for user-defined functions (UDF). These user-defined functions operate one-row-at-a-time, and thus suffer from high serialization and invocation overhead. As a result, many data pipelines define UDFs in Java and Scala and then invoke them from Python.\n",
    "\n",
    "Pandas UDFs built on top of Apache Arrow bring you the *best of both worlds*—the ability to define low-overhead, high-performance UDFs entirely in Python. In this simple example, we will build a Scalar Pandas UDF to convert the wT column from imperial units (1000-lbs) to metric units (metric tons).\n",
    "\n",
    "In addition, UDFs can be registered and invoked in SQL out of the box by registering a regular python function using the `@pandas_udf()` decorator. We can then apply this UDF to our `wt` column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Importing libraries and registering a UDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Pandas UDF function \n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.convert_wt(s: pandas.core.series.Series) -> pandas.core.series.Series>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@pandas_udf(\"float\")\n",
    "def convert_wt(s: pd.Series) -> pd.Series:\n",
    "    # The formula for converting from imperial to metric tons\n",
    "    return s * 0.45\n",
    "\n",
    "spark.udf.register(\"convert_weight\", convert_wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Applying the UDF to the tableview\n",
    "\n",
    "We can now apply the `convert_weight` user-defined-function to our `wt` column from the `cars` table view. This is done very simply using the SQL query shown below. In this example below we show both the original weight (in ton-lbs) and converted weight (in metric tons).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+-----+---+----+------------------+-----+---+---+----+----+------------------+-------------+\n",
      "|               name| mpg|cyl| disp| hp|drat|                wt| qsec| vs| am|gear|carb|   weight_imperial|weight_metric|\n",
      "+-------------------+----+---+-----+---+----+------------------+-----+---+---+----+----+------------------+-------------+\n",
      "|          Mazda RX4|21.0|  6|160.0|110| 3.9|              2.62|16.46|  0|  1|   4|   4|              2.62|        1.179|\n",
      "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|             2.875|17.02|  0|  1|   4|   4|             2.875|      1.29375|\n",
      "|         Datsun 710|22.8|  4|108.0| 93|3.85|              2.32|18.61|  1|  1|   4|   1|              2.32|        1.044|\n",
      "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|             3.215|19.44|  1|  0|   3|   1|             3.215|      1.44675|\n",
      "|  Hornet Sportabout|18.7|  8|360.0|175|3.15|              3.44|17.02|  0|  0|   3|   2|              3.44|        1.548|\n",
      "|            Valiant|18.1|  6|225.0|105|2.76|              3.46|20.22|  1|  0|   3|   1|              3.46|        1.557|\n",
      "|         Duster 360|14.3|  8|360.0|245|3.21|              3.57|15.84|  0|  0|   3|   4|              3.57|       1.6065|\n",
      "|          Merc 240D|24.4|  4|146.7| 62|3.69|              3.19| 20.0|  1|  0|   4|   2|              3.19|       1.4355|\n",
      "|           Merc 230|22.8|  4|140.8| 95|3.92|              3.15| 22.9|  1|  0|   4|   2|              3.15|       1.4175|\n",
      "|           Merc 280|19.2|  6|167.6|123|3.92|              3.44| 18.3|  1|  0|   4|   4|              3.44|        1.548|\n",
      "|          Merc 280C|17.8|  6|167.6|123|3.92|              3.44| 18.9|  1|  0|   4|   4|              3.44|        1.548|\n",
      "|         Merc 450SE|16.4|  8|275.8|180|3.07|              4.07| 17.4|  0|  0|   3|   3|              4.07|       1.8315|\n",
      "|         Merc 450SL|17.3|  8|275.8|180|3.07|              3.73| 17.6|  0|  0|   3|   3|              3.73|       1.6785|\n",
      "|        Merc 450SLC|15.2|  8|275.8|180|3.07|              3.78| 18.0|  0|  0|   3|   3|              3.78|        1.701|\n",
      "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93|              5.25|17.98|  0|  0|   3|   4|              5.25|       2.3625|\n",
      "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.4239999999999995|17.82|  0|  0|   3|   4|5.4239999999999995|       2.4408|\n",
      "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|             5.345|17.42|  0|  0|   3|   4|             5.345|      2.40525|\n",
      "|           Fiat 128|32.4|  4| 78.7| 66|4.08|               2.2|19.47|  1|  1|   4|   1|               2.2|         0.99|\n",
      "|        Honda Civic|30.4|  4| 75.7| 52|4.93|             1.615|18.52|  1|  1|   4|   2|             1.615|      0.72675|\n",
      "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|             1.835| 19.9|  1|  1|   4|   1|             1.835|      0.82575|\n",
      "+-------------------+----+---+-----+---+----+------------------+-----+---+---+----+----+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT *, wt AS weight_imperial, convert_weight(wt) as weight_metric FROM cars\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Basic SQL operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display all Mercedez car rows from the `cars` table view we created earlier. The Mercedez cars have the prefix \"Merc\" in the car name column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block for learners to answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for a hint.\n",
    "\n",
    "<!-- The hint is below:\n",
    "\n",
    "The SQL query word `like` is used to identify patterns. \n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- The answer is below:\n",
    "\n",
    "spark.sql(\"SELECT * FROM cars where name like 'Merc%'\").show()\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - User Defined Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we created a UDF to convert weight from imperial to metric units. Now for this exercise, please create a pandas UDF to convert the `mpg` column to `kmpl` (kilometers per liter). You can use the conversion factor of 0.425.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block for learners to answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- The answer is below:\n",
    "\n",
    "@pandas_udf(\"float\")\n",
    "def convert_mileage(s: pd.Series) -> pd.Series:\n",
    "    # The formula for converting from imperial to metric tons\n",
    "    return s * 0.425\n",
    "\n",
    "spark.udf.register(\"convert_mileage\", convert_mileage)\n",
    "\n",
    "spark.sql(\"SELECT *, mpg AS mpg, convert_weight(mpg) as kmpl FROM cars\").show()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
